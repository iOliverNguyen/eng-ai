<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 8: Decision Trees | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part2/08-decision-trees/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part2/08-decision-trees/"><meta property="og:title" content="Chapter 8: Decision Trees | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part2/08-decision-trees/"><meta name="twitter:title" content="Chapter 8: Decision Trees | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part2" data-astro-cid-ilhxcym7>Part II: Classical ML</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Decision Trees</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-8-decision-trees">Chapter 8: Decision Trees</h1>
<h2 id="learning-rules-from-data-ch8">Learning Rules from Data</h2>
<p>A decision tree is a model that learns a hierarchy of yes/no questions to make predictions. Unlike linear models that compute weighted sums, decision trees partition the input space by recursively splitting data based on feature thresholds. The result is a set of learned rules that are both interpretable and powerful.</p>
<p>Decision trees represent a fundamentally different approach to learning. Instead of finding smooth boundaries defined by equations, they learn discrete decision logic directly from data. This makes them particularly well-suited for problems where human reasoning follows a similar branching structure‚Äîcredit approval, medical diagnosis, fault detection‚Äîand where interpretability is as important as accuracy.</p>
<h2 id="splitting-data-ch8">Splitting Data</h2>
<p>The core operation in a decision tree is the split: choosing a feature and threshold that divides the data into two subsets that are more homogeneous with respect to the target variable. Homogeneity means that within each subset, most examples have the same label.</p>
<p>Consider a dataset of loan applications with features like income, credit score, and debt-to-income ratio. The root split might ask: ‚ÄúIs credit score ‚â• 700?‚Äù This divides applicants into two groups. One group (credit score ‚â• 700) is mostly approved loans. The other group (credit score &#x3C; 700) is more mixed. We continue recursively, asking new questions of each subset.</p>
<p>The algorithm measures homogeneity using impurity metrics. Two common measures are:</p>
<p><strong>Gini Impurity</strong>: Measures the probability of misclassifying a randomly chosen example if it were labeled according to the distribution in the node. For a node with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> examples and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> classes where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the proportion of examples in class <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Gini</mtext><mo>=</mo><mn>1</mn><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msubsup><mi>p</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\text{Gini} = 1 - \sum_{i=1}^{k} p_i^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Gini</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:3.1138em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8361em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">‚àë</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span>
<p>If all examples are the same class, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p_i = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> for one class and Gini = 0 (perfect purity). If classes are evenly distributed, Gini is maximum (maximum impurity).</p>
<p><strong>Entropy</strong>: Measures the average amount of information needed to specify the class of an example drawn from the node:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Entropy</mtext><mo>=</mo><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>p</mi><mi>i</mi></msub><msub><mrow><mi>log</mi><mo>‚Å°</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Entropy} = -\sum_{i=1}^{k} p_i \log_2(p_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Entropy</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1138em;vertical-align:-1.2777em;"></span><span class="mord">‚àí</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8361em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">‚àë</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>
<p>Entropy is 0 when the node is pure and maximum when classes are uniformly distributed. It‚Äôs derived from information theory and has a slightly different penalty structure than Gini, but in practice, the two produce similar trees.</p>
<p>The algorithm evaluates every possible split‚Äîevery feature and every threshold value‚Äîand chooses the one that maximally reduces impurity. This greedy approach is computationally efficient and works well despite not being globally optimal.</p>
<p><strong>Computational Complexity</strong>: For a dataset with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> samples and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span> features, finding the best split at a node requires <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>d</mi><mo>√ó</mo><mi>n</mi><mi>log</mi><mo>‚Å°</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d \times n \log n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span> time. For each feature, the algorithm sorts the values (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>log</mi><mo>‚Å°</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n \log n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>), then evaluates all possible thresholds (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>). Building the entire tree with depth <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> takes approximately <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>d</mi><mo>√ó</mo><mi>n</mi><mi>log</mi><mo>‚Å°</mo><mi>n</mi><mo>√ó</mo><mi>h</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d \times n \log n \times h)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mclose">)</span></span></span></span>. This makes trees efficient for moderately sized datasets but expensive for high-dimensional data with millions of features.</p>
<p><strong>Handling Continuous Features</strong>: For continuous features, the algorithm identifies candidate split points by sorting the feature values and considering midpoints between consecutive values that differ in their labels. If income values are [25k, 30k, 35k, 50k] and the first two are denied while the last two are approved, the algorithm tests thresholds like 32.5k and 42.5k. This is efficient because you only need to test thresholds where the class distribution changes.</p>
<p><strong>Handling Missing Values</strong>: Decision trees can handle missing data gracefully through <strong>surrogate splits</strong>. When a feature value is missing, the algorithm uses a backup feature that correlates with the primary split. If credit score is missing but income is correlated with credit score, the tree can route the example using income instead. Alternatively, some implementations send examples with missing values down both branches and combine predictions.</p>
<p><strong>Why Greedy Splitting Works</strong>: The greedy strategy‚Äîchoosing the best split at each step without looking ahead‚Äîis not globally optimal. A split that looks poor now might enable excellent splits later. But exhaustive search over all possible tree structures is computationally intractable (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>2</mn><mi>n</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2^n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> examples). Greedy splitting is a practical compromise that produces good trees efficiently.</p>
<h2 id="tree-growth-ch8">Tree Growth</h2>
<p>Building a decision tree is a recursive algorithm:</p>
<ol>
<li>Start with all training data at the root node</li>
<li>Find the best split (feature + threshold) that reduces impurity the most</li>
<li>Create two child nodes and partition the data based on the split</li>
<li>Recursively apply the same process to each child node</li>
<li>Stop when a stopping criterion is met</li>
</ol>
<p>Stopping criteria include:</p>
<ul>
<li>The node is pure (all examples have the same label)</li>
<li>The node has fewer than some minimum number of examples</li>
<li>The tree has reached a maximum depth</li>
<li>No split reduces impurity below some threshold</li>
</ul>
<p>Without stopping criteria, the tree grows until every leaf contains exactly one training example. This results in perfect training accuracy‚Äîthe tree memorizes every training example‚Äîbut it will not generalize. This is the classic overfitting problem from Part I.</p>
<p><img  src="/eng-ai/_astro/08-diagram.B5euNAgA_ZEyFoq.svg" alt="Tree Growth diagram" width="500" height="350" loading="lazy" decoding="async"></p>
<p>The diagram shows a decision tree for loan approval. Each internal node asks a question about a feature. Each leaf node makes a prediction. The path from root to leaf represents the learned rule for that prediction. For example: ‚ÄúIf credit ‚â• 700 and income ‚â• 50k, then approve.‚Äù</p>
<h2 id="interpretability-ch8">Interpretability</h2>
<p>Decision trees are interpretable in a way that linear models are not. A linear model tells you how much each feature contributes on average across all examples. A decision tree tells you exactly which features matter for a specific prediction and in what order.</p>
<p>Consider the loan approval tree. If an applicant is denied, you can trace the path through the tree and explain: ‚ÄúYou were denied because your credit score is below 700 and your income is below $30,000.‚Äù This is a complete explanation, not a statistical correlation.</p>
<p>This interpretability makes decision trees valuable in domains where users need to understand and contest decisions‚Äîcredit lending, medical diagnosis, hiring, criminal justice. Regulatory requirements like GDPR‚Äôs ‚Äúright to explanation‚Äù favor models that can provide concrete reasoning.</p>
<p>But interpretability has limits. As trees grow deeper with more splits, they become harder to understand. A tree with 20 levels and hundreds of leaves is not interpretable, even though each path is technically a rule. In practice, shallow trees (depth 3-5) are interpretable; deeper trees are not.</p>
<h2 id="feature-interactions-and-nonlinearity-ch8">Feature Interactions and Nonlinearity</h2>
<p>Decision trees automatically discover feature interactions without requiring explicit feature engineering. A linear model treats features independently‚Äîit learns a weight for income and a separate weight for credit score, then adds them together. To capture interactions like ‚Äúhigh income compensates for moderate credit score,‚Äù you must manually create interaction terms like income √ó credit_score.</p>
<p>Trees discover these interactions naturally through sequential splits. The loan approval tree splits first on credit score, then on income within each credit score bucket. This creates different income thresholds for different credit score ranges‚Äîexactly the interaction a linear model would miss.</p>
<p>Consider predicting whether a customer will buy a product. The pattern might be: ‚ÄúYoung customers with high income buy, regardless of location. Older customers only buy if they live in urban areas.‚Äù This is a three-way interaction between age, income, and location. A linear model cannot represent this without manually engineering interaction features. A decision tree discovers it automatically by splitting on age, then on income for young customers and on location for older customers.</p>
<p>Trees also handle nonlinearity without transformation. If the relationship between a feature and the outcome is a complex curve‚Äîhouse price increases logarithmically with square footage‚Äîa linear model requires you to manually apply a log transform. A tree learns the nonlinearity by creating multiple splits at different thresholds, effectively approximating the curve with a step function.</p>
<p>This power comes from <strong>axis-aligned splits</strong>. Each split divides the feature space into rectangular regions. The decision boundary is a combination of vertical and horizontal lines (or hyperplanes in higher dimensions). This differs from linear models, which create a single smooth boundary, and from neural networks, which can create arbitrarily shaped boundaries. Trees occupy a middle ground: more flexible than linear models, more structured than neural networks.</p>
<p>The downside: axis-aligned splits are inefficient for diagonal boundaries. If the true boundary is ‚Äúx + y = 1,‚Äù a tree needs many splits to approximate it, while a linear model captures it perfectly with a single hyperplane. Trees excel when the true boundary aligns with feature axes and involves complex interactions.</p>
<h2 id="why-trees-overfit-ch8">Why Trees Overfit</h2>
<p>Decision trees overfit aggressively. Without constraints, they grow until they memorize the training data. Every training example ends up in its own leaf, resulting in 100% training accuracy and poor test performance.</p>
<p>Why does this happen? Trees are extremely flexible. They can approximate any function by adding enough splits. This flexibility means they have <strong>high variance</strong>‚Äîsmall changes in training data can produce completely different trees. If one noisy example causes a different split at the root, the entire downstream tree structure changes. This instability is the hallmark of overfitting.</p>
<p>Consider training a tree on 1,000 loan applications. One example is a misclassified approval‚Äîsomeone with low credit score who got a loan due to special circumstances not captured in the features. If this example happens to fall near a split boundary, it might force the tree to create a new branch specifically to accommodate it. That branch is based on noise, not signal, and won‚Äôt generalize.</p>
<p>To visualize variance, imagine training trees on different random subsets of the same dataset. Each tree will have a different structure‚Äîdifferent splits, different thresholds, different leaf predictions. The trees agree on general patterns (high credit score is good) but disagree on specifics (exactly what income threshold to use). This disagreement is variance: the model‚Äôs predictions fluctuate with the training sample.</p>
<p>Compare this to linear models, which have low variance. A linear model trained on different subsets will learn similar weight vectors‚Äîthe training data constrains the model to a narrow hypothesis space. Trees have a vast hypothesis space (all possible tree structures), so different training samples can lead to vastly different hypotheses.</p>
<p>Preventing overfitting requires limiting this flexibility through regularization. Without regularization, trees will always memorize. Even a single deep tree on a small dataset will overfit catastrophically.</p>
<h2 id="pruning-strategies-ch8">Pruning Strategies</h2>
<p>Pruning constrains tree complexity to prevent overfitting. There are two main approaches: pre-pruning (stop growing early) and post-pruning (grow fully, then remove branches).</p>
<p><strong>Pre-Pruning (Early Stopping)</strong></p>
<p>Pre-pruning sets stopping criteria during tree growth. Once a criterion is met, the algorithm stops splitting that node, making it a leaf. Common pre-pruning criteria:</p>
<ul>
<li><strong>Max depth</strong>: Limit tree depth to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> levels. Shallow trees are less likely to memorize. Typical values: 3-10 for interpretable trees, 10-30 for ensembles.</li>
<li><strong>Min samples per leaf</strong>: Require each leaf to contain at least <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> examples. This prevents creating leaves for individual outliers. Typical values: 5-20 for classification, 1-10 for regression.</li>
<li><strong>Min samples per split</strong>: Require at least <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> examples to consider splitting a node. Similar to min samples per leaf but applies before the split.</li>
<li><strong>Min impurity decrease</strong>: Only split if the reduction in impurity exceeds some threshold <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ¥</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">Œ¥</span></span></span></span>. This prevents splits that barely improve purity but add complexity.</li>
</ul>
<p>Pre-pruning is efficient‚Äîyou avoid growing branches that will be removed later. But it‚Äôs greedy: you might stop too early. A split that looks useless now might enable great splits later. If max depth is 3, the tree never discovers a pattern that requires 4 levels of splits.</p>
<p><strong>Post-Pruning (Cost-Complexity Pruning)</strong></p>
<p>Post-pruning grows the tree to maximum depth (or until leaves are pure), then removes branches that don‚Äôt improve validation performance. This overcomes pre-pruning‚Äôs greediness‚Äîyou see the fully grown tree before deciding what to remove.</p>
<p><strong>Cost-complexity pruning</strong> is the most principled approach. It defines a cost function that balances training accuracy and tree size:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Cost</mtext><mo>=</mo><msub><mtext>Error</mtext><mtext>training</mtext></msub><mo>+</mo><mi>Œ±</mi><mo>√ó</mo><mtext>Number¬†of¬†leaves</mtext></mrow><annotation encoding="application/x-tex">\text{Cost} = \text{Error}_{\text{training}} + \alpha \times \text{Number of leaves}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Cost</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord text"><span class="mord">Error</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">training</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">Number¬†of¬†leaves</span></span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span></span></span></span> is a regularization parameter controlling the tradeoff. For each subtree, the algorithm computes whether collapsing it (removing the split and making the parent a leaf) reduces the cost. It prunes the least useful branches first, continuing until the cost stops decreasing.</p>
<p>By varying <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span></span></span></span> from 0 (no pruning) to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">‚àû</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord">‚àû</span></span></span></span> (prune everything, just a root node), you get a sequence of trees of different complexities. Use cross-validation to choose the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span></span></span></span> that minimizes validation error. This is analogous to tuning regularization strength in linear models.</p>
<p><strong>Reduced Error Pruning</strong> is simpler. Grow a full tree on training data. For each internal node, try replacing the subtree with a leaf (predicting the majority class at that node). Measure accuracy on a separate validation set. If accuracy improves or stays the same, prune the subtree. This is less principled than cost-complexity pruning but easier to implement.</p>
<p><strong>Tradeoff: Interpretability vs Accuracy</strong></p>
<p>Pruning forces a choice between simplicity and performance. A tree with 3 splits is interpretable but may miss important patterns. A tree with 100 splits captures more detail but is not human-readable. In production, the right tradeoff depends on the domain:</p>
<ul>
<li><strong>High interpretability required</strong> (lending, medical diagnosis): Use aggressive pruning, max depth 3-5, accept lower accuracy.</li>
<li><strong>Accuracy matters most</strong> (fraud detection, ranking): Use deeper trees, light pruning, or skip individual trees entirely and use ensembles.</li>
</ul>
<p>Even with optimal pruning, single decision trees have high variance. This is why ensembles‚Äîrandom forests and gradient boosting‚Äîare almost always better than single trees. Ensembles average away variance while preserving the benefits of tree-based learning.</p>
<h2 id="engineering-takeaway-ch8">Engineering Takeaway</h2>
<p>Decision trees remain widely used in production for several reasons:</p>
<p><strong>Interpretability is unmatched.</strong> In domains where decisions must be explained‚Äîlending, insurance, healthcare‚Äîdecision trees provide human-readable rules. You can visualize the entire decision logic. You can trace any prediction back to the specific conditions that caused it. Regulatory compliance and user trust often outweigh small gains in accuracy from black-box models. When a loan application is denied, ‚Äúyour credit score is below 700 and your income is below $30k‚Äù is far more acceptable than ‚Äúthe neural network assigned you a low score.‚Äù</p>
<p><strong>No preprocessing needed.</strong> Trees naturally handle both continuous and categorical features without requiring encoding schemes or normalization. They can split on ‚Äúcountry = USA‚Äù as easily as ‚Äúage > 30.‚Äù They‚Äôre robust to outliers‚Äîan extreme value just creates one split, rather than skewing the entire model. They handle missing values through surrogate splits or by sending examples down both branches. This simplifies feature engineering and makes trees resilient to messy real-world data.</p>
<p><strong>Discovers feature interactions automatically.</strong> Unlike linear models, trees automatically capture interactions and nonlinear relationships. They don‚Äôt assume any functional form‚Äîthey learn the structure directly from data. A tree can discover that ‚Äúhigh income compensates for low credit score‚Äù without you creating an income √ó credit_score interaction term. This makes trees powerful when feature interactions are complex and unknown.</p>
<p><strong>Feature importance is built-in.</strong> Trees provide natural feature importance scores based on how much each feature reduces impurity across all splits. Frequently used features with large impurity reductions are important; rarely used features are not. This helps with feature selection, debugging, and understanding what the model has learned. Feature importance from trees is more reliable than linear model coefficients when features are correlated or interactions exist.</p>
<p><strong>Overfits without regularization.</strong> Single decision trees are high-variance models that memorize training data without constraints. You must limit depth, enforce minimum samples per leaf, or use pruning to prevent overfitting. Even with regularization, single trees are unstable‚Äîsmall changes in data cause large changes in structure. This limits their standalone use in production, where stability matters.</p>
<p><strong>High variance makes single trees unreliable.</strong> Retrain a tree on a slightly different sample, and you get a completely different structure. This instability is why single trees are rarely deployed in modern systems. But variance is exactly what ensembles exploit: by averaging many high-variance trees, random forests and gradient boosting reduce variance without increasing bias. Understanding single-tree variance is essential to understanding why ensembles work.</p>
<p><strong>Foundation for random forests and gradient boosting.</strong> The real power of decision trees comes from ensembles. Random forests and gradient boosting machines (covered in Chapter 9) are the dominant methods for tabular data in Kaggle competitions and production systems. These ensembles rely on trees as the base learner. Understanding individual trees‚Äîhow they split, why they overfit, what makes them interpretable‚Äîis essential to understanding why ensembles are so effective.</p>
<p>The lesson: Decision trees trade smoothness for interpretability and flexibility. They overfit easily but provide the building blocks for some of the most powerful models in machine learning. Master single trees, and you understand the foundation of modern ensemble methods.</p>
<hr>
<h2 id="references-and-further-reading-ch8">References and Further Reading</h2>
<p><strong>Induction of Decision Trees</strong> ‚Äì J.R. Quinlan (1986)
<a href="https://link.springer.com/article/10.1007/BF00116251">https://link.springer.com/article/10.1007/BF00116251</a></p>
<p>This is the original paper that introduced the ID3 algorithm, the foundation of modern decision tree learning. Quinlan describes how trees are grown using information gain (entropy reduction) and explores when trees overfit. Reading this paper gives you the historical context and core intuition behind recursive partitioning. ID3 was later extended to C4.5 and CART, but the fundamental ideas originated here.</p>
<p><strong>Classification and Regression Trees (CART)</strong> ‚Äì Leo Breiman, Jerome Friedman, Richard Olshen, Charles Stone (1984)
<a href="https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-richard-olshen-charles-stone">https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-richard-olshen-charles-stone</a></p>
<p>CART is the canonical reference for decision trees. It covers both classification and regression trees, introduces Gini impurity, and describes pruning strategies for regularization. This book is mathematical and thorough‚Äîit‚Äôs the definitive treatment of tree-based learning. Most modern tree implementations (scikit-learn, XGBoost) are based on CART. If you want to deeply understand how trees work, this is the authoritative source.</p>
<p><strong>C4.5: Programs for Machine Learning</strong> ‚Äì J.R. Quinlan (1993)
Morgan Kaufmann Publishers</p>
<p>C4.5 is the successor to ID3 and one of the most influential decision tree algorithms. Quinlan addresses ID3‚Äôs limitations: handling continuous features efficiently, dealing with missing values, and pruning to prevent overfitting. C4.5 introduces gain ratio (normalized information gain) and post-pruning via error-based pruning. This book is practical‚Äîit describes a working system with code. C4.5 was the dominant tree algorithm until CART-based implementations became widespread. Understanding C4.5 shows how trees evolved from theoretical constructs to production-ready tools.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part2/07-logistic-regression" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Logistic Regression</span> </a> <a href="/eng-ai/part2/09-ensembles" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Ensembles</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#learning-rules-from-data-ch8" data-astro-cid-xvrfupwn>Learning Rules from Data</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#splitting-data-ch8" data-astro-cid-xvrfupwn>Splitting Data</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#tree-growth-ch8" data-astro-cid-xvrfupwn>Tree Growth</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#interpretability-ch8" data-astro-cid-xvrfupwn>Interpretability</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#feature-interactions-and-nonlinearity-ch8" data-astro-cid-xvrfupwn>Feature Interactions and Nonlinearity</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-trees-overfit-ch8" data-astro-cid-xvrfupwn>Why Trees Overfit</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#pruning-strategies-ch8" data-astro-cid-xvrfupwn>Pruning Strategies</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch8" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch8" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>