<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 9: Ensembles | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part2/09-ensembles/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part2/09-ensembles/"><meta property="og:title" content="Chapter 9: Ensembles | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part2/09-ensembles/"><meta name="twitter:title" content="Chapter 9: Ensembles | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part2" data-astro-cid-ilhxcym7>Part II: Classical ML</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Ensembles</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-9-ensembles">Chapter 9: Ensembles</h1>
<h2 id="why-many-weak-models-beat-one-strong-one-ch9">Why Many Weak Models Beat One Strong One</h2>
<p>An ensemble is a collection of models that make predictions together. Rather than training a single powerful model, you train many weaker models and combine their predictions. This counterintuitive approach‚Äîthat many mediocre predictors can outperform one sophisticated predictor‚Äîis one of the most important ideas in machine learning.</p>
<p>Ensembles dominate Kaggle competitions, fraud detection systems, recommendation engines, and risk scoring models. They achieve state-of-the-art performance on tabular data, often exceeding the accuracy of neural networks. Understanding why ensembles work requires understanding the bias-variance tradeoff from Part I and how averaging reduces variance without increasing bias.</p>
<h2 id="wisdom-of-crowds-ch9">Wisdom of Crowds</h2>
<p>The core insight behind ensembles is that independent errors cancel out when averaged. If you have ten models that each make different mistakes, the average of their predictions will be more accurate than any individual prediction‚Äîassuming the errors are uncorrelated.</p>
<p>Consider predicting house prices. Model A might overestimate prices for houses near parks because it overfits to this pattern. Model B might underestimate prices for recently renovated houses. Model C might struggle with outliers in expensive neighborhoods. If these errors are independent, averaging the three predictions produces a more accurate estimate than any single model.</p>
<p>This is the same principle behind the ‚Äúwisdom of crowds‚Äù phenomenon: asking 100 people to guess the number of jellybeans in a jar and averaging their guesses often produces a more accurate estimate than any individual guess. Individual guesses have random errors that cancel out in the average, leaving a more accurate central estimate.</p>
<p>Mathematically, if each model has error variance <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>œÉ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">œÉ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> and the errors are uncorrelated, the variance of the average of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> models is:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Var</mtext><mo stretchy="false">(</mo><mtext>average</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>œÉ</mi><mn>2</mn></msup><mi>n</mi></mfrac></mrow><annotation encoding="application/x-tex">\text{Var}(\text{average}) = \frac{\sigma^2}{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Var</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">average</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.1771em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4911em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">œÉ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>This means the variance decreases linearly with the number of models. With 100 models, the variance is 1/100th of a single model‚Äôs variance. This is variance reduction without increasing bias‚Äîwhich directly improves generalization.</p>
<p>The critical requirement is that the models‚Äô errors are uncorrelated. If all models make the same mistakes, averaging doesn‚Äôt help. This is why ensemble methods intentionally introduce diversity‚Äîby training on different subsets of data, using different features, or randomizing the training process.</p>
<h2 id="random-forests-ch9">Random Forests</h2>
<p>A random forest is an ensemble of decision trees where each tree is trained on a different random subset of the training data and considers a random subset of features at each split. This randomness ensures that the trees are diverse enough that their errors are uncorrelated.</p>
<p>The algorithm works as follows:</p>
<ol>
<li>
<p><strong>Bootstrap sampling</strong>: For each tree, sample the training data with replacement. This creates a dataset of the same size where some examples appear multiple times and some don‚Äôt appear at all. Each tree sees a slightly different view of the data.</p>
</li>
<li>
<p><strong>Random feature selection</strong>: At each split in each tree, only consider a random subset of features. If there are 100 features, each split might only evaluate 10 randomly chosen features. This prevents trees from all using the same strong features and forces them to explore different patterns.</p>
</li>
<li>
<p><strong>Grow deep trees</strong>: Each tree is grown deep without pruning, allowing it to fit the training data closely. Normally this would cause overfitting, but because we‚Äôre averaging many trees, the ensemble generalization is good despite each individual tree overfitting.</p>
</li>
<li>
<p><strong>Average predictions</strong>: For regression, average the predictions of all trees. For classification, take a majority vote or average the predicted probabilities.</p>
</li>
</ol>
<p><img  src="/eng-ai/_astro/09-diagram.CBw_FjYs_ZFWuob.svg" alt="Random Forests diagram" width="500" height="320" loading="lazy" decoding="async"></p>
<p>The diagram shows how random forests work: bootstrap sampling creates diverse training sets, each tree makes a prediction, and the average is the final output. Individual predictions vary, but the average is stable and accurate.</p>
<p>Why does this work so well? Decision trees are high-variance models‚Äîthey overfit and are sensitive to small changes in data. But they have relatively low bias‚Äîthey can approximate complex functions. Random forests reduce variance through averaging while preserving the low bias. This directly addresses the bias-variance tradeoff.</p>
<p><strong>Out-of-Bag Error Estimation</strong>: Random forests provide a clever way to estimate test error without a separate validation set. During bootstrap sampling, each tree sees about 63% of the training data (due to sampling with replacement). The remaining 37%‚Äîthe ‚Äúout-of-bag‚Äù (OOB) examples‚Äîwere not used to train that tree. You can use these OOB examples as a validation set for that tree. Aggregate the OOB predictions across all trees, and you get an unbiased estimate of test error. This is essentially free cross-validation‚Äîyou get a validation estimate without holding out data.</p>
<p><strong>Feature Importance</strong>: Random forests compute feature importance by measuring how much each feature reduces impurity (Gini or entropy) across all splits in all trees. Features used frequently at the top of trees (where they split large, impure nodes) have high importance. Features rarely used or used only in deep nodes have low importance. This aggregated importance score is more robust than single-tree importance because it averages across many trees with different structures.</p>
<p><strong>Hyperparameters</strong>: Key hyperparameters for random forests include:</p>
<ul>
<li><strong>Number of trees</strong> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mtext>estimators</mtext></msub></mrow><annotation encoding="application/x-tex">n_{\text{estimators}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">estimators</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>): More trees almost always improve performance, with diminishing returns after 100-500 trees. Random forests rarely overfit from too many trees‚Äîmore trees just reduce variance further.</li>
<li><strong>Max features per split</strong> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>max_features</mtext></mrow><annotation encoding="application/x-tex">\text{max\_features}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">max_features</span></span></span></span></span>): Typical values are <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>d</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1078em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">d</span></span></span><span style="top:-2.8922em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.1078em;"><span></span></span></span></span></span></span></span></span> for classification and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi mathvariant="normal">/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">d/3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mord">/3</span></span></span></span> for regression, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span> is the number of features. Smaller values increase diversity but may miss important features.</li>
<li><strong>Max depth and min samples per leaf</strong>: These control individual tree complexity. Random forests typically use deep trees (high max depth, low min samples per leaf) to reduce bias, relying on averaging to control variance.</li>
</ul>
<h2 id="gradient-boosting-ch9">Gradient Boosting</h2>
<p>Boosting takes a different approach to ensembles. Instead of training many trees independently and averaging them, boosting trains trees sequentially, where each new tree focuses on the mistakes of the previous trees. This sequential error correction produces highly accurate models.</p>
<p>The most important boosting algorithm is gradient boosting, which works as follows:</p>
<ol>
<li>
<p><strong>Initialize</strong>: Start with a simple prediction (e.g., the mean of the target variable).</p>
</li>
<li>
<p><strong>Compute residuals</strong>: Calculate the errors (residuals) of the current ensemble on the training data.</p>
</li>
<li>
<p><strong>Train a tree on residuals</strong>: Train a new tree to predict these residuals‚Äînot the original target, but the errors the ensemble currently makes.</p>
</li>
<li>
<p><strong>Add to ensemble</strong>: Add this new tree to the ensemble with a small weight (learning rate). The ensemble now predicts: old prediction + (learning rate √ó new tree).</p>
</li>
<li>
<p><strong>Repeat</strong>: Iterate, each time training a new tree to correct the remaining errors.</p>
</li>
</ol>
<p>After many iterations, the ensemble consists of hundreds or thousands of trees, each one correcting the mistakes of its predecessors. The final prediction is the sum of all trees‚Äô predictions, scaled by the learning rate.</p>
<p>Gradient boosting is called ‚Äúgradient‚Äù boosting because it‚Äôs actually performing gradient descent in function space. The residuals are the negative gradient of the loss function with respect to the predictions. By fitting trees to residuals, we‚Äôre moving the predictions in the direction that reduces loss‚Äîjust like gradient descent adjusts parameters to reduce loss.</p>
<p>This connection to optimization is fundamental. Boosting is not just an ensemble method‚Äîit‚Äôs an optimization algorithm that incrementally builds a model by fitting additive components that reduce loss.</p>
<p><strong>Learning Rate (Shrinkage)</strong>: The learning rate controls the contribution of each tree. A small learning rate (e.g., 0.01-0.1) means each tree has a small effect, requiring more trees but producing better generalization. A large learning rate (e.g., 0.3-1.0) means fewer trees are needed but the model may overfit. There‚Äôs a tradeoff between number of trees and learning rate: small learning rate + many trees ‚âà large learning rate + few trees, but the former generalizes better.</p>
<p><strong>Why Boosting Reduces Bias</strong>: Early trees capture the main patterns‚Äîthe low-hanging fruit. Later trees capture subtler patterns, interactions, and edge cases that earlier trees missed. This sequential refinement reduces bias by allowing the model to fit increasingly complex functions. Each tree adds a small correction, and thousands of corrections sum to a highly accurate predictor.</p>
<p><strong>Overfitting Risk</strong>: Unlike random forests, gradient boosting can overfit if you train too many trees. Each new tree fits the training residuals, and eventually, it starts fitting noise rather than signal. This is why boosting uses <strong>early stopping</strong>: monitor validation error and stop adding trees when validation error stops improving. Alternatively, use regularization techniques like tree depth limits, subsampling, or stronger learning rate decay.</p>
<p><strong>Modern Variants</strong>: Modern gradient boosting implementations add optimizations and regularization:</p>
<ul>
<li>
<p><strong>XGBoost</strong> (Extreme Gradient Boosting): Adds L1 and L2 regularization on tree weights, handles sparse data efficiently, and includes advanced splitting algorithms. It‚Äôs highly optimized for speed and dominates Kaggle competitions.</p>
</li>
<li>
<p><strong>LightGBM</strong> (Light Gradient Boosting Machine): Uses histogram-based splitting instead of sorting features at each split, making it much faster for large datasets. It also uses leaf-wise tree growth (versus XGBoost‚Äôs level-wise growth), which can produce more accurate trees with the same number of leaves.</p>
</li>
<li>
<p><strong>CatBoost</strong> (Categorical Boosting): Handles categorical features natively without requiring one-hot encoding. It uses ordered boosting to reduce overfitting and target leakage. It‚Äôs particularly strong when your data has many categorical features.</p>
</li>
</ul>
<p>All three are production-grade implementations with GPU support, distributed training, and extensive tuning options. They dominate tabular data problems in industry and competitions.</p>
<h2 id="other-ensemble-methods-ch9">Other Ensemble Methods</h2>
<p>Beyond random forests and gradient boosting, several other ensemble techniques are used in practice:</p>
<p><strong>Bagging (Bootstrap Aggregating)</strong>: Bagging is the general technique behind random forests. Train multiple models on bootstrap samples (sampling with replacement) and average their predictions. Bagging works with any base model‚Äîdecision trees, neural networks, even linear models‚Äîbut it‚Äôs most effective with high-variance models like deep trees. Bagging reduces variance without increasing bias, making unstable models more robust.</p>
<p><strong>Voting Ensembles</strong>: Combine predictions from multiple different model types. For example, train a random forest, a gradient boosting model, and a logistic regression model, then:</p>
<ul>
<li><strong>Hard voting</strong> (classification): Each model votes for a class, and the majority wins.</li>
<li><strong>Soft voting</strong> (classification): Average the predicted probabilities from each model and choose the class with the highest average probability.</li>
<li><strong>Averaging</strong> (regression): Average the predictions from each model.</li>
</ul>
<p>Voting works best when the base models are diverse‚Äîthey make different types of errors. Combining a tree-based model with a linear model can capture both nonlinear patterns (trees) and linear trends (logistic regression).</p>
<p><strong>Stacking (Stacked Generalization)</strong>: Stacking trains a meta-learner on top of base models. First, train several base models (e.g., random forest, gradient boosting, SVM). Then, use their predictions as features to train a meta-learner (often logistic regression or a simple neural network) that learns how to best combine them. The meta-learner discovers which base models are reliable in which situations. Stacking can outperform simple averaging but requires careful cross-validation to avoid overfitting.</p>
<p><strong>Blending</strong>: Blending is a simpler version of stacking. Split the training data into two parts. Train base models on the first part, get their predictions on the second part (holdout set), then train the meta-learner on these holdout predictions. Blending is easier to implement than full stacking but uses less data for training base models.</p>
<p><strong>When to Use Each</strong>:</p>
<ul>
<li><strong>Bagging</strong>: When you have a high-variance model and want to stabilize it without changing the model type.</li>
<li><strong>Voting</strong>: When you have multiple trained models and want a simple way to combine them.</li>
<li><strong>Stacking</strong>: When you want the best possible accuracy and can afford the complexity of training a meta-learner.</li>
<li><strong>Boosting</strong>: When you want state-of-the-art accuracy on tabular data and can tune hyperparameters carefully.</li>
</ul>
<h2 id="hyperparameter-tuning-for-ensembles-ch9">Hyperparameter Tuning for Ensembles</h2>
<p>Ensembles have many hyperparameters that control the bias-variance tradeoff. Effective tuning is critical for production performance.</p>
<p><strong>Random Forest Hyperparameters</strong>:</p>
<ul>
<li><strong>Number of trees</strong> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mtext>estimators</mtext></msub></mrow><annotation encoding="application/x-tex">n_{\text{estimators}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">estimators</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>): Start with 100, increase to 200-500 if you have compute. More trees rarely hurt.</li>
<li><strong>Max features</strong> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>max_features</mtext></mrow><annotation encoding="application/x-tex">\text{max\_features}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">max_features</span></span></span></span></span>): Default of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>d</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1078em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">d</span></span></span><span style="top:-2.8922em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.1078em;"><span></span></span></span></span></span></span></span></span> (classification) or <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi mathvariant="normal">/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">d/3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mord">/3</span></span></span></span> (regression) works well. Decrease to increase diversity, increase to let trees find best features.</li>
<li><strong>Max depth</strong>: Default is unlimited (grow until leaves are pure). If overfitting, limit to 10-30.</li>
<li><strong>Min samples per leaf</strong>: Default is 1. Increase to 5-20 if you have noisy data or want to prevent overfitting.</li>
</ul>
<p>Random forests are robust to hyperparameters‚Äîdefaults usually work well. The main tuning knob is the number of trees.</p>
<p><strong>Gradient Boosting Hyperparameters</strong>:</p>
<ul>
<li><strong>Learning rate</strong> (shrinkage): Typical values: 0.01-0.3. Smaller is better but requires more trees. Use 0.01-0.05 for large datasets, 0.1-0.2 for small datasets.</li>
<li><strong>Number of trees</strong> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mtext>estimators</mtext></msub></mrow><annotation encoding="application/x-tex">n_{\text{estimators}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">estimators</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>): Depends on learning rate. With learning rate 0.1, try 100-1000 trees. With learning rate 0.01, try 1000-5000 trees. Use early stopping to find the optimal number.</li>
<li><strong>Max depth</strong>: Typical values: 3-10. Shallow trees (3-6) prevent overfitting and are faster. Deeper trees (7-10) capture more interactions.</li>
<li><strong>Subsample</strong>: Fraction of training data to use per tree (stochastic gradient boosting). Typical: 0.5-1.0. Lower values (0.5-0.8) prevent overfitting by adding randomness.</li>
<li><strong>Min child weight / min samples per leaf</strong>: Regularization to prevent overfitting. Typical: 1-10.</li>
</ul>
<p>Gradient boosting requires more tuning than random forests. Start with learning rate 0.1, max depth 6, and 100-500 trees, then tune.</p>
<p><strong>Tuning Strategies</strong>:</p>
<ul>
<li><strong>Grid search</strong>: Try all combinations of hyperparameters from predefined lists. Exhaustive but slow.</li>
<li><strong>Random search</strong>: Sample random combinations of hyperparameters. Often finds good solutions faster than grid search because not all hyperparameters matter equally.</li>
<li><strong>Bayesian optimization</strong>: Use a surrogate model to predict which hyperparameters will perform well, then test those. More efficient than random search but requires more setup.</li>
</ul>
<p>Always use <strong>cross-validation</strong> to evaluate hyperparameters. K-fold cross-validation (typically k=5 or k=10) ensures your tuning doesn‚Äôt overfit to a single train/validation split. For time series data, use time-based splitting (train on past, validate on future).</p>
<h2 id="bias-variance-reduction-ch9">Bias-Variance Reduction</h2>
<p>Why do ensembles work? Because they reduce variance without significantly increasing bias.</p>
<p><strong>Random forests</strong> reduce variance by averaging many high-variance, low-bias trees. Each tree can overfit, but the average cannot‚Äîrandom errors cancel out, leaving the systematic patterns. This is pure variance reduction.</p>
<p><strong>Boosting</strong> reduces bias by sequentially adding trees that correct errors. Early trees capture the main patterns (low bias). Later trees capture subtler patterns and interactions (further reducing bias). The small learning rate and early stopping prevent overfitting, keeping variance controlled.</p>
<p>Both methods exploit the bias-variance tradeoff from Chapter 4, but in different ways:</p>
<ul>
<li>Random forests: low bias (flexible trees), reduced variance (averaging)</li>
<li>Boosting: reduced bias (sequential correction), controlled variance (small learning rate, regularization)</li>
</ul>
<p>This is why ensembles dominate tabular data. Neural networks are powerful for images, text, and audio because they learn hierarchical representations. But for structured data‚Äîdatabase tables with mixed features‚Äîensembles of trees are often more accurate, more robust, and faster to train.</p>
<h2 id="engineering-takeaway-ch9">Engineering Takeaway</h2>
<p>Ensembles are the workhorses of production machine learning for structured data. Random forests and gradient boosting (especially implementations like XGBoost, LightGBM, and CatBoost) power fraud detection, credit scoring, recommendation systems, and ranking models at companies like Google, Amazon, and financial institutions.</p>
<p><strong>State-of-the-art on tabular data.</strong> Ensembles consistently win Kaggle competitions involving structured data. When the input is rows and columns rather than images or text, ensembles are the default choice. They handle missing values, mixed data types, and noisy features better than neural networks. In production systems processing transactional data, user behavior logs, or sensor readings, gradient boosting is often the go-to algorithm.</p>
<p><strong>Feature importance and interpretation.</strong> Ensembles provide reliable feature importance scores, which help with feature selection, model debugging, and explaining predictions to stakeholders. SHAP (SHapley Additive exPlanations) values extend this further by attributing each prediction to specific feature contributions, making ensembles viable in regulated domains like lending and healthcare. You can explain individual predictions while maintaining high accuracy.</p>
<p><strong>Robustness to hyperparameters.</strong> Random forests work well with default settings‚Äîthey‚Äôre remarkably forgiving. Gradient boosting requires more tuning (learning rate, number of trees, max depth) but is far more forgiving than neural networks, which have dozens of architectural and optimization hyperparameters. This robustness reduces iteration time in production: you can get a strong baseline quickly, then tune incrementally.</p>
<p><strong>Fast parallel training.</strong> Random forests parallelize perfectly‚Äîeach tree is independent. Modern implementations (scikit-learn, XGBoost, LightGBM) train hundreds of trees in parallel across CPU cores. Gradient boosting is inherently sequential (each tree depends on previous residuals), but modern implementations parallelize the tree-growing step and support GPU acceleration. You can train models on millions of rows in minutes.</p>
<p><strong>Handles complex interactions naturally.</strong> Trees discover feature interactions automatically through sequential splits. Ensembles aggregate these interactions across hundreds of trees, capturing complex nonlinear relationships without manual feature engineering. This makes ensembles powerful when you don‚Äôt fully understand which features interact or when the interaction structure is too complex to engineer manually.</p>
<p><strong>Overfitting resistance varies by method.</strong> Random forests rarely overfit‚Äîadding more trees almost always helps. Gradient boosting can overfit if you train too many trees, but early stopping prevents this by monitoring validation error. Both methods are more resistant to overfitting than deep neural networks on small-to-medium tabular datasets (&#x3C; 100k examples), where neural networks tend to memorize.</p>
<p><strong>Scalability with modern implementations.</strong> XGBoost, LightGBM, and CatBoost scale to billions of examples and millions of features through algorithmic optimizations: histogram-based splitting, sparsity-aware algorithms, and distributed training. LightGBM‚Äôs leaf-wise growth strategy trains faster than XGBoost‚Äôs level-wise strategy on large datasets. CatBoost‚Äôs categorical feature handling eliminates expensive one-hot encoding. These implementations make ensembles production-ready at scale.</p>
<p>The lesson: If your data is tabular‚Äîfeatures in columns, examples in rows‚Äîyour first attempt should be gradient boosting or random forests. Reserve neural networks for domains where hierarchical feature learning is critical. Ensembles are not just competitive; they‚Äôre often the best choice for structured data.</p>
<hr>
<h2 id="references-and-further-reading-ch9">References and Further Reading</h2>
<p><strong>Random Forests</strong> ‚Äì Leo Breiman (2001)
<a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf</a></p>
<p>This is the original paper that introduced random forests. Breiman explains bootstrap aggregating (bagging), random feature selection, and why the method works so well. He also introduces out-of-bag error estimation and feature importance measures. Reading this paper gives you the foundational intuition for why diversity in ensembles leads to variance reduction. It‚Äôs remarkably accessible for a machine learning paper and shows the thought process behind one of ML‚Äôs most successful algorithms.</p>
<p><strong>Greedy Function Approximation: A Gradient Boosting Machine</strong> ‚Äì Jerome Friedman (2001)
<a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">https://statweb.stanford.edu/~jhf/ftp/trebst.pdf</a></p>
<p>This is the paper that formalized gradient boosting as an optimization procedure. Friedman shows that boosting is equivalent to gradient descent in function space and introduces regularization techniques (shrinkage, subsampling) that make boosting practical. This paper is more technical than Breiman‚Äôs random forests paper, but it‚Äôs essential for understanding why boosting works and how modern implementations like XGBoost are designed. It reveals the deep connection between ensemble learning and optimization.</p>
<p><strong>XGBoost: A Scalable Tree Boosting System</strong> ‚Äì Tianqi Chen and Carlos Guestrin (2016)
<a href="https://arxiv.org/abs/1603.02754">https://arxiv.org/abs/1603.02754</a></p>
<p>XGBoost is one of the most widely used machine learning libraries in production and Kaggle competitions. This paper explains the engineering optimizations that make gradient boosting fast and scalable: sparsity-aware learning, cache-aware algorithms, parallel tree construction, and distributed training. Reading this shows how algorithmic ideas become production systems. XGBoost‚Äôs dominance in ML competitions demonstrates that engineering and optimization matter as much as algorithmic novelty.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part2/08-decision-trees" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Decision Trees</span> </a> <a href="/eng-ai/part2/10-loss-functions" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Loss Functions and Optimization</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-many-weak-models-beat-one-strong-one-ch9" data-astro-cid-xvrfupwn>Why Many Weak Models Beat One Strong One</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#wisdom-of-crowds-ch9" data-astro-cid-xvrfupwn>Wisdom of Crowds</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#random-forests-ch9" data-astro-cid-xvrfupwn>Random Forests</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#gradient-boosting-ch9" data-astro-cid-xvrfupwn>Gradient Boosting</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#other-ensemble-methods-ch9" data-astro-cid-xvrfupwn>Other Ensemble Methods</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#hyperparameter-tuning-for-ensembles-ch9" data-astro-cid-xvrfupwn>Hyperparameter Tuning for Ensembles</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#bias-variance-reduction-ch9" data-astro-cid-xvrfupwn>Bias-Variance Reduction</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch9" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch9" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>