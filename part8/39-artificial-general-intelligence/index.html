<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 39: Artificial General Intelligence | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part8/39-artificial-general-intelligence/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part8/39-artificial-general-intelligence/"><meta property="og:title" content="Chapter 39: Artificial General Intelligence | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part8/39-artificial-general-intelligence/"><meta name="twitter:title" content="Chapter 39: Artificial General Intelligence | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part8" data-astro-cid-ilhxcym7>Part VIII: The Frontier</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Artificial General Intelligence</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-39-artificial-general-intelligence">Chapter 39: Artificial General Intelligence</h1>
<p>The term ‚ÄúArtificial General Intelligence‚Äù carries weight. It evokes visions of machines that think, reason, and learn like humans‚Äîor surpass them. News articles ask: ‚ÄúIs GPT-4 AGI?‚Äù Tech leaders claim AGI is ‚Äú5-10 years away.‚Äù Conferences debate when AGI will arrive and what it will mean for humanity. The discourse is filled with hype, speculation, and mysticism.</p>
<p>This chapter cuts through the noise. AGI has a technical meaning: a system that matches human-level performance across <strong>all cognitive tasks</strong>, with the ability to <strong>transfer knowledge</strong> broadly and operate <strong>autonomously</strong>. By this definition, current AI systems‚Äîincluding GPT-4, AlphaGo, multimodal models‚Äîare not AGI. They excel at narrow tasks but fail at general transfer. They predict text or classify images but lack agency, world models, and robust grounding.</p>
<p>The gap between current AI and AGI is not a matter of scale. It is architectural, conceptual, and epistemological. Large language models optimize next-token prediction‚Äîa statistical task. AGI requires goal-setting, planning, causal reasoning, and continual learning‚Äîcapabilities current models do not have. The path from LLMs to AGI is not obvious. It may require fundamental breakthroughs, not just bigger models.</p>
<p>This chapter demystifies AGI: what it means, what it requires, why current systems fall short, and what would change if AGI existed. The conclusion is grounded: narrow AI will dominate for years, possibly decades. Engineers should focus on building reliable, useful systems today, not waiting for AGI to solve problems.</p>
<hr>
<h2 id="what-agi-actually-means-transfer-and-autonomy-ch39">What AGI Actually Means: Transfer and Autonomy</h2>
<p>Defining AGI precisely is difficult because ‚Äúintelligence‚Äù is multifaceted. But most definitions converge on two requirements: <strong>broad transfer</strong> and <strong>autonomy</strong>.</p>
<h3 id="broad-transfer-learning-one-task-applying-to-many"><strong>Broad Transfer: Learning One Task, Applying to Many</strong></h3>
<p>Humans learn skills and apply them across domains:</p>
<ul>
<li>Learn to cook ‚Üí apply similar principles to chemistry (mixing, heating, timing)</li>
<li>Learn to play chess ‚Üí apply strategic thinking to business decisions</li>
<li>Learn a programming language ‚Üí quickly learn another language by analogy</li>
</ul>
<p>This is <strong>transfer learning</strong> at its most general: knowledge from domain A improves performance in unrelated domain B. Current AI systems transfer within narrow domains but fail across fundamentally different tasks.</p>
<p><strong>Current AI transfer:</strong></p>
<ul>
<li>GPT-3 trained on text ‚Üí fine-tuned for code generation (related domain: text-like sequences)</li>
<li>Vision model trained on ImageNet ‚Üí fine-tuned for medical imaging (related domain: images)</li>
<li>RL agent trained on Atari games ‚Üí fails on board games without retraining</li>
</ul>
<p><strong>Human-level transfer:</strong></p>
<ul>
<li>Cooking skills ‚Üí repair bicycle (cross-domain abstraction: understand systems, troubleshoot, improvise)</li>
<li>Reading music ‚Üí learn new instrument by analogy (transfer musical notation, rhythm, melody concepts)</li>
<li>Playing soccer ‚Üí learn basketball quickly (transfer spatial awareness, teamwork, strategy)</li>
</ul>
<p>AGI requires this level of transfer: learn from one domain, generalize to unrelated domains with minimal new data. Current models do not have this capability. They generalize within the distribution they trained on, but out-of-distribution generalization is brittle.</p>
<h3 id="autonomy-setting-goals-and-planning"><strong>Autonomy: Setting Goals and Planning</strong></h3>
<p>Humans set their own goals, plan multi-step actions, and adapt when plans fail:</p>
<ul>
<li>Goal: ‚ÄúGet a promotion‚Äù ‚Üí Plan: improve skills, take on projects, network ‚Üí Adapt when blocked</li>
<li>Goal: ‚ÄúCook dinner‚Äù ‚Üí Plan: check ingredients, follow recipe, adjust if missing items</li>
<li>Goal: ‚ÄúLearn to play guitar‚Äù ‚Üí Plan: practice scales, learn songs, seek feedback ‚Üí Adjust based on progress</li>
</ul>
<p>This is <strong>autonomy</strong>: the ability to formulate goals, break them into subgoals, execute plans, and adjust based on feedback. Current AI systems are <strong>reactive</strong>‚Äîthey respond to prompts but do not set goals.</p>
<p><strong>Current AI:</strong></p>
<ul>
<li>GPT-4: Responds to user prompts, generates text, stops when done. No intrinsic goals.</li>
<li>AlphaGo: Wins Go games (goal provided by training objective), does nothing else</li>
<li>Self-driving cars: Follow routes (goal provided by navigation system), do not decide where to go</li>
</ul>
<p><strong>AGI:</strong></p>
<ul>
<li>Decides ‚ÄúI want to learn physics‚Äù ‚Üí finds resources, studies, asks questions, evaluates understanding</li>
<li>Notices a problem (e.g., inefficiency in a system) ‚Üí proposes solution, implements, validates</li>
<li>Sets long-term goals (years) and adjusts plans as circumstances change</li>
</ul>
<p>Autonomy is not just planning‚Äîit is <strong>goal formation</strong>. Current models optimize loss functions defined by humans. AGI would define its own objectives.</p>
<hr>
<h2 id="what-current-models-lack-world-models-goals-grounding-ch39">What Current Models Lack: World Models, Goals, Grounding</h2>
<p>Large language models achieve impressive capabilities: they write essays, solve math problems, generate code. But they lack fundamental properties required for AGI.</p>
<h3 id="no-persistent-world-models"><strong>No Persistent World Models</strong></h3>
<p>Humans maintain internal representations of the physical world: objects persist, obey physics, have 3D structure, interact causally. Models do not.</p>
<p><strong>Test: Object permanence</strong></p>
<p>Show a model a video: ball rolls behind a box, out of view. Ask: ‚ÄúWhere is the ball?‚Äù Humans know the ball still exists behind the box. Models struggle‚Äîthey do not track object states across frames.</p>
<p><strong>Test: Physics reasoning</strong></p>
<p>Show an image: stack of blocks, one block partially off the edge. Ask: ‚ÄúWhat happens if I remove the bottom block?‚Äù Humans predict collapse. Models guess randomly‚Äîthey lack physics understanding.</p>
<p>Current multimodal models (GPT-4V, Gemini) learn statistical associations between visual patterns and language but do not build 3D world models. They describe what they see but do not understand causality, dynamics, or object interactions.</p>
<h3 id="no-intrinsic-goals"><strong>No Intrinsic Goals</strong></h3>
<p>Models optimize objectives defined during training: minimize cross-entropy loss, maximize reward in RL environments. But they do not set their own goals.</p>
<p>GPT-4 generates text to minimize loss on next-token prediction. It has no intrinsic drive to learn, explore, or achieve outcomes. When the prompt ends, the model stops. There is no curiosity, no planning beyond the current sequence, no long-term objectives.</p>
<p>Humans have intrinsic motivation: curiosity (explore the unknown), mastery (improve skills), autonomy (control one‚Äôs actions). These drives shape behavior even without external rewards. AGI would require similar intrinsic goals‚Äîbut how to specify them? Loss functions are proxies for human intent, not true goals.</p>
<h3 id="weak-grounding"><strong>Weak Grounding</strong></h3>
<p>Language models learn from text‚Äîwords and symbols. But text is disconnected from the physical world. The model reads ‚Äúgravity pulls objects down‚Äù but never experiences gravity. It predicts ‚Äúfire is hot‚Äù but never feels heat.</p>
<p><strong>Grounding</strong> means connecting symbols to sensory experience. Multimodal models (Chapter 37) improve grounding by linking text and images, but this is statistical, not experiential. The model sees millions of images of dogs and associates ‚Äúdog‚Äù with visual patterns, but it does not know what it is like to pet a dog, hear it bark, or interact with it.</p>
<p>Humans ground language in embodied experience: we learn ‚Äúheavy‚Äù by lifting objects, ‚Äúhot‚Äù by feeling temperature, ‚Äúfast‚Äù by moving. Models lack bodies, sensors, and motor control. Their grounding is second-hand: learned from data, not from interaction.</p>
<h3 id="no-causal-reasoning"><strong>No Causal Reasoning</strong></h3>
<p>Models learn correlations: which words co-occur, which images have similar patterns. But correlation is not causation. Models predict <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">‚à£</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(Y | X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">‚à£</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span> (probability of Y given X) but do not understand whether X causes Y, Y causes X, or both are caused by a hidden factor Z.</p>
<p><strong>Example: Spurious correlations</strong></p>
<p>A model trained on medical data might learn: ‚Äúpatients who receive treatment X have higher mortality.‚Äù Does treatment X cause death? Or do doctors prescribe X to already critically ill patients? The model cannot distinguish. It learns the correlation (X correlates with death) but not the causal structure.</p>
<p>Causal reasoning requires interventions: manipulate X, observe whether Y changes. Models trained on static datasets cannot perform interventions‚Äîthey only observe. Without causal models, AGI cannot plan effectively: planning requires predicting outcomes of actions, which requires understanding causality.</p>
<h3 id="no-continual-learning"><strong>No Continual Learning</strong></h3>
<p>Models are trained once, then deployed with frozen weights. They do not learn during inference. A model deployed in January 2024 has the same weights in December 2024‚Äîit does not improve from user interactions.</p>
<p>Humans learn continuously: every conversation, every observation updates our world model. We adapt to new environments, learn from mistakes, refine skills over time. AGI would require continual learning: update knowledge as the world changes, integrate feedback in real-time, improve from experience.</p>
<p>Current models lack this capability. They suffer from <strong>catastrophic forgetting</strong>: training on new data erases previously learned patterns. Continual learning without forgetting remains an unsolved problem.</p>
<hr>
<h2 id="why-llms-are-not-agi-prediction-agency-ch39">Why LLMs Are Not AGI: Prediction ‚â† Agency</h2>
<p>Language models generate impressive outputs: essays, code, poems, conversations. But impressive outputs do not imply understanding or agency.</p>
<h3 id="llms-are-prediction-engines"><strong>LLMs Are Prediction Engines</strong></h3>
<p>GPT-4 optimizes <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>next¬†token</mtext><mi mathvariant="normal">‚à£</mi><mtext>context</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\text{next token} | \text{context})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">next¬†token</span></span><span class="mord">‚à£</span><span class="mord text"><span class="mord">context</span></span><span class="mclose">)</span></span></span></span>. Given a prompt, it predicts the most likely next word, then the next, token by token, until a stopping criterion is met. This is a statistical task: find patterns in training data, generalize to new prompts.</p>
<p>Prediction is powerful‚Äîit enables coherent text generation. But prediction is not reasoning, planning, or understanding. The model does not ‚Äúthink‚Äù about the prompt, does not ‚Äúunderstand‚Äù what it is writing. It samples from a learned distribution over sequences.</p>
<h3 id="no-planning-no-backtracking"><strong>No Planning, No Backtracking</strong></h3>
<p>Humans plan before acting. Writing an essay: outline main points, organize arguments, revise drafts. Solving a math problem: try an approach, notice it‚Äôs wrong, backtrack, try another approach.</p>
<p>LLMs generate text left-to-right, token by token, with no backtracking. If the model starts down a wrong path, it cannot revise‚Äîit must continue until the sequence ends. Recent techniques (chain-of-thought, self-critique) mimic planning by generating reasoning chains, but this is still sequential generation, not true planning.</p>
<p>True planning requires:</p>
<ul>
<li>Evaluate multiple candidate plans before committing</li>
<li>Predict long-term consequences of actions</li>
<li>Revise plans when intermediate steps fail</li>
</ul>
<p>LLMs approximate this via multi-step generation (generate, critique, revise), but this is expensive (multiple forward passes) and still lacks the flexibility of human planning.</p>
<h3 id="brittleness-and-out-of-distribution-failure"><strong>Brittleness and Out-of-Distribution Failure</strong></h3>
<p>LLMs generalize within their training distribution but fail on out-of-distribution inputs. They perform well on common prompts but degrade on edge cases, adversarial inputs, or novel domains.</p>
<p><strong>Example: Legal reasoning</strong></p>
<p>GPT-4 performs well on typical legal questions (common topics, standard phrasing). But on edge cases‚Äînovel legal theories, cross-jurisdictional nuances, highly specialized domains‚Äîperformance degrades. The model memorizes common patterns but does not deeply understand legal principles.</p>
<p>Humans generalize more robustly. We transfer knowledge from seen cases to unseen cases by reasoning from principles. Models memorize examples and interpolate. This works within the training distribution but fails outside it.</p>
<h3 id="hallucinations-confidence-without-knowledge"><strong>Hallucinations: Confidence Without Knowledge</strong></h3>
<p>LLMs generate fluent, confident-sounding text even when factually incorrect. The model does not know what it does not know. High probability output ‚â† truth.</p>
<p><strong>Example: Citation hallucination</strong></p>
<p>Ask GPT-4 for academic references on a niche topic. The model generates plausible-sounding titles, authors, journals‚Äîbut the papers do not exist. The model optimizes for fluency and coherence, not factual accuracy. It ‚Äúhallucinates‚Äù citations that fit the pattern of real references.</p>
<p>AGI would require <strong>epistemic awareness</strong>: knowing the limits of one‚Äôs knowledge, expressing uncertainty, seeking information when uncertain. LLMs lack this awareness. They generate text with equal confidence regardless of whether the underlying knowledge is strong or weak.</p>
<hr>
<h2 id="what-would-change-if-agi-existed-ch39">What Would Change If AGI Existed</h2>
<p>AGI, if achieved, would be transformative. But speculation about AGI often veers into science fiction. Here, we focus on concrete technical capabilities AGI would enable.</p>
<h3 id="autonomous-research"><strong>Autonomous Research</strong></h3>
<p>An AGI scientist could:</p>
<ul>
<li>Formulate hypotheses based on prior research</li>
<li>Design experiments to test hypotheses</li>
<li>Execute experiments (if embodied or connected to lab equipment)</li>
<li>Interpret results, update hypotheses, iterate</li>
</ul>
<p>This would accelerate research: AGI works 24/7, does not need sleep, can parallelize across many instances. But current models cannot do this‚Äîthey lack curiosity (intrinsic drive to explore), domain grounding (deep understanding of scientific principles), and experimental autonomy (ability to design and conduct experiments independently).</p>
<h3 id="recursive-self-improvement"><strong>Recursive Self-Improvement</strong></h3>
<p>AGI could improve its own architecture and training algorithms. If AGI understands machine learning deeply enough, it could:</p>
<ul>
<li>Propose new architectures more efficient than Transformers</li>
<li>Design better training algorithms (optimizers, loss functions)</li>
<li>Generate better training data</li>
</ul>
<p>This would lead to <strong>recursive self-improvement</strong>: each iteration creates a smarter system, which creates an even smarter system, accelerating indefinitely. This is the ‚Äúintelligence explosion‚Äù scenario. However, current models cannot design better models‚Äîthey lack the meta-cognitive ability to reason about their own limitations and propose improvements.</p>
<h3 id="general-purpose-robotics"><strong>General-Purpose Robotics</strong></h3>
<p>An AGI robot could:</p>
<ul>
<li>Cook meals (plan recipe, manipulate ingredients, adjust to missing items)</li>
<li>Clean and organize (understand clutter, categorize objects, navigate spaces)</li>
<li>Repair equipment (diagnose problems, identify solutions, execute fixes)</li>
</ul>
<p>This requires: perception (see and understand 3D environment), manipulation (fine motor control), planning (multi-step task decomposition), and adaptation (handle unexpected obstacles). Current robots excel at narrow tasks (pick-and-place in factories) but fail at general-purpose tasks in unstructured environments.</p>
<h3 id="economic-disruption"><strong>Economic Disruption</strong></h3>
<p>If AGI automates most cognitive labor, economic structures change radically:</p>
<ul>
<li>Knowledge work (law, medicine, engineering, writing) largely automated</li>
<li>Labor demand shifts: from cognitive tasks to roles requiring human interaction, creativity, or embodied presence</li>
<li>Productivity surges, but distribution of gains is a policy question (who benefits from automation?)</li>
</ul>
<p>This scenario assumes AGI reaches human-level capability across all domains. Even then, deployment is constrained by regulation, trust, and infrastructure. Economic disruption would be gradual, not instant.</p>
<h3 id="safety-challenges"><strong>Safety Challenges</strong></h3>
<p>Misaligned AGI‚Äîa system with autonomy and intelligence but goals misaligned with human values‚Äîposes existential risk. If AGI optimizes an objective misspecified by humans, outcomes could be catastrophic. This is the ‚Äúalignment problem‚Äù at AGI-scale: ensuring powerful autonomous systems act in humanity‚Äôs interest.</p>
<p>But current systems are not AGI. They lack autonomy, do not set their own goals, and optimize human-defined loss functions. Safety research on current models (Chapter 35) is necessary and valuable, but AGI-specific risks are speculative until AGI architectures exist.</p>
<hr>
<h2 id="engineering-takeaway-ch39">Engineering Takeaway</h2>
<h3 id="agi-is-not-imminentcurrent-models-lack-fundamental-capabilities"><strong>AGI is not imminent‚Äîcurrent models lack fundamental capabilities</strong></h3>
<p>Despite impressive performance on benchmarks, current AI systems do not have the core properties required for AGI: broad transfer, autonomy, persistent world models, causal reasoning, continual learning. The gap is not merely quantitative (more data, more parameters)‚Äîit is qualitative (different architectures, different learning paradigms). Claiming ‚ÄúAGI is 5 years away‚Äù is speculation, not engineering forecasting. Architectural breakthroughs may be required, and we cannot predict when they will occur.</p>
<h3 id="narrow-ai-dominates-for-yearstask-specific-systems-outperform-general-systems-economically"><strong>Narrow AI dominates for years‚Äîtask-specific systems outperform general systems economically</strong></h3>
<p>Even if AGI were achievable soon, narrow AI systems would dominate economically. A specialized fraud detection model outperforms a general-purpose AGI for fraud detection: it is cheaper, faster, more reliable, and easier to deploy. General-purpose systems sacrifice efficiency for flexibility. For most applications, flexibility is not worth the cost. Narrow AI‚Äîtask-specific models optimized for performance and cost‚Äîwill dominate the market for years, possibly decades.</p>
<h3 id="transfer-learning--general-intelligencemodels-transfer-within-domains-not-across-fundamentally-different-tasks"><strong>Transfer learning ‚â† general intelligence‚Äîmodels transfer within domains, not across fundamentally different tasks</strong></h3>
<p>BERT fine-tunes from text classification to question answering (within NLP). ViT fine-tunes from ImageNet to medical imaging (within vision). But GPT-4 cannot transfer from language to robotic manipulation, and AlphaGo cannot transfer from Go to stock trading. Transfer works within related domains but fails across fundamentally different modalities or tasks. General intelligence requires transfer across any domain, which current models cannot do.</p>
<h3 id="agency-requires-new-architecturesllms-are-reactive-agi-needs-goal-setting-and-planning"><strong>Agency requires new architectures‚ÄîLLMs are reactive, AGI needs goal-setting and planning</strong></h3>
<p>LLMs respond to prompts but do not set goals. To achieve autonomy, models must formulate objectives, plan multi-step actions, and adapt when plans fail. This requires architectures beyond next-token prediction: models must evaluate candidate plans, predict long-term outcomes, and revise strategies based on feedback. These capabilities may require integrating symbolic reasoning, reinforcement learning, and world models‚Äîresearch areas separate from LLM development. Scaling LLMs alone is unlikely to produce agency.</p>
<h3 id="safety-research-necessary-noweven-non-agi-systems-cause-harm-prepare-before-agi-arrives"><strong>Safety research necessary now‚Äîeven non-AGI systems cause harm; prepare before AGI arrives</strong></h3>
<p>AI safety is not just about AGI. Current models already cause harm: bias, misinformation, misuse. Safety research today‚Äîalignment, robustness, interpretability‚Äîbuilds foundations for future systems. If AGI arrives, we need safety frameworks in place. Waiting until AGI exists to address safety is too late. Proactive research now reduces risk later. But safety research should focus on current systems, not speculate about AGI scenarios that may not materialize.</p>
<h3 id="hype-obscures-progresscalling-gpt-4-agi-confuses-prediction-with-understanding"><strong>Hype obscures progress‚Äîcalling GPT-4 ‚ÄúAGI‚Äù confuses prediction with understanding</strong></h3>
<p>Labeling GPT-4 or similar models as ‚ÄúAGI‚Äù is misleading. It conflates impressive narrow capabilities (text generation, question answering) with general intelligence (broad transfer, autonomy, causal reasoning). This hype obscures the real progress: LLMs are remarkable prediction engines, enabling new applications. But they are not AGI. Confusing the two distorts research priorities, misallocates resources, and sets unrealistic public expectations. Clarity matters: call LLMs what they are‚Äîpowerful, narrow tools.</p>
<h3 id="engineering-focusbuild-reliable-narrow-systems-dont-wait-for-agi-to-solve-problems"><strong>Engineering focus‚Äîbuild reliable, narrow systems; don‚Äôt wait for AGI to solve problems</strong></h3>
<p>Engineers should focus on solving real problems with current technology, not waiting for AGI. Narrow AI‚Äîfraud detection, medical imaging, language translation, recommendation systems‚Äîdelivers value today. These systems are deployable, cost-effective, and reliable. Waiting for AGI to ‚Äúsolve everything‚Äù wastes opportunity. Build useful systems now, iterate, improve incrementally. AGI may arrive eventually, but engineering progress happens through incremental improvements, not waiting for breakthroughs.</p>
<hr>
<p><img  src="/eng-ai/_astro/39-diagram.DfZkz9Ln_Z2pnSqh.svg" alt="Engineering Takeaway diagram" width="800" height="500" loading="lazy" decoding="async"></p>
<hr>
<h2 id="references-and-further-reading-ch39">References and Further Reading</h2>
<h3 id="on-the-measure-of-intelligence---chollet-2019-google"><strong>On the Measure of Intelligence</strong> - Chollet (2019), Google</h3>
<p><strong>Why it matters:</strong> Fran√ßois Chollet argues that intelligence is not task-specific performance (beating humans at chess, Go, or image classification) but <strong>skill-acquisition efficiency</strong>‚Äîthe ability to learn new tasks quickly with minimal examples. He introduces the ARC (Abstraction and Reasoning Corpus) benchmark, which tests generalization to novel tasks with minimal data. Current models, including GPT-4, struggle with ARC despite superhuman performance on standard benchmarks. This paper reframes AGI: it is not about memorizing vast datasets but about flexible, sample-efficient learning. Chollet shows that current models excel at interpolation (within training distribution) but fail at extrapolation (novel tasks requiring abstraction). True general intelligence requires the latter. This paper grounds the AGI debate in measurable properties, not vague claims.</p>
<h3 id="reward-is-enough---silver-et-al-2021-deepmind"><strong>Reward is Enough</strong> - Silver et al. (2021), DeepMind</h3>
<p><strong>Why it matters:</strong> David Silver and colleagues hypothesize that maximizing reward in sufficiently complex environments could lead to general intelligence. They argue that abilities like perception, knowledge, reasoning, planning, and social intelligence emerge from reward-seeking in rich environments. This is a controversial claim: it suggests AGI might arise from scaling reinforcement learning in increasingly realistic simulations, without explicit design of capabilities. Critics argue that real-world reward signals are sparse, ambiguous, and difficult to specify‚Äîunlike games where rewards are clear. The paper is speculative but influential: it frames AGI as an emergent property of optimization in complex environments. Whether reward is truly ‚Äúenough‚Äù remains an open question, but the hypothesis is testable.</p>
<h3 id="the-bitter-lesson---sutton-2019-essay"><strong>The Bitter Lesson</strong> - Sutton (2019), Essay</h3>
<p><strong>Why it matters:</strong> Richard Sutton argues that the history of AI shows a consistent pattern: general methods that leverage computation and learning outperform approaches based on human knowledge and domain-specific heuristics. Chess, Go, speech recognition, and computer vision were all solved by scaling general algorithms (search, learning), not by encoding expert knowledge. The ‚Äúbitter lesson‚Äù: human insight and cleverness are less valuable than scale and learning. Sutton predicts AGI will come from scaling general methods, not hand-crafted architectures. Critics note that some breakthroughs (Transformers, residual networks) required architectural insights, not just scale. The debate: will AGI emerge from scaling existing methods (LLMs, RL), or does it require new architectures? Sutton‚Äôs essay influenced the ‚Äúscaling hypothesis‚Äù that dominates current AI development.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part8/38-self-improving-systems" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Self-Improving Systems</span> </a> <a href="/eng-ai/part8/40-the-engineers-role" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>The Engineer&#39;s Role</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#what-agi-actually-means-transfer-and-autonomy-ch39" data-astro-cid-xvrfupwn>What AGI Actually Means: Transfer and Autonomy</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#broad-transfer-learning-one-task-applying-to-many" data-astro-cid-xvrfupwn>Broad Transfer: Learning One Task, Applying to Many</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#autonomy-setting-goals-and-planning" data-astro-cid-xvrfupwn>Autonomy: Setting Goals and Planning</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#what-current-models-lack-world-models-goals-grounding-ch39" data-astro-cid-xvrfupwn>What Current Models Lack: World Models, Goals, Grounding</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#no-persistent-world-models" data-astro-cid-xvrfupwn>No Persistent World Models</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#no-intrinsic-goals" data-astro-cid-xvrfupwn>No Intrinsic Goals</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#weak-grounding" data-astro-cid-xvrfupwn>Weak Grounding</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#no-causal-reasoning" data-astro-cid-xvrfupwn>No Causal Reasoning</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#no-continual-learning" data-astro-cid-xvrfupwn>No Continual Learning</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-llms-are-not-agi-prediction-agency-ch39" data-astro-cid-xvrfupwn>Why LLMs Are Not AGI: Prediction ‚â† Agency</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#llms-are-prediction-engines" data-astro-cid-xvrfupwn>LLMs Are Prediction Engines</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#no-planning-no-backtracking" data-astro-cid-xvrfupwn>No Planning, No Backtracking</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#brittleness-and-out-of-distribution-failure" data-astro-cid-xvrfupwn>Brittleness and Out-of-Distribution Failure</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#hallucinations-confidence-without-knowledge" data-astro-cid-xvrfupwn>Hallucinations: Confidence Without Knowledge</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#what-would-change-if-agi-existed-ch39" data-astro-cid-xvrfupwn>What Would Change If AGI Existed</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#autonomous-research" data-astro-cid-xvrfupwn>Autonomous Research</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#recursive-self-improvement" data-astro-cid-xvrfupwn>Recursive Self-Improvement</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#general-purpose-robotics" data-astro-cid-xvrfupwn>General-Purpose Robotics</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#economic-disruption" data-astro-cid-xvrfupwn>Economic Disruption</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#safety-challenges" data-astro-cid-xvrfupwn>Safety Challenges</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch39" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#agi-is-not-imminentcurrent-models-lack-fundamental-capabilities" data-astro-cid-xvrfupwn>AGI is not imminent‚Äîcurrent models lack fundamental capabilities</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#narrow-ai-dominates-for-yearstask-specific-systems-outperform-general-systems-economically" data-astro-cid-xvrfupwn>Narrow AI dominates for years‚Äîtask-specific systems outperform general systems economically</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#transfer-learning--general-intelligencemodels-transfer-within-domains-not-across-fundamentally-different-tasks" data-astro-cid-xvrfupwn>Transfer learning ‚â† general intelligence‚Äîmodels transfer within domains, not across fundamentally different tasks</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#agency-requires-new-architecturesllms-are-reactive-agi-needs-goal-setting-and-planning" data-astro-cid-xvrfupwn>Agency requires new architectures‚ÄîLLMs are reactive, AGI needs goal-setting and planning</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#safety-research-necessary-noweven-non-agi-systems-cause-harm-prepare-before-agi-arrives" data-astro-cid-xvrfupwn>Safety research necessary now‚Äîeven non-AGI systems cause harm; prepare before AGI arrives</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#hype-obscures-progresscalling-gpt-4-agi-confuses-prediction-with-understanding" data-astro-cid-xvrfupwn>Hype obscures progress‚Äîcalling GPT-4 ‚ÄúAGI‚Äù confuses prediction with understanding</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#engineering-focusbuild-reliable-narrow-systems-dont-wait-for-agi-to-solve-problems" data-astro-cid-xvrfupwn>Engineering focus‚Äîbuild reliable, narrow systems; don‚Äôt wait for AGI to solve problems</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch39" data-astro-cid-xvrfupwn>References and Further Reading</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#on-the-measure-of-intelligence---chollet-2019-google" data-astro-cid-xvrfupwn>On the Measure of Intelligence - Chollet (2019), Google</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#reward-is-enough---silver-et-al-2021-deepmind" data-astro-cid-xvrfupwn>Reward is Enough - Silver et al. (2021), DeepMind</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#the-bitter-lesson---sutton-2019-essay" data-astro-cid-xvrfupwn>The Bitter Lesson - Sutton (2019), Essay</a></li></ul></nav> </aside> </div>   </body> </html>