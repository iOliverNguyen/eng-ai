<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 37: Multimodal Models | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part8/37-multimodal-models/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part8/37-multimodal-models/"><meta property="og:title" content="Chapter 37: Multimodal Models | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part8/37-multimodal-models/"><meta name="twitter:title" content="Chapter 37: Multimodal Models | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part8" data-astro-cid-ilhxcym7>Part VIII: The Frontier</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Multimodal Models</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-37-multimodal-models">Chapter 37: Multimodal Models</h1>
<p>For decades, AI systems were specialists. Computer vision models recognized images. Speech recognition models transcribed audio. Language models generated text. Each modality‚Äîvision, audio, language‚Äîhad its own architecture, its own training pipeline, its own community of researchers. The models did not talk to each other. A vision model could classify an image as ‚Äúdog,‚Äù but it could not explain why. A language model could write about dogs, but it could not see them.</p>
<p>This changed with the realization that all modalities can be represented as <strong>tokens</strong>‚Äîdiscrete units processed by the same Transformer architecture. Images are patches. Audio is spectrograms. Text is words. All become sequences fed into a unified model. In 2021, OpenAI released CLIP, a model that learned to align images and text by training on 400 million image-caption pairs scraped from the internet. CLIP did not require carefully labeled data‚Äîjust images with associated text, mined from the web. This weak supervision, at scale, enabled zero-shot transfer: CLIP could classify images it had never seen during training, guided only by text descriptions.</p>
<p>CLIP was the turning point. After CLIP came Flamingo (DeepMind), which interleaved images and text for few-shot visual question answering. Then Whisper (OpenAI), which transcribed speech across 97 languages with unprecedented robustness. Then GPT-4V, which analyzed images, charts, and diagrams alongside text. Then Gemini (Google), trained natively on text, images, audio, and video. Multimodal AI is no longer experimental‚Äîit is the default for frontier models.</p>
<p>This chapter explains how multimodal models work, why unifying modalities improves performance, and where current models still fail. Understanding multimodal models is understanding the next generation of AI systems: not text-only assistants, but systems that see, hear, and speak.</p>
<hr>
<h2 id="unifying-modalities-text-vision-audio-as-tokens-ch37">Unifying Modalities: Text, Vision, Audio as Tokens</h2>
<p>The Transformer architecture processes sequences of tokens. Originally designed for text, Transformers work on any sequence: image patches, audio frames, video clips. The key insight: <strong>everything can be tokenized</strong>.</p>
<h3 id="text-tokenization"><strong>Text Tokenization</strong></h3>
<p>Text is already discrete. Break sentences into words, subwords (BPE, SentencePiece), or characters. Each token maps to an embedding vector. The Transformer processes these embeddings.</p>
<p>Example: ‚ÄúThe cat sat‚Äù ‚Üí tokens <code>["The", " cat", " sat"]</code> ‚Üí embeddings ‚Üí Transformer.</p>
<h3 id="vision-tokenization-vision-transformer-vit"><strong>Vision Tokenization (Vision Transformer, ViT)</strong></h3>
<p>Images are continuous 2D grids of pixels. To tokenize:</p>
<ol>
<li>Divide the image into <strong>patches</strong> (e.g., 16√ó16 pixel squares)</li>
<li>Flatten each patch into a vector</li>
<li>Project each vector into an embedding</li>
<li>Treat the sequence of patch embeddings as tokens</li>
</ol>
<p>Example: 224√ó224 image divided into 14√ó14 = 196 patches of 16√ó16 pixels. Each patch becomes a token. The Transformer processes 196 tokens representing the image.</p>
<p><strong>Why this works:</strong> Patches capture local structure (edges, textures, objects), and the Transformer‚Äôs self-attention learns spatial relationships between patches. After training on millions of images, ViT matches or beats convolutional neural networks (CNNs) on image classification.</p>
<h3 id="audio-tokenization"><strong>Audio Tokenization</strong></h3>
<p>Audio is a continuous waveform. To tokenize:</p>
<ol>
<li>Convert waveform to <strong>spectrogram</strong> (frequency representation over time)</li>
<li>Divide spectrogram into time slices (e.g., 20ms windows)</li>
<li>Treat each time slice as a token embedding</li>
</ol>
<p>Alternatively, encode raw audio waveforms with a learned encoder (Wav2Vec, HuBERT) that outputs discrete tokens.</p>
<p>Example: 10-second audio clip ‚Üí 500 spectrogram frames ‚Üí 500 tokens ‚Üí Transformer.</p>
<p>Whisper (OpenAI‚Äôs speech recognition model) uses this approach: audio ‚Üí log-mel spectrogram ‚Üí Transformer encoder ‚Üí text tokens (transcription).</p>
<h3 id="unified-architecture"><strong>Unified Architecture</strong></h3>
<p>Once all modalities are tokenized, the same Transformer architecture processes them. Text tokens, image patch tokens, and audio tokens all flow through self-attention and feedforward layers. The model learns representations that bridge modalities.</p>
<p><strong>Key advantage:</strong> Unified models leverage data from multiple modalities. A model that learns from both text and images develops richer representations than a text-only or vision-only model. Language grounds vision; vision grounds language.</p>
<hr>
<h2 id="cross-modal-grounding-why-meaning-becomes-richer-ch37">Cross-Modal Grounding: Why Meaning Becomes Richer</h2>
<p>Language models trained on text alone learn statistical patterns: which words follow which, which phrases sound natural. But text is disconnected from the physical world. The model reads ‚Äúred ball‚Äù without seeing red or round. It predicts ‚Äúgravity pulls objects down‚Äù without understanding forces or motion.</p>
<p><strong>Grounding</strong> means connecting language to perception. Multimodal models learn associations between words and sensory inputs: ‚Äúred‚Äù linked to red pixels, ‚Äúball‚Äù linked to circular shapes. This grounding improves generalization and enables new capabilities.</p>
<h3 id="clip-contrastive-language-image-pretraining"><strong>CLIP: Contrastive Language-Image Pretraining</strong></h3>
<p>CLIP (OpenAI, 2021) trained two encoders‚Äîone for images, one for text‚Äîto align in a shared embedding space.</p>
<p><strong>Training process:</strong></p>
<ol>
<li>Collect 400 million (image, text) pairs from the internet
<ul>
<li>Images with captions, alt-text, surrounding text</li>
</ul>
</li>
<li>Encode each image with a vision encoder (ViT)</li>
<li>Encode each caption with a text encoder (Transformer)</li>
<li>Compute similarity between image and text embeddings (cosine similarity)</li>
<li>Optimize <strong>contrastive loss</strong>:
<ul>
<li>Maximize similarity for correct (image, caption) pairs</li>
<li>Minimize similarity for incorrect pairs</li>
</ul>
</li>
</ol>
<p><strong>Contrastive loss forces alignment:</strong> If an image shows a dog, its embedding should be close to ‚Äúa photo of a dog‚Äù and far from ‚Äúa photo of a cat.‚Äù After training on 400M pairs, CLIP learns to associate visual patterns with language descriptions.</p>
<p><strong>Zero-shot transfer:</strong></p>
<p>CLIP enables zero-shot image classification without fine-tuning. To classify an image into categories <code>{cat, dog, car}</code>:</p>
<ol>
<li>Encode the image ‚Üí embedding <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span></li>
<li>Encode text prompts: ‚Äúa photo of a cat‚Äù, ‚Äúa photo of a dog‚Äù, ‚Äúa photo of a car‚Äù ‚Üí embeddings <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">t_1, t_2, t_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li>Compute similarity: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>sim</mtext><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{sim}(v, t_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">sim</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> for each category</li>
<li>Predict the category with highest similarity</li>
</ol>
<p>CLIP achieves competitive accuracy on ImageNet (image classification) despite <strong>never being trained on ImageNet labels</strong>. It generalizes to new tasks using language as the interface.</p>
<p><strong>Why grounding matters:</strong></p>
<p>CLIP‚Äôs text encoder learns richer representations than a text-only model. Because it is trained to align with images, it learns that ‚Äúred‚Äù relates to color, ‚Äúball‚Äù relates to shape, ‚Äúdog‚Äù relates to furry four-legged animals. These visual associations improve language understanding.</p>
<p>Conversely, CLIP‚Äôs vision encoder learns richer representations than a vision-only model. Because it is trained to align with text, it learns high-level semantic concepts (‚Äúdog,‚Äù ‚Äúrunning,‚Äù ‚Äúoutdoors‚Äù) instead of just low-level features (edges, textures).</p>
<h3 id="flamingo-interleaving-images-and-text"><strong>Flamingo: Interleaving Images and Text</strong></h3>
<p>Flamingo (DeepMind, 2022) extended multimodal learning to <strong>few-shot visual question answering</strong>. Given a sequence of interleaved images and text, Flamingo answers questions about the images.</p>
<p><strong>Architecture:</strong></p>
<ul>
<li>Frozen vision encoder (processes images ‚Üí embeddings)</li>
<li>Language model (processes text tokens)</li>
<li><strong>Cross-attention layers</strong> connecting vision and language
<ul>
<li>Language model attends to image embeddings when generating text</li>
<li>‚ÄúWhat is in this image?‚Äù ‚Üí model looks at image embeddings ‚Üí generates caption</li>
</ul>
</li>
</ul>
<p><strong>Few-shot learning:</strong></p>
<p>Flamingo can learn new tasks from a few examples in context. Provide 2-3 (image, question, answer) examples, then ask a new question about a new image. The model generalizes from the in-context examples‚Äîsimilar to GPT-3‚Äôs few-shot learning, but for vision.</p>
<p><strong>Why this matters:</strong></p>
<p>Flamingo shows that multimodal models can <strong>reason visually</strong> using language as scaffolding. Language guides attention: ‚ÄúWhat color is the car?‚Äù directs the model to look at the car region and extract color information. Perception and language work together.</p>
<hr>
<h2 id="perception-language-how-world-models-form-ch37">Perception + Language: How World Models Form</h2>
<p>Multimodal models begin to form <strong>world models</strong>‚Äîinternal representations of objects, scenes, and their relationships. These models are not explicit 3D simulations, but statistical associations learned from data.</p>
<h3 id="object-recognition-and-localization"><strong>Object Recognition and Localization</strong></h3>
<p>A model trained on images and captions learns to associate words with visual regions:</p>
<ul>
<li>‚ÄúDog‚Äù ‚Üí furry four-legged object</li>
<li>‚ÄúCar‚Äù ‚Üí rectangular object with wheels</li>
<li>‚ÄúTree‚Äù ‚Üí green vertical structure with branches</li>
</ul>
<p>GPT-4V (GPT-4 with vision) can describe images: ‚ÄúA golden retriever sitting on grass in a park.‚Äù It identifies objects (dog, grass), attributes (golden, sitting), and context (park). This requires recognizing objects and understanding their relationships.</p>
<h3 id="spatial-understanding-limited"><strong>Spatial Understanding (Limited)</strong></h3>
<p>Current multimodal models struggle with spatial reasoning:</p>
<ul>
<li><strong>Counting</strong>: ‚ÄúHow many apples are in the image?‚Äù Often inaccurate</li>
<li><strong>Depth perception</strong>: Cannot reliably estimate distance between objects</li>
<li><strong>3D structure</strong>: Struggle with occluded objects, viewpoint changes</li>
<li><strong>Physical reasoning</strong>: Do not understand gravity, support, balance</li>
</ul>
<p>Example: Show an image of a stack of blocks. Ask: ‚ÄúIf I remove the middle block, what happens?‚Äù Humans know the top blocks fall. Models struggle‚Äîthey lack physics understanding.</p>
<h3 id="temporal-understanding-very-limited"><strong>Temporal Understanding (Very Limited)</strong></h3>
<p>Video models process sequences of frames, but temporal reasoning remains weak:</p>
<ul>
<li><strong>Event detection</strong>: Models detect ‚Äúa person runs‚Äù but struggle with ‚Äúa person starts running, then stops‚Äù</li>
<li><strong>Long-term dynamics</strong>: Cannot track objects across many frames reliably</li>
<li><strong>Causality</strong>: Do not understand which events cause which</li>
</ul>
<h3 id="why-world-models-are-still-weak"><strong>Why World Models Are Still Weak</strong></h3>
<p>Multimodal models learn correlations from data, not causal mechanisms. They see millions of images of dogs, learn that ‚Äúdog‚Äù correlates with certain pixel patterns, but do not understand what a dog is‚Äîan animal with biology, behavior, needs. Language grounds perception statistically, but not conceptually.</p>
<p><strong>What‚Äôs missing:</strong></p>
<ul>
<li><strong>Physical interaction</strong>: Models observe images/videos but do not interact with objects</li>
<li><strong>Embodiment</strong>: No body, no sensors, no motor control</li>
<li><strong>Long-term memory</strong>: No persistent memory across conversations/sessions</li>
<li><strong>Causal models</strong>: Learn <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">‚à£</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(Y | X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">‚à£</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span> but not ‚ÄúX causes Y‚Äù</li>
</ul>
<p>Multimodal models are progress toward world models, but far from human-like understanding.</p>
<hr>
<h2 id="limitations-why-sensory-understanding-is-still-weak-ch37">Limitations: Why Sensory Understanding Is Still Weak</h2>
<p>Despite impressive capabilities, multimodal models have fundamental limitations:</p>
<h3 id="data-efficiency-gap"><strong>Data Efficiency Gap</strong></h3>
<p>CLIP trained on 400 million image-text pairs. Humans learn object recognition from dozens of examples. A child sees ‚Äúdog‚Äù 10-20 times and generalizes to all dogs. Models need millions of examples for comparable generalization. This inefficiency suggests models are not learning the same way humans do‚Äîthey memorize statistical patterns, not concepts.</p>
<h3 id="fragile-generalization"><strong>Fragile Generalization</strong></h3>
<p>Multimodal models generalize within their training distribution but fail on out-of-distribution inputs:</p>
<ul>
<li>CLIP trained mostly on photos ‚Üí struggles with sketches, paintings, abstract art</li>
<li>GPT-4V trained on natural images ‚Üí struggles with medical scans, satellite imagery</li>
<li>Whisper trained on speech ‚Üí struggles with music, environmental sounds, accents far from training data</li>
</ul>
<p>Humans transfer knowledge across domains easily. Models do not.</p>
<h3 id="lack-of-common-sense"><strong>Lack of Common Sense</strong></h3>
<p>Show an image of a person holding an umbrella indoors on a sunny day. Ask: ‚ÄúIs this unusual?‚Äù Humans immediately recognize the inconsistency. Models struggle‚Äîthey lack common sense about when umbrellas are used, what ‚Äúindoors‚Äù means contextually, and what makes a situation unusual.</p>
<h3 id="inference-costs"><strong>Inference Costs</strong></h3>
<p>Processing images is <strong>10-100x more expensive</strong> than processing text.</p>
<ul>
<li>Text token: ~1 embedding lookup, ~1K FLOPs per layer</li>
<li>Image patch token: ~1 embedding projection, ~1K FLOPs per layer, but images have <strong>196-256 tokens per image</strong></li>
</ul>
<p>Analyzing a single image costs as much as processing 200-500 words of text. For video (30 frames/second), costs explode: 10 seconds of video = 300 frames = 60,000 tokens = equivalent to processing 30,000 words.</p>
<p><strong>Practical implications:</strong></p>
<p>GPT-4V is expensive to run. Analyzing a 10-image document costs 10x a text-only query. Applications must balance functionality and cost: where is vision worth 10-100x more compute?</p>
<h3 id="alignment-is-harder"><strong>Alignment Is Harder</strong></h3>
<p>Multimodal models inherit text model alignment challenges (hallucinations, bias) and add new ones:</p>
<ul>
<li><strong>Visual hallucinations</strong>: Describing objects not in the image</li>
<li><strong>Misidentification</strong>: Confusing visually similar objects</li>
<li><strong>Cultural bias</strong>: Models trained on Western images struggle with non-Western contexts</li>
</ul>
<p>Aligning multimodal models requires human feedback on vision tasks‚Äîmore expensive than text-only feedback (humans must review images, not just text).</p>
<hr>
<h2 id="engineering-takeaway-ch37">Engineering Takeaway</h2>
<h3 id="token-abstraction-enables-unificationsame-architecture-handles-all-modalities"><strong>Token abstraction enables unification‚Äîsame architecture handles all modalities</strong></h3>
<p>Tokenizing images, audio, and text into sequences allows a single Transformer to process all modalities. This engineering win simplifies model architecture: no need for separate vision networks, audio networks, language networks. One architecture, multiple modalities. This unification accelerates research and deployment‚Äîimprovements to Transformers benefit all modalities simultaneously.</p>
<h3 id="contrastive-learning-scalesweak-supervision-beats-careful-labeling"><strong>Contrastive learning scales‚Äîweak supervision beats careful labeling</strong></h3>
<p>CLIP trained on 400 million (image, text) pairs scraped from the internet. No manual labeling, no curated datasets‚Äîjust whatever images and text co-occur on the web. Weak supervision at scale beats careful labeling at small scale. This lesson generalizes: use massive noisy data, not small clean data. Scale compensates for noise.</p>
<h3 id="grounding-improves-generalizationmultimodal-models-transfer-better-than-text-only"><strong>Grounding improves generalization‚Äîmultimodal models transfer better than text-only</strong></h3>
<p>Models that learn from both text and vision develop richer representations. Language grounds vision (semantic concepts), vision grounds language (perceptual meaning). CLIP‚Äôs text encoder outperforms text-only models on certain NLP tasks because it has visual grounding. Multimodal training improves all modalities, not just the multimodal tasks.</p>
<h3 id="inference-costs-multiplyimages-are-10-100x-more-expensive-than-text"><strong>Inference costs multiply‚Äîimages are 10-100x more expensive than text</strong></h3>
<p>Analyzing one image costs as much as processing 200-500 words. Video is worse: 10 seconds = 30,000 words equivalent. Applications must justify the cost. When is vision worth 10x more compute? Document understanding (analyze receipts, forms), visual QA (customer support with screenshots), image generation. But for text-only tasks, adding vision is wasteful.</p>
<h3 id="data-alignment-is-the-bottleneckpaired-multimodal-data-is-scarcer-than-text-alone"><strong>Data alignment is the bottleneck‚Äîpaired multimodal data is scarcer than text alone</strong></h3>
<p>Text-only data is abundant: web pages, books, articles, trillions of tokens. Image-text pairs are scarcer: need images with captions or alt-text. High-quality pairs (descriptive captions, not just ‚Äúimage.jpg‚Äù) are rarer still. Video-text alignment is even scarcer. Collecting and cleaning paired multimodal data is a major engineering challenge. This limits how far multimodal models can scale with current methods.</p>
<h3 id="applications-become-richerdocument-understanding-visual-assistants-video-analysis"><strong>Applications become richer‚Äîdocument understanding, visual assistants, video analysis</strong></h3>
<p>Multimodal models enable new applications:</p>
<ul>
<li><strong>Document understanding</strong>: Analyze receipts, invoices, forms with mixed text and images</li>
<li><strong>Visual assistants</strong>: Answer questions about screenshots, charts, diagrams</li>
<li><strong>Medical imaging</strong>: Describe X-rays, MRIs, pathology slides</li>
<li><strong>Accessibility</strong>: Generate captions for images, describe scenes for visually impaired users</li>
<li><strong>Creative tools</strong>: DALL-E (text ‚Üí image), video editing with language commands</li>
</ul>
<p>The future of AI applications is multimodal: not text-only chatbots, but assistants that see, hear, and speak.</p>
<h3 id="gaps-remain-largeno-true-spatial-reasoning-physics-understanding-embodied-grounding"><strong>Gaps remain large‚Äîno true spatial reasoning, physics understanding, embodied grounding</strong></h3>
<p>Despite progress, multimodal models lack fundamental capabilities:</p>
<ul>
<li>Cannot reliably count objects, estimate depths, understand 3D structure</li>
<li>Do not understand physics: gravity, support, collision, causality</li>
<li>Lack embodied grounding: never interact with physical world, never use a body to learn sensorimotor associations</li>
<li>Temporal reasoning weak: struggle with long-term dynamics, event causality</li>
</ul>
<p>These gaps mean multimodal models are powerful pattern recognizers, not true world modelers. They excel at classification, description, retrieval‚Äîbut fail at reasoning, planning, physical understanding.</p>
<hr>
<p><img  src="/eng-ai/_astro/37-diagram.BhYGGeIT_4OrnG.svg" alt="Engineering Takeaway diagram" width="800" height="500" loading="lazy" decoding="async"></p>
<hr>
<h2 id="references-and-further-reading-ch37">References and Further Reading</h2>
<h3 id="learning-transferable-visual-models-from-natural-language-supervision-clip---radford-et-al-2021-openai"><strong>Learning Transferable Visual Models From Natural Language Supervision (CLIP)</strong> - Radford et al. (2021), OpenAI</h3>
<p><strong>Why it matters:</strong> CLIP revolutionized computer vision by showing that language can supervise vision at scale. Instead of training on carefully labeled datasets like ImageNet (1M images, 1000 classes), CLIP trained on 400 million (image, text) pairs scraped from the internet. No manual labeling‚Äîjust whatever images and captions co-occur on the web. Contrastive learning aligned vision and text encoders in a shared embedding space, enabling zero-shot transfer: CLIP classifies images into categories it never saw during training, guided only by text descriptions. This approach generalized better than supervised learning and enabled applications like text-to-image generation (DALL-E uses CLIP), visual question answering, and image search. CLIP changed computer vision from task-specific models to general-purpose multimodal models.</p>
<h3 id="flamingo-a-visual-language-model-for-few-shot-learning---alayrac-et-al-2022-deepmind"><strong>Flamingo: A Visual Language Model for Few-Shot Learning</strong> - Alayrac et al. (2022), DeepMind</h3>
<p><strong>Why it matters:</strong> Flamingo demonstrated that multimodal models can perform few-shot visual reasoning‚Äîlearning new tasks from a handful of in-context examples, like GPT-3 for vision. Given 2-3 examples of (image, question, answer) tuples, Flamingo answers questions about new images. The key innovation: <strong>cross-attention layers</strong> connecting a frozen vision encoder and a language model. The language model attends to visual features when generating text, enabling perception-guided language generation. Flamingo showed that multimodal models are not just better at classification‚Äîthey can reason, generalize, and solve novel tasks with minimal examples. This set the stage for GPT-4V and other vision-language models that combine perception and reasoning.</p>
<h3 id="robust-speech-recognition-via-large-scale-weak-supervision-whisper---radford-et-al-2022-openai"><strong>Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)</strong> - Radford et al. (2022), OpenAI</h3>
<p><strong>Why it matters:</strong> Whisper trained on 680,000 hours of audio-text pairs scraped from the web, covering 97 languages. No careful curation‚Äîjust massive scale and weak supervision. Result: Whisper transcribes speech more robustly than models trained on carefully labeled datasets. It handles accents, background noise, code-switching (mixing languages), and domain shifts (podcasts, phone calls, lectures) better than supervised models. The lesson: weak supervision at scale beats strong supervision at small scale. Whisper also demonstrated that multimodal architectures (audio encoder ‚Üí text decoder) transfer well across languages and domains. It became the de facto standard for speech recognition, powering accessibility tools, transcription services, and voice interfaces. Whisper showed that multimodal learning is not just for vision‚Äîaudio-text alignment follows the same principles.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part8/36-scaling-laws" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Scaling Laws - Why Bigger Keeps Winning</span> </a> <a href="/eng-ai/part8/38-self-improving-systems" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Self-Improving Systems</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#unifying-modalities-text-vision-audio-as-tokens-ch37" data-astro-cid-xvrfupwn>Unifying Modalities: Text, Vision, Audio as Tokens</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#text-tokenization" data-astro-cid-xvrfupwn>Text Tokenization</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#vision-tokenization-vision-transformer-vit" data-astro-cid-xvrfupwn>Vision Tokenization (Vision Transformer, ViT)</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#audio-tokenization" data-astro-cid-xvrfupwn>Audio Tokenization</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#unified-architecture" data-astro-cid-xvrfupwn>Unified Architecture</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#cross-modal-grounding-why-meaning-becomes-richer-ch37" data-astro-cid-xvrfupwn>Cross-Modal Grounding: Why Meaning Becomes Richer</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#clip-contrastive-language-image-pretraining" data-astro-cid-xvrfupwn>CLIP: Contrastive Language-Image Pretraining</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#flamingo-interleaving-images-and-text" data-astro-cid-xvrfupwn>Flamingo: Interleaving Images and Text</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#perception-language-how-world-models-form-ch37" data-astro-cid-xvrfupwn>Perception + Language: How World Models Form</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#object-recognition-and-localization" data-astro-cid-xvrfupwn>Object Recognition and Localization</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#spatial-understanding-limited" data-astro-cid-xvrfupwn>Spatial Understanding (Limited)</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#temporal-understanding-very-limited" data-astro-cid-xvrfupwn>Temporal Understanding (Very Limited)</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#why-world-models-are-still-weak" data-astro-cid-xvrfupwn>Why World Models Are Still Weak</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#limitations-why-sensory-understanding-is-still-weak-ch37" data-astro-cid-xvrfupwn>Limitations: Why Sensory Understanding Is Still Weak</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#data-efficiency-gap" data-astro-cid-xvrfupwn>Data Efficiency Gap</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#fragile-generalization" data-astro-cid-xvrfupwn>Fragile Generalization</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#lack-of-common-sense" data-astro-cid-xvrfupwn>Lack of Common Sense</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#inference-costs" data-astro-cid-xvrfupwn>Inference Costs</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#alignment-is-harder" data-astro-cid-xvrfupwn>Alignment Is Harder</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch37" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#token-abstraction-enables-unificationsame-architecture-handles-all-modalities" data-astro-cid-xvrfupwn>Token abstraction enables unification‚Äîsame architecture handles all modalities</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#contrastive-learning-scalesweak-supervision-beats-careful-labeling" data-astro-cid-xvrfupwn>Contrastive learning scales‚Äîweak supervision beats careful labeling</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#grounding-improves-generalizationmultimodal-models-transfer-better-than-text-only" data-astro-cid-xvrfupwn>Grounding improves generalization‚Äîmultimodal models transfer better than text-only</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#inference-costs-multiplyimages-are-10-100x-more-expensive-than-text" data-astro-cid-xvrfupwn>Inference costs multiply‚Äîimages are 10-100x more expensive than text</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#data-alignment-is-the-bottleneckpaired-multimodal-data-is-scarcer-than-text-alone" data-astro-cid-xvrfupwn>Data alignment is the bottleneck‚Äîpaired multimodal data is scarcer than text alone</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#applications-become-richerdocument-understanding-visual-assistants-video-analysis" data-astro-cid-xvrfupwn>Applications become richer‚Äîdocument understanding, visual assistants, video analysis</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#gaps-remain-largeno-true-spatial-reasoning-physics-understanding-embodied-grounding" data-astro-cid-xvrfupwn>Gaps remain large‚Äîno true spatial reasoning, physics understanding, embodied grounding</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch37" data-astro-cid-xvrfupwn>References and Further Reading</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#learning-transferable-visual-models-from-natural-language-supervision-clip---radford-et-al-2021-openai" data-astro-cid-xvrfupwn>Learning Transferable Visual Models From Natural Language Supervision (CLIP) - Radford et al. (2021), OpenAI</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#flamingo-a-visual-language-model-for-few-shot-learning---alayrac-et-al-2022-deepmind" data-astro-cid-xvrfupwn>Flamingo: A Visual Language Model for Few-Shot Learning - Alayrac et al. (2022), DeepMind</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#robust-speech-recognition-via-large-scale-weak-supervision-whisper---radford-et-al-2022-openai" data-astro-cid-xvrfupwn>Robust Speech Recognition via Large-Scale Weak Supervision (Whisper) - Radford et al. (2022), OpenAI</a></li></ul></nav> </aside> </div>   </body> </html>