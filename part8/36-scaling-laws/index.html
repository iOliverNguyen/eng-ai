<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 36: Scaling Laws - Why Bigger Keeps Winning | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part8/36-scaling-laws/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part8/36-scaling-laws/"><meta property="og:title" content="Chapter 36: Scaling Laws - Why Bigger Keeps Winning | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part8/36-scaling-laws/"><meta name="twitter:title" content="Chapter 36: Scaling Laws - Why Bigger Keeps Winning | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part8" data-astro-cid-ilhxcym7>Part VIII: The Frontier</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Scaling Laws - Why Bigger Keeps Winning</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-36-scaling-laws---why-bigger-keeps-winning">Chapter 36: Scaling Laws - Why Bigger Keeps Winning</h1>
<p>In 2020, researchers at OpenAI published a paper that changed how the AI industry thinks about progress. They trained hundreds of language models, varying the number of parameters from 768 to 1.5 billion, the dataset size from 22 million to 23 billion tokens, and the compute budget across six orders of magnitude. They measured the loss‚Äîhow well each model predicted text‚Äîand discovered something remarkable: performance followed smooth, predictable power laws. Bigger models, trained on more data with more compute, consistently did better. And the relationship wasn‚Äôt just consistent‚Äîit was mathematically precise across massive scale ranges.</p>
<p>This discovery transformed AI development from trial-and-error to engineering. Before scaling laws, researchers tried different architectures, hoping for breakthroughs. After scaling laws, the industry realized: scale itself is the breakthrough. GPT-3‚Äôs 175 billion parameters, trained on 300 billion tokens, achieved capabilities that smaller models could not. GPT-4, rumored to be trained with 10-100x more compute, pushed further. The frontier of AI is not algorithmic cleverness‚Äîit is infrastructure: bigger clusters, more GPUs, larger datasets, months of training.</p>
<p>This chapter explains why scale drives progress, what that means for AI development, and why infrastructure has become destiny. Scaling laws make the future predictable‚Äîwithin the explored range. But they also reveal limits: each generation requires 10x more resources for incremental gains. Understanding scaling laws is understanding why AI is accelerating and where the limits lie.</p>
<hr>
<h2 id="power-laws-how-performance-grows-with-compute-ch36">Power Laws: How Performance Grows with Compute</h2>
<p>In most engineering systems, performance plateaus. You add more resources, and gains diminish to nothing. Double the effort, get 10% improvement. Eventually, returns vanish. But language models are different. Performance improves smoothly as models get bigger, and the relationship follows a power law: a straight line on a log-log plot.</p>
<p><strong>The scaling law for model size:</strong></p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo><mo>‚àù</mo><msup><mi>N</mi><mrow><mo>‚àí</mo><mi>Œ±</mi></mrow></msup></mrow><annotation encoding="application/x-tex">L(N) \propto N^{-\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àù</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7713em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚àí</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">Œ±</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span> is loss (cross-entropy), <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> is the number of parameters, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span></span></span></span> is the scaling exponent (empirically around 0.076 for language models). This says: if you increase model size by 10x, loss decreases by roughly <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mn>0.076</mn></msup><mo>‚âà</mo><mn>1.19</mn><mi>x</mi></mrow><annotation encoding="application/x-tex">10^{0.076} \approx 1.19x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0.076</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.19</span><span class="mord mathnormal">x</span></span></span></span>. On a log-log plot, this is a straight line with slope <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚àí</mo><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">-\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">‚àí</span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span></span></span></span>.</p>
<p><strong>What this means in practice:</strong></p>
<ul>
<li><strong>GPT-1 (117M parameters)</strong>: Loss ~3.3</li>
<li><strong>GPT-2 (1.5B parameters)</strong>: Loss ~2.5 (13x larger, loss reduced ~25%)</li>
<li><strong>GPT-3 (175B parameters)</strong>: Loss ~2.0 (117x larger than GPT-2, loss reduced another ~20%)</li>
<li><strong>GPT-4 (estimated 1.7T parameters)</strong>: Loss unknown but likely ~1.7-1.8 (10x larger, loss reduced ~15-20%)</li>
</ul>
<p>The power law holds across six orders of magnitude in model size. This is extraordinary: most systems break down at scale, but language models get predictably better.</p>
<p><strong>Why power laws matter:</strong></p>
<p>Power laws enable forecasting. Before training GPT-4, OpenAI could estimate its loss based on compute budget and model size. This turns AI development into an optimization problem: how much compute should we invest, and how should we allocate it between model size and training data? Scaling laws answer this quantitatively.</p>
<p><strong>But power laws are not limitless.</strong></p>
<p>The power law formula <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>‚àù</mo><msup><mi>N</mi><mrow><mo>‚àí</mo><mi>Œ±</mi></mrow></msup></mrow><annotation encoding="application/x-tex">L \propto N^{-\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àù</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7713em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚àí</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">Œ±</span></span></span></span></span></span></span></span></span></span></span></span> implies diminishing returns. Reducing loss from 3.0 to 2.0 requires 10x scale. Reducing from 2.0 to 1.0 requires 100x scale. Reducing from 1.0 to 0.5 requires 1000x scale. Each improvement is harder than the last. Eventually, physical limits‚Äîenergy, cost, available data‚Äîconstrain further scaling.</p>
<p><strong>The Chinchilla surprise:</strong></p>
<p>In 2022, DeepMind published a follow-up study that revised the scaling laws. They found that most models were <strong>undertrained</strong>: too many parameters, not enough training data. OpenAI‚Äôs original scaling laws optimized for a fixed compute budget, but they underweighted data. DeepMind trained Chinchilla, a 70 billion parameter model, on 1.4 trillion tokens‚Äî4x more data than Gopher, a 280 billion parameter model trained on 300 billion tokens. Result: Chinchilla matched or exceeded Gopher‚Äôs performance despite being 4x smaller.</p>
<p><strong>The revised scaling law:</strong></p>
<p>Optimal compute allocation should balance model size and data roughly equally. If you double compute, increase both model size and training data by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mn>2</mn></msqrt><mo>‚âà</mo><mn>1.4</mn><mi>x</mi></mrow><annotation encoding="application/x-tex">\sqrt{2} \approx 1.4x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1328em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">2</span></span></span><span style="top:-2.8672em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.4</span><span class="mord mathnormal">x</span></span></span></span>. This means previous models (GPT-3, Gopher) were too large for their training data. A smaller model, trained longer, performs better.</p>
<p><strong>Why this matters:</strong></p>
<p>Chinchilla showed that <strong>algorithmic improvements still matter</strong>. Scale is powerful, but how you allocate compute is equally important. GPT-3 trained on 300B tokens; GPT-4 likely trained on 10-100T tokens. This reallocation‚Äîmore data, proportionally scaled model size‚Äîexplains some of GPT-4‚Äôs improvements without requiring pure parameter scaling.</p>
<hr>
<h2 id="data-model-compute-the-three-levers-ch36">Data, Model, Compute: The Three Levers</h2>
<p>Scaling laws reveal three levers for improving model performance:</p>
<p><strong>1. Model Parameters (N)</strong></p>
<p>The number of trainable weights in the model. More parameters = more capacity to memorize patterns and represent complex functions.</p>
<ul>
<li><strong>GPT-2</strong>: 1.5 billion parameters</li>
<li><strong>GPT-3</strong>: 175 billion parameters (117x larger)</li>
<li><strong>GPT-4</strong>: Estimated 1-1.7 trillion parameters (6-10x larger than GPT-3)</li>
<li><strong>Llama 3.1</strong>: 405 billion parameters (largest open model as of 2024)</li>
</ul>
<p>Increasing parameters requires more memory (GPU RAM or distributed across many GPUs), more compute per forward pass (matrix multiplications scale with parameters), and more time to train.</p>
<p><strong>Example:</strong> GPT-3 (175B parameters) requires ~700GB of memory in FP16 precision (2 bytes per parameter). Training requires thousands of GPUs for months.</p>
<p><strong>2. Training Data (D)</strong></p>
<p>The number of tokens (words, subwords) the model sees during training. More data = more examples to learn from, better generalization.</p>
<ul>
<li><strong>GPT-2</strong>: ~40GB of text (web scrape)</li>
<li><strong>GPT-3</strong>: 300 billion tokens (~600GB of text, filtered from Common Crawl)</li>
<li><strong>GPT-4</strong>: Unknown, but likely 10-100 trillion tokens</li>
<li><strong>LLaMA 2</strong>: 2 trillion tokens (curated mix of web data, books, code)</li>
</ul>
<p>Collecting and cleaning data is non-trivial. Common Crawl contains billions of web pages, but most are low-quality: spam, duplicates, non-English, generated text. Filtering and deduplicating data is an engineering challenge. The quality of training data determines model quality‚Äîgarbage in, garbage out.</p>
<p><strong>Chinchilla insight:</strong> Most models trained before 2022 were data-starved. Doubling data improves performance as much as quadrupling parameters. Optimal allocation: if compute budget increases 10x, increase model size 3x and data 3x.</p>
<p><strong>3. Compute Budget (C)</strong></p>
<p>Total floating-point operations (FLOPs) used during training. Compute is the product of model size, data size, and training time.</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>‚âà</mo><mn>6</mn><mo>√ó</mo><mi>N</mi><mo>√ó</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">C \approx 6 \times N \times D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span></p>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> is parameters, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span> is tokens, and the factor of 6 accounts for forward and backward passes plus other overheads.</p>
<p><strong>GPT-3 training compute:</strong></p>
<ul>
<li>175B parameters</li>
<li>300B tokens</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>‚âà</mo><mn>6</mn><mo>√ó</mo><mn>175</mn><mo>√ó</mo><msup><mn>10</mn><mn>9</mn></msup><mo>√ó</mo><mn>300</mn><mo>√ó</mo><msup><mn>10</mn><mn>9</mn></msup><mo>=</mo><mn>3.14</mn><mo>√ó</mo><msup><mn>10</mn><mn>23</mn></msup></mrow><annotation encoding="application/x-tex">C \approx 6 \times 175 \times 10^9 \times 300 \times 10^9 = 3.14 \times 10^{23}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">175</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">300</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3.14</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">23</span></span></span></span></span></span></span></span></span></span></span></span> FLOPs</li>
</ul>
<p>At <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">10^{15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span></span></span></span> FLOPs/second per GPU (NVIDIA A100), this is ~31.4 million GPU-seconds, or ~10,000 GPU-days. With 10,000 GPUs, this is ~1 day of compute‚Äîbut training is not perfectly parallelized, so actual training time was likely weeks to months.</p>
<p><strong>Estimated cost:</strong> At <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>‚àí</mo><mn>3</mn><mi mathvariant="normal">/</mi><mi>G</mi><mi>P</mi><mi>U</mi><mo>‚àí</mo><mi>h</mi><mi>o</mi><mi>u</mi><mi>r</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>c</mi><mi>l</mi><mi>o</mi><mi>u</mi><mi>d</mi><mi>c</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi>e</mi><mo separator="true">,</mo><mi>G</mi><mi>P</mi><mi>T</mi><mo>‚àí</mo><mn>3</mn><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi>c</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>p</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>x</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>l</mi><mi>y</mi><mo>‚àó</mo><mo>‚àó</mo></mrow><annotation encoding="application/x-tex">2-3/GPU-hour for cloud compute, GPT-3 training cost approximately **</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3/</span><span class="mord mathnormal" style="margin-right:0.13889em;">GP</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">orc</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">d</span><span class="mord mathnormal">co</span><span class="mord mathnormal">m</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">GPT</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">3</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">ainin</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">cos</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">pp</span><span class="mord mathnormal">ro</span><span class="mord mathnormal">x</span><span class="mord mathnormal">ima</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4653em;"></span><span class="mord">‚àó</span></span></span></span>4-5 million** in compute alone (not counting engineering time, infrastructure, failed runs).</p>
<p><strong>GPT-4 training compute:</strong> Estimated 10-100x more than GPT-3, implying training costs of <strong>$40-500 million</strong>. Only organizations with deep pockets‚ÄîOpenAI (funded by Microsoft), Google, Meta, Anthropic (funded by Google and others)‚Äîcan afford frontier model training.</p>
<p><strong>Energy costs:</strong></p>
<p>Training large models consumes massive energy. GPT-3 training is estimated to have consumed <strong>1,287 MWh</strong> (megawatt-hours) of electricity. For context, the average U.S. household uses ~10 MWh per year. GPT-3 training = 130 households for a year. GPT-4 training likely consumed 10,000+ MWh.</p>
<p>Energy costs and carbon emissions are becoming engineering constraints. Datacenters are limited by power availability. Sustainable AI requires efficient architectures, better hardware (specialized AI chips), and renewable energy sources.</p>
<hr>
<h2 id="diminishing-returns-why-progress-is-predictable-ch36">Diminishing Returns: Why Progress Is Predictable</h2>
<p>Power laws guarantee that scaling improves performance, but they also guarantee diminishing returns. The exponent <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>‚âà</mo><mn>0.076</mn></mrow><annotation encoding="application/x-tex">\alpha \approx 0.076</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4831em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.076</span></span></span></span> means each 10x increase in model size reduces loss by ~19%. To cut loss in half requires 100x more parameters. To cut it in half again requires 10,000x more parameters.</p>
<p><strong>Concrete example:</strong></p>
<p>Start with a 1B parameter model with loss 2.5. To reach loss 1.25 (half), you need a 100B parameter model (100x scale). To reach loss 0.625 (half again), you need a 10,000B = 10 trillion parameter model (100x scale again). Costs compound: compute, memory, energy, data.</p>
<p><strong>What does lower loss buy you?</strong></p>
<p>Loss measures how surprised the model is by the next token. Lower loss = better predictions = higher accuracy on downstream tasks. But the relationship between loss and task performance is not linear. Some tasks benefit enormously from small loss reductions; others plateau.</p>
<p><strong>Example: Math word problems (GSM8K benchmark)</strong></p>
<ul>
<li><strong>Small models (1B params, loss 2.5)</strong>: ~5% accuracy (random guessing)</li>
<li><strong>Medium models (13B params, loss 2.2)</strong>: ~10-20% accuracy</li>
<li><strong>Large models (100B+ params, loss 2.0)</strong>: 40-60% accuracy</li>
<li><strong>Frontier models (1T+ params, loss ~1.8)</strong>: 80-90% accuracy</li>
</ul>
<p>A 20% reduction in loss (2.5 ‚Üí 2.0) yields a 10x improvement in accuracy (5% ‚Üí 50%). But further loss reductions (2.0 ‚Üí 1.8) yield smaller gains (50% ‚Üí 80%). Diminishing returns appear twice: once in scaling compute to reduce loss, again in converting loss to task performance.</p>
<p><strong>Economic implications:</strong></p>
<p>Diminishing returns mean each generation of models is more expensive than the last. GPT-2 trained for ~<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mi>K</mi><mi mathvariant="normal">.</mi><mi>G</mi><mi>P</mi><mi>T</mi><mo>‚àí</mo><mn>3</mn><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>d</mi><mi>f</mi><mi>o</mi><mi>r</mi><mtext>¬†</mtext></mrow><annotation encoding="application/x-tex">50K. GPT-3 trained for ~</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord">50</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.13889em;">GPT</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">3</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">ain</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">df</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mspace nobreak">¬†</span></span></span></span>5M (100x). GPT-4 trained for ~<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mo>‚àí</mo><mn>100</mn><mi>M</mi><mo stretchy="false">(</mo><mn>10</mn><mo>‚àí</mo><mn>20</mn><mi>x</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi><mi>G</mi><mi>P</mi><mi>T</mi><mo>‚àí</mo><mn>5</mn><mi>m</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>c</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">50-100M (10-20x). GPT-5 might cost </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">50</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">100</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord">10</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">20</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.13889em;">GPT</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">5</span><span class="mord mathnormal">mi</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mord mathnormal">cos</span><span class="mord mathnormal">t</span></span></span></span>500M-1B. At some point, ROI (return on investment) becomes unfavorable. Spending $1B to improve accuracy from 90% to 95% is not worthwhile for most applications.</p>
<p><strong>Practical constraints:</strong></p>
<ul>
<li><strong>Compute availability</strong>: Tens of thousands of GPUs are not easy to procure or manage</li>
<li><strong>Energy</strong>: Training GPT-4 requires a small power plant‚Äôs worth of electricity</li>
<li><strong>Data</strong>: High-quality text is finite; models risk running out of unique, valuable data</li>
<li><strong>Time</strong>: Training takes months; faster iteration beats marginal performance gains</li>
</ul>
<p>These constraints mean scaling will slow. The industry will shift focus from pure scale to efficiency: better architectures, better data, better training methods.</p>
<hr>
<h2 id="emergent-abilities-why-new-skills-appear-suddenly-ch36">Emergent Abilities: Why New Skills Appear Suddenly</h2>
<p>Scaling laws predict smooth improvements in loss. But some capabilities do not improve smoothly‚Äîthey appear suddenly at a certain scale. These are called <strong>emergent abilities</strong>: skills that small models cannot perform at all, but large models can.</p>
<p><strong>Examples of emergent abilities:</strong></p>
<p><strong>Few-shot in-context learning</strong>: GPT-3 (175B) can learn from a few examples in the prompt without fine-tuning. GPT-2 (1.5B) cannot. This capability ‚Äúemerges‚Äù somewhere between 13B and 175B parameters.</p>
<p><strong>Multi-step reasoning</strong>: Models below ~10B parameters fail at grade-school math word problems (GSM8K). Models above ~100B parameters achieve 40%+ accuracy. The capability jumps sharply, not gradually.</p>
<p><strong>Instruction following</strong>: Small models generate text but do not follow instructions reliably (‚ÄúWrite a poem about trees‚Äù ‚Üí random text). Large models (10B+) follow instructions accurately (‚ÄúWrite a poem about trees‚Äù ‚Üí coherent poem).</p>
<p><strong>Translation between languages not seen during training</strong>: Large models (100B+) can translate between low-resource language pairs (Swahili ‚Üî Turkish) despite minimal training data. Small models cannot.</p>
<p><strong>Why do emergent abilities appear?</strong></p>
<p>Two explanations:</p>
<p><strong>1. Phase transitions in capability:</strong></p>
<p>As loss decreases smoothly, the model crosses a threshold where a task becomes solvable. Below the threshold, the model lacks sufficient capacity to represent the solution. Above the threshold, the model can solve the task. Loss decreases smoothly, but task accuracy jumps sharply.</p>
<p>Analogy: Water temperature decreases smoothly from 5¬∞C to -5¬∞C, but at 0¬∞C, water freezes‚Äîa phase transition. Similarly, model loss decreases smoothly, but task performance transitions sharply.</p>
<p><strong>2. Measurement artifacts:</strong></p>
<p>Emergent abilities might be an artifact of how tasks are measured. If a task is scored as binary (correct/incorrect), smooth improvements in loss appear as sudden jumps in accuracy. Using continuous metrics (e.g., partial credit) might reveal smooth improvements.</p>
<p>Recent research suggests emergence is partly measurement-dependent: tasks scored continuously show smoother scaling. But some capabilities (like few-shot learning) do appear genuinely emergent‚Äîsmall models cannot do it at all, large models can.</p>
<p><strong>Why emergent abilities matter:</strong></p>
<p>Emergence means capabilities are <strong>unpredictable</strong> before you reach the necessary scale. GPT-3‚Äôs few-shot learning was not predicted by scaling laws‚Äîit was a surprise. This raises the question: <strong>what other capabilities will emerge at larger scales?</strong></p>
<ul>
<li>Will 10T parameter models develop true reasoning?</li>
<li>Will they learn to plan multi-step actions reliably?</li>
<li>Will they generalize across domains like humans do?</li>
</ul>
<p>Scaling laws predict loss will decrease, but they do not predict which capabilities will emerge. This makes frontier AI development both exciting and uncertain.</p>
<p><strong>Engineering implications:</strong></p>
<p>You cannot forecast emergent abilities before training. You must train the model, evaluate it, and discover what it can do. This makes large-scale training a high-stakes bet: invest $100M in training, hope new capabilities emerge that justify the cost.</p>
<hr>
<h2 id="engineering-takeaway-ch36">Engineering Takeaway</h2>
<h3 id="forecasting-becomes-possiblebut-only-for-loss-not-capabilities"><strong>Forecasting becomes possible‚Äîbut only for loss, not capabilities</strong></h3>
<p>Scaling laws allow predicting loss before training. Given compute budget, model size, and data size, you can estimate final loss within tight error bars. This enables planning: ‚ÄúIf we invest $50M in compute, we‚Äôll reach loss 1.9.‚Äù But loss does not directly predict downstream task performance. Emergent abilities can surprise. Forecasting is powerful but incomplete.</p>
<h3 id="infrastructure-is-destinycompute-access-determines-competitiveness"><strong>Infrastructure is destiny‚Äîcompute access determines competitiveness</strong></h3>
<p>Training frontier models requires thousands of GPUs, months of time, and tens of millions of dollars. Only a few organizations can afford this: OpenAI (Microsoft-backed), Google, Meta, Anthropic (Google-backed), Amazon. Compute access is the bottleneck. Smaller labs cannot compete at the frontier without funding. Infrastructure‚Äîdatacenter capacity, chip supply, energy availability‚Äîdetermines who wins the race.</p>
<h3 id="algorithmic-innovations-still-matterchinchilla-shows-better-allocation-beats-pure-scale"><strong>Algorithmic innovations still matter‚ÄîChinchilla shows better allocation beats pure scale</strong></h3>
<p>Chinchilla (70B parameters, 1.4T tokens) matched Gopher (280B parameters, 300B tokens) by training longer on more data. This means smarter training‚Äîbetter data, better compute allocation‚Äîcan match or beat larger models trained inefficiently. Pure scale is not the only path. Research into data quality, curriculum learning, and efficient architectures remains valuable.</p>
<h3 id="diminishing-returns-constrain-strategyeach-generation-costs-10x-more-for-marginal-gains"><strong>Diminishing returns constrain strategy‚Äîeach generation costs 10x more for marginal gains</strong></h3>
<p>GPT-2: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mi>K</mi><mi mathvariant="normal">.</mi><mi>G</mi><mi>P</mi><mi>T</mi><mo>‚àí</mo><mn>3</mn><mo>:</mo></mrow><annotation encoding="application/x-tex">50K. GPT-3: </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord">50</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.13889em;">GPT</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span></span></span></span>5M. GPT-4: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mo>‚àí</mo><mn>100</mn><mi>M</mi><mi mathvariant="normal">.</mi><mi>G</mi><mi>P</mi><mi>T</mi><mo>‚àí</mo><mn>5</mn><mo>:</mo></mrow><annotation encoding="application/x-tex">50-100M. GPT-5: </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">50</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord">100</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.13889em;">GPT</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span></span></span></span>500M-1B? Each generation requires 10x more compute for incrementally smaller improvements. At some point, ROI becomes negative. The industry must shift focus from pure scale to efficiency: inference optimization, model compression, application-specific models. Scaling continues but slows.</p>
<h3 id="emergent-abilities-are-unpredictablenew-skills-appear-but-we-dont-know-which-or-when"><strong>Emergent abilities are unpredictable‚Äînew skills appear, but we don‚Äôt know which or when</strong></h3>
<p>Few-shot learning emerged at ~13B parameters. Multi-step reasoning emerged at ~100B parameters. What emerges at 1T? 10T? No one knows until the model is trained and evaluated. This makes large-scale training a high-risk, high-reward investment. Capabilities might exceed expectations‚Äîor plateau. Emergent abilities keep scaling exciting but uncertain.</p>
<h3 id="economics-drive-developmentonly-well-funded-organizations-can-afford-frontier-training"><strong>Economics drive development‚Äîonly well-funded organizations can afford frontier training</strong></h3>
<p>Training GPT-4 costs <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mo>‚àí</mo><mn>100</mn><mi>M</mi><mi mathvariant="normal">.</mi><mi>G</mi><mi>P</mi><mi>T</mi><mo>‚àí</mo><mn>5</mn><mi>m</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>c</mi><mi>o</mi><mi>s</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">50-100M. GPT-5 might cost </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">50</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord">100</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.13889em;">GPT</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">5</span><span class="mord mathnormal">mi</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mord mathnormal">cos</span><span class="mord mathnormal">t</span></span></span></span>500M-1B. Only organizations with massive funding can train frontier models. This concentrates power: OpenAI (Microsoft), Google, Meta, Anthropic, Amazon. Smaller labs focus on fine-tuning, distillation, or open models. The economics of AI favor scale and capital.</p>
<h3 id="energy-costs-compoundsustainability-becomes-an-engineering-constraint"><strong>Energy costs compound‚Äîsustainability becomes an engineering constraint</strong></h3>
<p>GPT-3 training consumed 1,287 MWh. GPT-4 consumed 10,000+ MWh. Datacenter power consumption is a bottleneck. Regions with limited power infrastructure cannot host large training runs. Energy efficiency‚Äîbetter hardware (H100 vs A100, specialized AI chips), algorithmic optimizations‚Äîbecomes critical. Sustainable AI requires renewable energy and efficient architectures.</p>
<hr>
<p><img  src="/eng-ai/_astro/36-diagram.6c0VtTUm_Z17uQi1.svg" alt="Engineering Takeaway diagram" width="800" height="500" loading="lazy" decoding="async"></p>
<hr>
<h2 id="references-and-further-reading-ch36">References and Further Reading</h2>
<h3 id="scaling-laws-for-neural-language-models---kaplan-et-al-2020-openai"><strong>Scaling Laws for Neural Language Models</strong> - Kaplan et al. (2020), OpenAI</h3>
<p><strong>Why it matters:</strong> This paper established that language model performance follows predictable power laws across six orders of magnitude in model size, dataset size, and compute. It showed that loss scales as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>‚àù</mo><msup><mi>N</mi><mrow><mo>‚àí</mo><mi>Œ±</mi></mrow></msup></mrow><annotation encoding="application/x-tex">L \propto N^{-\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àù</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7713em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚àí</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">Œ±</span></span></span></span></span></span></span></span></span></span></span></span> for model parameters, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>‚àù</mo><msup><mi>D</mi><mrow><mo>‚àí</mo><mi>Œ±</mi></mrow></msup></mrow><annotation encoding="application/x-tex">L \propto D^{-\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àù</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7713em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚àí</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">Œ±</span></span></span></span></span></span></span></span></span></span></span></span> for data size, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>‚àù</mo><msup><mi>C</mi><mrow><mo>‚àí</mo><mi>Œ±</mi></mrow></msup></mrow><annotation encoding="application/x-tex">L \propto C^{-\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àù</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7713em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚àí</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">Œ±</span></span></span></span></span></span></span></span></span></span></span></span> for compute budget. This enabled forecasting: given a compute budget, predict final loss before training. The paper changed AI development from trial-and-error experimentation to engineering optimization. It justified massive investments in scale: if power laws hold, bigger models will predictably perform better. This paper is the foundation for GPT-3, GPT-4, and the entire scaling paradigm.</p>
<h3 id="training-compute-optimal-large-language-models---hoffmann-et-al-2022-deepmind-chinchilla-paper"><strong>Training Compute-Optimal Large Language Models</strong> - Hoffmann et al. (2022), DeepMind (Chinchilla paper)</h3>
<p><strong>Why it matters:</strong> This paper revised OpenAI‚Äôs scaling laws, showing that most models were <strong>undertrained</strong>‚Äîtoo many parameters, not enough training data. DeepMind trained Chinchilla (70B parameters) on 1.4 trillion tokens (4x more data than typical) and matched Gopher (280B parameters trained on 300B tokens). The key insight: optimal compute allocation should balance model size and data roughly equally. If compute increases 10x, increase model size 3x and data 3x. This finding reshaped the industry: GPT-4 likely followed Chinchilla‚Äôs allocation strategy, training on 10-100T tokens instead of scaling parameters alone. The paper showed that smarter training beats pure scale.</p>
<h3 id="emergent-abilities-of-large-language-models---wei-et-al-2022-google-brain"><strong>Emergent Abilities of Large Language Models</strong> - Wei et al. (2022), Google Brain</h3>
<p><strong>Why it matters:</strong> This paper catalogued capabilities that appear suddenly at scale: multi-step reasoning, instruction following, few-shot learning. Small models cannot perform these tasks at all; large models can. The paper showed that loss decreases smoothly, but task performance transitions sharply‚Äîphase-transition-like behavior. This raised a critical question: <strong>what other abilities will emerge at larger scales?</strong> The paper also cautioned that emergence may be measurement-dependent: tasks scored as binary (correct/incorrect) show sharper transitions than tasks scored continuously. Regardless, emergent abilities make scaling both exciting and unpredictable. They justify high-risk, high-reward investments in frontier models.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part8" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Part VIII: The Frontier</span> </a> <a href="/eng-ai/part8/37-multimodal-models" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Multimodal Models</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#power-laws-how-performance-grows-with-compute-ch36" data-astro-cid-xvrfupwn>Power Laws: How Performance Grows with Compute</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#data-model-compute-the-three-levers-ch36" data-astro-cid-xvrfupwn>Data, Model, Compute: The Three Levers</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#diminishing-returns-why-progress-is-predictable-ch36" data-astro-cid-xvrfupwn>Diminishing Returns: Why Progress Is Predictable</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#emergent-abilities-why-new-skills-appear-suddenly-ch36" data-astro-cid-xvrfupwn>Emergent Abilities: Why New Skills Appear Suddenly</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch36" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#forecasting-becomes-possiblebut-only-for-loss-not-capabilities" data-astro-cid-xvrfupwn>Forecasting becomes possible‚Äîbut only for loss, not capabilities</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#infrastructure-is-destinycompute-access-determines-competitiveness" data-astro-cid-xvrfupwn>Infrastructure is destiny‚Äîcompute access determines competitiveness</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#algorithmic-innovations-still-matterchinchilla-shows-better-allocation-beats-pure-scale" data-astro-cid-xvrfupwn>Algorithmic innovations still matter‚ÄîChinchilla shows better allocation beats pure scale</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#diminishing-returns-constrain-strategyeach-generation-costs-10x-more-for-marginal-gains" data-astro-cid-xvrfupwn>Diminishing returns constrain strategy‚Äîeach generation costs 10x more for marginal gains</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#emergent-abilities-are-unpredictablenew-skills-appear-but-we-dont-know-which-or-when" data-astro-cid-xvrfupwn>Emergent abilities are unpredictable‚Äînew skills appear, but we don‚Äôt know which or when</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#economics-drive-developmentonly-well-funded-organizations-can-afford-frontier-training" data-astro-cid-xvrfupwn>Economics drive development‚Äîonly well-funded organizations can afford frontier training</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#energy-costs-compoundsustainability-becomes-an-engineering-constraint" data-astro-cid-xvrfupwn>Energy costs compound‚Äîsustainability becomes an engineering constraint</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch36" data-astro-cid-xvrfupwn>References and Further Reading</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#scaling-laws-for-neural-language-models---kaplan-et-al-2020-openai" data-astro-cid-xvrfupwn>Scaling Laws for Neural Language Models - Kaplan et al. (2020), OpenAI</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#training-compute-optimal-large-language-models---hoffmann-et-al-2022-deepmind-chinchilla-paper" data-astro-cid-xvrfupwn>Training Compute-Optimal Large Language Models - Hoffmann et al. (2022), DeepMind (Chinchilla paper)</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#emergent-abilities-of-large-language-models---wei-et-al-2022-google-brain" data-astro-cid-xvrfupwn>Emergent Abilities of Large Language Models - Wei et al. (2022), Google Brain</a></li></ul></nav> </aside> </div>   </body> </html>