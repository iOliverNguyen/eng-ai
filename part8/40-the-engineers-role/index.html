<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 40: The Engineer&#39;s Role | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part8/40-the-engineers-role/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part8/40-the-engineers-role/"><meta property="og:title" content="Chapter 40: The Engineer's Role | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part8/40-the-engineers-role/"><meta name="twitter:title" content="Chapter 40: The Engineer's Role | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part8" data-astro-cid-ilhxcym7>Part VIII: The Frontier</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>The Engineer&#39;s Role</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-40-the-engineers-role">Chapter 40: The Engineer‚Äôs Role</h1>
<p>We opened this book with a claim: intelligence is not magic. It is optimization, representation, data, and scale. Forty chapters later, we have traced the arc from linear regression to frontier language models, from backpropagation to multimodal systems, from supervised learning to self-improving AI. The lesson throughout: AI is engineering. Models are functions optimized over data. Capabilities emerge from architecture and scale. Failures come from design choices, not mystical forces.</p>
<p>And if AI is engineering, then <strong>engineers control it</strong>. Every chapter has shown decision points: which loss function to optimize, which architecture to use, which data to collect, which features to include, which thresholds to set. These choices shape behavior. Models do not design themselves‚Äîengineers design them. Models do not decide their own objectives‚Äîengineers specify loss functions. Models do not choose their own data‚Äîengineers curate datasets.</p>
<p>This chapter is about agency. Not model agency‚Äîcurrent AI systems lack it (Chapter 39). Human agency. Engineer agency. The people who build AI systems shape what those systems do, how they fail, and what impact they have. This is not abstract philosophy. It is concrete technical reality: design determines behavior, and engineers control design.</p>
<p>The future of AI is not predetermined. It depends on choices made today. Engineers who understand how AI works‚Äînot just how to call APIs, but how models learn, generalize, and fail‚Äîhave the knowledge to build responsibly. This final chapter shows where engineers shape outcomes, where ethical choices appear, and why responsibility matters. The conclusion: <strong>AI is a tool, not a destiny. Engineers who build it control its trajectory.</strong></p>
<hr>
<h2 id="humans-in-the-loop-why-people-matter-ch40">Humans in the Loop: Why People Matter</h2>
<p>AI systems are tools. They process data, make predictions, generate outputs. But they do not decide what to do with those outputs. Humans decide.</p>
<h3 id="models-provide-information-humans-take-action"><strong>Models Provide Information, Humans Take Action</strong></h3>
<p>A fraud detection model outputs: ‚ÄúTransaction X has 85% probability of fraud.‚Äù What happens next?</p>
<ul>
<li><strong>Human decision</strong>: Bank employee reviews transaction details, contacts customer, confirms fraud or false positive</li>
<li><strong>Model output</strong>: Just a number‚Äî85%‚Äînot a decision</li>
</ul>
<p>The model provides information. The human interprets it, considers context (customer history, transaction details), and acts. This separation is critical: models are not autonomous agents. They are information sources.</p>
<h3 id="high-stakes-decisions-require-human-judgment"><strong>High-Stakes Decisions Require Human Judgment</strong></h3>
<p>In high-stakes domains‚Äîmedicine, criminal justice, hiring‚Äîhuman oversight is not optional, it is necessary.</p>
<p><strong>Medical diagnosis:</strong></p>
<ul>
<li>Model analyzes chest X-ray, outputs: ‚ÄúPossible pneumonia, confidence 70%‚Äù</li>
<li>Radiologist reviews image, considers patient history (symptoms, age, comorbidities), consults guidelines</li>
<li>Radiologist decides: order further tests, prescribe treatment, or dismiss as false positive</li>
</ul>
<p>The model assists‚Äîit highlights potential issues. But the radiologist decides. The radiologist has liability, context, and responsibility. The model does not.</p>
<p><strong>Criminal justice:</strong></p>
<ul>
<li>Risk assessment model predicts recidivism risk</li>
<li>Judge reviews model output, considers case details (crime severity, defendant circumstances, community impact)</li>
<li>Judge makes sentencing decision‚Äîmodel is one input among many</li>
</ul>
<p>Relying solely on model outputs in high-stakes domains is dangerous: models are trained on historical data (which embeds historical biases), lack context (cannot account for individual circumstances), and make errors (false positives and negatives). Human judgment integrates model outputs with domain expertise and ethical considerations.</p>
<h3 id="calibration-enables-effective-collaboration"><strong>Calibration Enables Effective Collaboration</strong></h3>
<p>For humans to trust model outputs, models must be <strong>calibrated</strong>: if a model says ‚Äú90% confidence,‚Äù it should be correct 90% of the time. Miscalibrated models‚Äîwhere stated confidence does not match actual accuracy‚Äîmislead humans.</p>
<p><strong>Example: Overconfident models</strong></p>
<p>A model outputs ‚Äú99% confidence this is cancer‚Äù but is only correct 70% of the time. Doctors trust the high confidence and skip further testing‚Äîpatients suffer. Calibration matters: models must express uncertainty honestly.</p>
<p><strong>Tools for calibration:</strong></p>
<ul>
<li>Temperature scaling (adjust output probabilities to match true frequencies)</li>
<li>Confidence intervals (provide ranges, not point estimates)</li>
<li>Uncertainty quantification (flag inputs where model is uncertain)</li>
</ul>
<p>Well-calibrated models enable effective human-AI collaboration: humans trust outputs when confidence is high, scrutinize outputs when confidence is low, and override when context demands it.</p>
<h3 id="humans-catch-errors-before-harm"><strong>Humans Catch Errors Before Harm</strong></h3>
<p>Models make mistakes. Always. Humans are the last line of defense.</p>
<p><strong>Scenario: Resume screening</strong></p>
<ul>
<li>Model scores 1,000 resumes, ranks top 50 for interviews</li>
<li>Human recruiter reviews top 50, notices model missed a strong candidate (unusual background, nonstandard formatting)</li>
<li>Human adds candidate to interview list</li>
</ul>
<p>The model automated 95% of screening (filtered 1,000 ‚Üí 50). The human corrected the remaining 5%. This hybrid approach‚Äîmodel handles volume, human handles edge cases‚Äîbalances efficiency and accuracy.</p>
<p>Without human oversight, model errors compound. With oversight, humans catch mistakes before they cause harm.</p>
<hr>
<h2 id="system-design-where-engineers-shape-behavior-ch40">System Design: Where Engineers Shape Behavior</h2>
<p>AI systems are not just models. They are architectures: data pipelines, models, guardrails, evaluation, monitoring, deployment. Engineers design every component. These design choices shape behavior more than model choice.</p>
<h3 id="architecture-choices"><strong>Architecture Choices</strong></h3>
<p><strong>RAG (Retrieval-Augmented Generation) vs Fine-Tuning:</strong></p>
<ul>
<li>
<p><strong>RAG</strong>: Model retrieves documents, generates answer grounded in retrieval</p>
<ul>
<li>Advantage: Up-to-date information (retrieval pulls latest data), explainable (cite sources)</li>
<li>Disadvantage: Slower (retrieval + generation), dependent on retrieval quality</li>
</ul>
</li>
<li>
<p><strong>Fine-tuning</strong>: Train model on domain-specific data</p>
<ul>
<li>Advantage: Faster inference (no retrieval), better domain adaptation</li>
<li>Disadvantage: Outdated (data frozen at training time), less explainable</li>
</ul>
</li>
</ul>
<p>Which to choose? Depends on use case:</p>
<ul>
<li>Customer support with FAQ database ‚Üí RAG (cite answers to FAQs)</li>
<li>Medical diagnosis ‚Üí Fine-tuning (domain-specific training improves accuracy)</li>
</ul>
<p>Engineers decide. The choice shapes accuracy, latency, explainability, cost.</p>
<h3 id="data-pipeline-design"><strong>Data Pipeline Design</strong></h3>
<p>What data to collect? How to label it? Which features to include?</p>
<p><strong>Example: Credit scoring</strong></p>
<p>Which features predict creditworthiness?</p>
<ul>
<li><strong>Standard features</strong>: Income, debt, payment history</li>
<li><strong>Proxies for protected attributes</strong>: Zip code (correlates with race), education (correlates with socioeconomic status)</li>
</ul>
<p>Engineers decide whether to include zip code. Including it improves accuracy (zip code correlates with default risk via local economic conditions) but embeds geographic bias (redlining historically denied credit to minority neighborhoods). Excluding it reduces bias but also reduces accuracy.</p>
<p>This is a design choice. There is no ‚Äúcorrect‚Äù answer‚Äîonly trade-offs. Engineers make the call based on values (fairness vs accuracy) and constraints (legal requirements, company policy).</p>
<h3 id="guardrail-implementation"><strong>Guardrail Implementation</strong></h3>
<p>How to prevent harmful outputs?</p>
<ul>
<li><strong>Input filters</strong>: Block offensive prompts, jailbreak attempts</li>
<li><strong>Output filters</strong>: Detect toxic language, personal information, medical advice</li>
<li><strong>Refusal training</strong>: Teach model to decline harmful requests (‚ÄúI can‚Äôt help with that‚Äù)</li>
<li><strong>Rate limiting</strong>: Prevent abuse via usage caps</li>
</ul>
<p>Engineers design these guardrails. Too strict ‚Üí false positives (benign requests blocked). Too lenient ‚Üí harmful outputs slip through. The balance is a design choice.</p>
<h3 id="evaluation-design"><strong>Evaluation Design</strong></h3>
<p>Which metrics matter? Which test sets to use? Which failure modes to monitor?</p>
<p><strong>Example: Translation model</strong></p>
<p>Metrics:</p>
<ul>
<li><strong>BLEU score</strong>: Measures overlap between model translation and reference translation</li>
<li><strong>Human evaluation</strong>: Fluency, accuracy, cultural appropriateness</li>
</ul>
<p>BLEU is fast and cheap (automated). Human evaluation is slow and expensive but captures nuances BLEU misses. Engineers decide: optimize for BLEU (fast iteration) or human judgment (better quality)?</p>
<p>This choice shapes development: BLEU-optimized models produce technically correct but unnatural translations. Human-optimized models produce fluent, context-appropriate translations but cost more to develop.</p>
<h3 id="deployment-strategy"><strong>Deployment Strategy</strong></h3>
<p>How to roll out new models? A/B testing? Gradual rollout? Shadow deployment?</p>
<ul>
<li><strong>A/B testing</strong>: Serve new model to 5% of users, compare metrics to old model</li>
<li><strong>Gradual rollout</strong>: Increase traffic to new model if metrics improve (5% ‚Üí 10% ‚Üí 50% ‚Üí 100%)</li>
<li><strong>Shadow deployment</strong>: Run new model alongside old, log outputs but don‚Äôt serve to users (monitor for errors before switching)</li>
</ul>
<p>Engineers design deployment. The strategy determines risk: gradual rollout minimizes harm if new model fails, but delays benefits if it succeeds. Trade-offs everywhere. Engineers decide.</p>
<hr>
<h2 id="ethical-leverage-where-choices-are-made-ch40">Ethical Leverage: Where Choices Are Made</h2>
<p>Ethics are not external to engineering. Ethical decisions are embedded in technical choices. Engineers have leverage at multiple points.</p>
<h3 id="feature-selection"><strong>Feature Selection</strong></h3>
<p>Which signals to use in a model? This determines what the model learns.</p>
<p><strong>Example: Hiring model</strong></p>
<p>Features:</p>
<ul>
<li><strong>Resume content</strong>: Skills, experience, education</li>
<li><strong>Demographic proxies</strong>: Name (correlates with race/gender), university (correlates with socioeconomic status)</li>
</ul>
<p>Including name improves accuracy (because historical hiring was biased‚Äîmodels learn that bias). Excluding name reduces discrimination but may reduce accuracy. Engineers choose: prioritize accuracy or fairness?</p>
<p>Some argue: ‚ÄúLet the model use all available data, optimize for accuracy.‚Äù But this embeds historical bias. If past hiring favored men, a model trained on past hires learns: men = better candidates. The model perpetuates discrimination.</p>
<p>Others argue: ‚ÄúExclude all demographic proxies.‚Äù But proxies are everywhere‚Äîzip code, university, even word choice in resumes correlates with demographics. Perfect exclusion is impossible.</p>
<p>Engineers must navigate these trade-offs. There is no purely technical solution‚Äîevery choice reflects values.</p>
<h3 id="threshold-tuning"><strong>Threshold Tuning</strong></h3>
<p>Where to set the decision boundary? This determines false positive vs false negative rates.</p>
<p><strong>Example: Spam filter</strong></p>
<ul>
<li><strong>Low threshold (permissive)</strong>: Fewer false positives (important emails not marked spam), more false negatives (spam gets through)</li>
<li><strong>High threshold (strict)</strong>: Fewer false negatives (spam blocked), more false positives (important emails marked spam)</li>
</ul>
<p>Which is worse? Missing an important email (false positive) or seeing spam (false negative)? Engineers decide based on user priorities.</p>
<p><strong>Example: Fraud detection</strong></p>
<ul>
<li><strong>Low threshold</strong>: Catch more fraud (fewer false negatives), but more legitimate transactions flagged (false positives, customer frustration)</li>
<li><strong>High threshold</strong>: Fewer false positives, but more fraud slips through (false negatives, financial loss)</li>
</ul>
<p>Financial impact: False positives annoy customers (calls to confirm legitimate transactions). False negatives cost money (fraudulent transactions not caught). Engineers tune thresholds to balance these costs. This is an ethical decision: whose inconvenience matters more‚Äîcustomers or the bank?</p>
<h3 id="dataset-curation"><strong>Dataset Curation</strong></h3>
<p>Whose data is included? Whose is excluded? This determines representation.</p>
<p><strong>Example: Facial recognition</strong></p>
<p>Early datasets (FaceNet, VGGFace) overrepresented light-skinned individuals. Models trained on these datasets performed poorly on dark-skinned individuals‚Äîhigher error rates, more misidentifications. The bias was not in the algorithm‚Äîit was in the data.</p>
<p>Researchers fixed this by curating balanced datasets (equal representation across demographics). Accuracy improved across all groups. This required intentional effort: measure representation, collect additional data for underrepresented groups, balance the dataset.</p>
<p>Engineers control curation. If they collect data passively (scrape the internet), bias is embedded. If they curate intentionally (measure, balance, correct), bias is reduced. This is a choice.</p>
<h3 id="use-case-constraints"><strong>Use Case Constraints</strong></h3>
<p>Which applications are permitted? Which are forbidden?</p>
<p><strong>Example: OpenAI‚Äôs use policy for GPT</strong></p>
<p>Prohibited uses:</p>
<ul>
<li>Surveillance and monitoring</li>
<li>Political campaigning and lobbying</li>
<li>Impersonation without disclosure</li>
<li>Medical advice without disclaimers</li>
<li>Legal advice without disclaimers</li>
</ul>
<p>These constraints are policy decisions. Other companies make different choices. Engineers (and their organizations) decide which use cases to enable.</p>
<h3 id="transparency"><strong>Transparency</strong></h3>
<p>What information to disclose to users? How the model works? What data it trained on? Its limitations?</p>
<p><strong>Model cards</strong> (Mitchell et al., 2019) document:</p>
<ul>
<li>Intended use</li>
<li>Performance across demographics</li>
<li>Known limitations</li>
<li>Ethical considerations</li>
</ul>
<p>Engineers decide what to include in model cards. More transparency builds trust but exposes vulnerabilities (adversaries exploit known weaknesses). Less transparency hides problems. Balance is a design choice.</p>
<hr>
<h2 id="long-term-responsibility-why-builders-matter-ch40">Long-Term Responsibility: Why Builders Matter</h2>
<p>Engineers are not passive. They shape capabilities, incentives, norms. The systems built today affect society for years.</p>
<h3 id="builders-shape-capabilities"><strong>Builders Shape Capabilities</strong></h3>
<p>What gets built determines what is possible. If engineers build surveillance systems, surveillance becomes easier. If engineers build accessibility tools, disability support improves. The decision of <strong>what to build</strong> shapes the future.</p>
<p><strong>Example: Facial recognition</strong></p>
<ul>
<li>Built for surveillance ‚Üí enables mass monitoring, authoritarian control</li>
<li>Built for accessibility ‚Üí enables photo organization, assistive devices for visually impaired</li>
</ul>
<p>Same technology, different applications. Engineers (and their employers) choose which applications to prioritize. Those choices have societal impact.</p>
<h3 id="builders-shape-incentives"><strong>Builders Shape Incentives</strong></h3>
<p>What does the model optimize? Engagement? Accuracy? User satisfaction? Profit?</p>
<p><strong>Example: Social media recommendation algorithms</strong></p>
<ul>
<li>Optimize engagement (clicks, time spent) ‚Üí amplifies outrage, misinformation (because controversial content drives engagement)</li>
<li>Optimize user satisfaction (surveys, long-term retention) ‚Üí promotes quality content, reduces toxicity</li>
</ul>
<p>The choice of objective function determines outcomes. Facebook‚Äôs 2010s recommendation algorithms optimized engagement‚Äîresult: misinformation spread, polarization increased. Later adjustments optimized satisfaction‚Äîresult: less toxicity, lower engagement. Engineers chose the objective; society experienced the consequences.</p>
<h3 id="builders-shape-norms"><strong>Builders Shape Norms</strong></h3>
<p>How AI is deployed establishes expectations. If companies deploy models without transparency, users accept opacity. If companies deploy models with explanations, users expect accountability.</p>
<p><strong>Example: Loan denials</strong></p>
<ul>
<li>No explanation: User denied loan, no reason given ‚Üí frustration, distrust, no recourse</li>
<li>With explanation: ‚ÄúDenied due to high debt-to-income ratio‚Äù ‚Üí user understands, can take corrective action</li>
</ul>
<p>Providing explanations sets a norm: users expect transparency. Withholding explanations sets a different norm: users accept black-box decisions. Engineers (and their organizations) shape which norm prevails by deciding what to build.</p>
<h3 id="technical-debt-accumulates"><strong>Technical Debt Accumulates</strong></h3>
<p>Shortcuts today become systemic problems tomorrow. Engineers often face pressure: ship quickly, optimize for short-term metrics, skip testing. These decisions create <strong>technical debt</strong>‚Äîfragile systems, hard-to-debug errors, scaling failures.</p>
<p><strong>Example: Data pipeline shortcuts</strong></p>
<ul>
<li>Skip data validation (to ship faster) ‚Üí bad data enters training set ‚Üí model learns garbage</li>
<li>Years later: model deployed at scale, producing biased outputs, no one remembers why</li>
</ul>
<p>Technical debt compounds. Fixing it later costs more than doing it right initially. Engineers who resist shortcuts build sustainable systems. Those who prioritize speed build fragile ones. The choice affects long-term reliability.</p>
<h3 id="dual-use-technology-can-be-misused"><strong>Dual Use: Technology Can Be Misused</strong></h3>
<p>Powerful tools have dual use: beneficial applications and harmful misuse. Engineers cannot prevent all misuse, but they can anticipate it and design safeguards.</p>
<p><strong>Example: Large language models</strong></p>
<ul>
<li>Beneficial: Education (tutoring), accessibility (text-to-speech), productivity (writing assistance)</li>
<li>Harmful: Misinformation (generate fake news), phishing (craft convincing scams), spam (automate low-quality content)</li>
</ul>
<p>Engineers cannot stop misuse entirely. But they can design safeguards: rate limits (prevent mass spam), watermarking (identify AI-generated content), usage monitoring (detect abuse patterns). These safeguards reduce harm without eliminating capability.</p>
<p>Ignoring dual use is negligent. Anticipating it and mitigating risk is responsible engineering.</p>
<hr>
<h2 id="final-takeaway-why-ai-is-a-tool-not-a-destiny-ch40">Final Takeaway: Why AI Is a Tool, Not a Destiny</h2>
<p>We have spent 40 chapters building an understanding of AI: what it is, how it works, where it succeeds, where it fails, and where it is going. The conclusion is not that AI is dangerous, nor that it is salvation. The conclusion is: <strong>AI is a tool. Engineers control it.</strong></p>
<h3 id="ai-does-not-have-agency"><strong>AI Does Not Have Agency</strong></h3>
<p>Language models predict text. Vision models classify images. RL agents optimize rewards. None of these systems set their own goals, choose their own objectives, or act autonomously. They do what they are trained to do. Engineers choose the training data, the loss function, the architecture, the deployment. Engineers control behavior.</p>
<p>The notion that ‚ÄúAI is out of control‚Äù is false. Current systems do not have agency. Future systems‚Äîeven AGI, if it arrives‚Äîwill be designed by engineers. Design choices determine outcomes. Engineers are not passive observers. They are builders.</p>
<h3 id="progress-is-not-inevitable"><strong>Progress Is Not Inevitable</strong></h3>
<p>Scaling requires resources: compute, data, energy, funding. Resources require investment. Investment requires decisions. Those decisions are made by people: researchers, engineers, executives, policymakers. AI progresses because people choose to allocate resources to it.</p>
<p>Alternative futures are possible. A future where AI augments human capabilities rather than replaces them. A future where AI is open and accessible, not controlled by a few corporations. A future where AI is safe, aligned, and beneficial. Or a future where AI amplifies inequality, spreads misinformation, and concentrates power.</p>
<p><strong>Which future occurs depends on choices made today.</strong> Engineers, by virtue of building the systems, shape those choices.</p>
<h3 id="responsibility-is-collective"><strong>Responsibility Is Collective</strong></h3>
<p>No single engineer determines AI‚Äôs trajectory. But every engineer contributes. Researchers choose what to study. Engineers choose what to build. Product managers choose what to deploy. Policymakers choose what to regulate. The outcome is collective.</p>
<p>Responsibility is not diffuse‚Äîit is distributed. Each person‚Äôs choices matter. A researcher who investigates fairness advances equity. An engineer who builds accessibility features improves inclusion. A product manager who requires transparency enables accountability. A policymaker who regulates harmful use reduces abuse.</p>
<p>Collective responsibility means individual actions matter. Engineers are not powerless cogs in a machine. They have agency. They can choose to build responsibly, even when pressured not to.</p>
<h3 id="the-long-view-matters"><strong>The Long View Matters</strong></h3>
<p>AI systems deployed today will be used for years. Data collected today will train models tomorrow. Norms established today will persist. Engineers must think beyond immediate goals (ship the feature, hit the metric, satisfy the customer) and consider long-term consequences.</p>
<p><strong>Questions to ask:</strong></p>
<ul>
<li>If this system scales 100x, what breaks?</li>
<li>If this data is used to train the next generation of models, what bias is amplified?</li>
<li>If this deployment norm becomes standard, what does the industry look like in 10 years?</li>
</ul>
<p>Short-term optimization leads to long-term problems. Sustainable engineering requires thinking ahead.</p>
<h3 id="engineers-shape-the-future"><strong>Engineers Shape the Future</strong></h3>
<p>The final lesson: <strong>AI is made, not discovered. It is designed, not inevitable. Engineers who understand how it works control its trajectory.</strong></p>
<p>You have spent 40 chapters learning how AI works: how models learn from data, how architectures shape capabilities, how loss functions determine behavior, how scaling drives progress, how alignment prevents harm. This knowledge is power. Power to build reliably. Power to understand failure modes. Power to design responsibly.</p>
<p><strong>The future is not predetermined.</strong> It depends on choices: which systems to build, which objectives to optimize, which data to use, which safeguards to implement. Engineers make those choices.</p>
<p><strong>AI is a tool.</strong> Tools can be used well or poorly. Engineers decide.</p>
<hr>
<p><img  src="/eng-ai/_astro/40-diagram.BkBmgx9__wbPG1.svg" alt="Final Takeaway: Why AI Is a Tool, Not a Destiny diagram" width="800" height="550" loading="lazy" decoding="async"></p>
<hr>
<h2 id="references-and-further-reading-ch40">References and Further Reading</h2>
<h3 id="datasheets-for-datasets---gebru-et-al-2018-microsoft-research"><strong>Datasheets for Datasets</strong> - Gebru et al. (2018), Microsoft Research</h3>
<p><strong>Why it matters:</strong> This paper introduced <strong>datasheets for datasets</strong>, structured documentation analogous to electronics datasheets. A datasheet documents: motivation (why the dataset was created), composition (what data it contains), collection process (how data was gathered), preprocessing steps, recommended uses, distribution, and maintenance plan. This transparency enables informed decisions: users know what biases exist, what limitations apply, whether the dataset fits their use case. Before datasheets, datasets were often poorly documented‚Äîusers trained models on data without understanding its provenance or biases. Datasheets became standard practice in responsible AI, required by many organizations. This paper showed that transparency is an engineering responsibility: document your work so others can use it safely.</p>
<h3 id="model-cards-for-model-reporting---mitchell-et-al-2019-google"><strong>Model Cards for Model Reporting</strong> - Mitchell et al. (2019), Google</h3>
<p><strong>Why it matters:</strong> This paper introduced <strong>model cards</strong>, structured documentation for models. A model card includes: intended use, performance across demographics (accuracy for different subgroups), known limitations, ethical considerations, training data, and evaluation procedures. Model cards make model behavior transparent to users: they know what the model does well, where it fails, and whether it is appropriate for their use case. Before model cards, models were black boxes‚Äîusers did not know how they were trained, what biases they had, or how they would perform on their data. Model cards are now required by many organizations deploying AI. This paper demonstrated that accountability requires documentation: if you build it, document it so users can trust it (or know when not to).</p>
<h3 id="fairness-and-abstraction-in-sociotechnical-systems---selbst-et-al-2019-data--society"><strong>Fairness and Abstraction in Sociotechnical Systems</strong> - Selbst et al. (2019), Data &#x26; Society</h3>
<p><strong>Why it matters:</strong> This paper argues that fairness cannot be solved by algorithms alone‚Äîit is embedded in social context. The ‚Äúabstraction trap‚Äù: treating AI systems as isolated technical artifacts, ignoring the social systems they operate within. Example: A hiring algorithm may be ‚Äúfair‚Äù (equal accuracy across demographics) but still perpetuate inequality if deployed in a context with structural barriers (education access, network effects, implicit bias in interviews). Fairness requires understanding stakeholders, power dynamics, and societal context‚Äînot just optimizing metrics. This paper warns engineers: do not assume fairness is a purely technical problem solvable by better algorithms. Engage with social context. Understand who is affected, how, and why. Fairness is a sociotechnical challenge, not a mathematical optimization. This paper influenced responsible AI practice: fairness requires collaboration between engineers, domain experts, and affected communities.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part8/39-artificial-general-intelligence" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Artificial General Intelligence</span> </a>  </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#humans-in-the-loop-why-people-matter-ch40" data-astro-cid-xvrfupwn>Humans in the Loop: Why People Matter</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#models-provide-information-humans-take-action" data-astro-cid-xvrfupwn>Models Provide Information, Humans Take Action</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#high-stakes-decisions-require-human-judgment" data-astro-cid-xvrfupwn>High-Stakes Decisions Require Human Judgment</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#calibration-enables-effective-collaboration" data-astro-cid-xvrfupwn>Calibration Enables Effective Collaboration</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#humans-catch-errors-before-harm" data-astro-cid-xvrfupwn>Humans Catch Errors Before Harm</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#system-design-where-engineers-shape-behavior-ch40" data-astro-cid-xvrfupwn>System Design: Where Engineers Shape Behavior</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#architecture-choices" data-astro-cid-xvrfupwn>Architecture Choices</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#data-pipeline-design" data-astro-cid-xvrfupwn>Data Pipeline Design</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#guardrail-implementation" data-astro-cid-xvrfupwn>Guardrail Implementation</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#evaluation-design" data-astro-cid-xvrfupwn>Evaluation Design</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#deployment-strategy" data-astro-cid-xvrfupwn>Deployment Strategy</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#ethical-leverage-where-choices-are-made-ch40" data-astro-cid-xvrfupwn>Ethical Leverage: Where Choices Are Made</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#feature-selection" data-astro-cid-xvrfupwn>Feature Selection</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#threshold-tuning" data-astro-cid-xvrfupwn>Threshold Tuning</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#dataset-curation" data-astro-cid-xvrfupwn>Dataset Curation</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#use-case-constraints" data-astro-cid-xvrfupwn>Use Case Constraints</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#transparency" data-astro-cid-xvrfupwn>Transparency</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#long-term-responsibility-why-builders-matter-ch40" data-astro-cid-xvrfupwn>Long-Term Responsibility: Why Builders Matter</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#builders-shape-capabilities" data-astro-cid-xvrfupwn>Builders Shape Capabilities</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#builders-shape-incentives" data-astro-cid-xvrfupwn>Builders Shape Incentives</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#builders-shape-norms" data-astro-cid-xvrfupwn>Builders Shape Norms</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#technical-debt-accumulates" data-astro-cid-xvrfupwn>Technical Debt Accumulates</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#dual-use-technology-can-be-misused" data-astro-cid-xvrfupwn>Dual Use: Technology Can Be Misused</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#final-takeaway-why-ai-is-a-tool-not-a-destiny-ch40" data-astro-cid-xvrfupwn>Final Takeaway: Why AI Is a Tool, Not a Destiny</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#ai-does-not-have-agency" data-astro-cid-xvrfupwn>AI Does Not Have Agency</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#progress-is-not-inevitable" data-astro-cid-xvrfupwn>Progress Is Not Inevitable</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#responsibility-is-collective" data-astro-cid-xvrfupwn>Responsibility Is Collective</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#the-long-view-matters" data-astro-cid-xvrfupwn>The Long View Matters</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#engineers-shape-the-future" data-astro-cid-xvrfupwn>Engineers Shape the Future</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch40" data-astro-cid-xvrfupwn>References and Further Reading</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#datasheets-for-datasets---gebru-et-al-2018-microsoft-research" data-astro-cid-xvrfupwn>Datasheets for Datasets - Gebru et al. (2018), Microsoft Research</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#model-cards-for-model-reporting---mitchell-et-al-2019-google" data-astro-cid-xvrfupwn>Model Cards for Model Reporting - Mitchell et al. (2019), Google</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#fairness-and-abstraction-in-sociotechnical-systems---selbst-et-al-2019-data--society" data-astro-cid-xvrfupwn>Fairness and Abstraction in Sociotechnical Systems - Selbst et al. (2019), Data &amp; Society</a></li></ul></nav> </aside> </div>   </body> </html>