<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 4: The Bias-Variance Tradeoff | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part1/04-bias-variance-tradeoff/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part1/04-bias-variance-tradeoff/"><meta property="og:title" content="Chapter 4: The Bias-Variance Tradeoff | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part1/04-bias-variance-tradeoff/"><meta name="twitter:title" content="Chapter 4: The Bias-Variance Tradeoff | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part1" data-astro-cid-ilhxcym7>Part I: Foundations</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>The Bias-Variance Tradeoff</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-4-the-bias-variance-tradeoff">Chapter 4: The Bias-Variance Tradeoff</h1>
<h2 id="underfitting-when-models-are-too-simple-ch4">Underfitting: When Models Are Too Simple</h2>
<p>A model underfits when it‚Äôs too simple to capture the patterns in the data. It makes systematic errors because it lacks the flexibility to represent the true relationship between inputs and outputs. Underfitting is high bias‚Äîthe model is biased toward a particular form that doesn‚Äôt match reality.</p>
<p>Consider predicting house prices based on square footage. Suppose the true relationship is: price increases quickly for small houses, then more slowly for larger houses‚Äîa logarithmic curve. But you fit a horizontal line (predicting the same price for all houses). This model has high bias: it assumes price doesn‚Äôt depend on square footage, which is wrong. It systematically underestimates large houses and overestimates small ones.</p>
<p>Bias represents the error introduced by approximating a complex problem with a simpler model. Every model makes assumptions. Linear models assume linear relationships. Decision trees assume the space can be partitioned with axis-aligned splits. Neural networks with limited depth assume shallow feature compositions. When these assumptions don‚Äôt match the true data-generating process, you get bias.</p>
<p><strong>Model capacity</strong> determines how complex a function the model can represent. A linear model has low capacity‚Äîit can only represent lines and hyperplanes. A 10-degree polynomial has higher capacity‚Äîit can fit curves with many bends. A deep neural network has very high capacity‚Äîit can approximate arbitrary nonlinear functions. When your model‚Äôs capacity is too low for the complexity of the true function, you get bias.</p>
<p>Consider predicting whether a tumor is malignant from multiple medical features. If the true decision boundary is a complex, nonlinear surface in feature space, a linear classifier will make systematic errors. It cannot represent the boundary, so it settles for a poor linear approximation. This is high bias: the model‚Äôs assumptions (linearity) don‚Äôt match reality (nonlinearity).</p>
<p>Or consider a decision tree with maximum depth 2 trying to model a complex interaction between dozens of features. The tree can only make 3 sequential splits, creating at most 8 leaf nodes. If the true pattern requires considering many features jointly, the shallow tree cannot capture it. The model is biased toward simple decision rules when complex rules are needed.</p>
<p>High bias restricts the <strong>hypothesis space</strong>‚Äîthe set of functions the model can learn. Linear models restrict hypotheses to linear functions. By constraining the hypothesis space, you reduce variance (the model is less sensitive to training data) but increase bias (you may exclude the true function).</p>
<p>The signal of underfitting is poor performance on both training and test data. If your model can‚Äôt even fit the training data well, it‚Äôs too simple. The training error itself is high, not because of noise, but because the model fundamentally cannot represent the patterns that exist.</p>
<p>Common causes of underfitting:</p>
<ul>
<li><strong>Model too simple</strong>: Using a linear model when the relationship is highly nonlinear.</li>
<li><strong>Features insufficient</strong>: Missing important features that explain the outcome.</li>
<li><strong>Regularization too strong</strong>: Over-penalizing complexity, preventing the model from fitting even systematic patterns.</li>
</ul>
<p>Fixing underfitting requires increasing model capacity: use a more flexible model class, add more features, reduce regularization, or train longer. The goal is to give the model enough expressiveness to capture the true patterns.</p>
<h2 id="overfitting-when-models-are-too-complex-ch4">Overfitting: When Models Are Too Complex</h2>
<p>A model overfits when it‚Äôs so flexible that it memorizes the training data, including noise and outliers. It achieves near-perfect training accuracy but poor test accuracy because it has learned patterns specific to the training set that don‚Äôt generalize. Overfitting is high variance‚Äîsmall changes in the training data lead to large changes in the learned model.</p>
<p>Imagine fitting a 10th-degree polynomial to 15 data points. The polynomial can pass exactly through every point, achieving zero training error. But between the points, the curve oscillates wildly‚Äîshooting up and down to fit every quirk of the training set. On new data, the predictions are terrible because the model has memorized rather than learned.</p>
<p>Variance represents sensitivity to the training data. A high-variance model is excessively influenced by the specific examples it sees. If you retrain on a slightly different dataset‚Äîsame distribution, different samples‚Äîthe learned function changes dramatically. This instability means the model hasn‚Äôt converged on a reliable pattern; it‚Äôs chasing noise.</p>
<p>Consider k-nearest neighbors (k-NN) with k=1. For any new example, the model predicts the label of the single closest training example. If that training example happened to have a noisy or mislabeled outcome, the model reproduces the error. More critically, the decision boundary is jagged and irregular, wrapping tightly around each training point. A different random sample would produce a completely different boundary. This is high variance: the model‚Äôs predictions depend strongly on which specific examples were in the training set.</p>
<p>Or consider a decision tree grown without depth limits on a small dataset. The tree expands until each leaf contains one training example. It achieves perfect training accuracy by memorizing: ‚Äúif feature1=0.52 and feature2=1.3 and feature3=0.8, predict class A.‚Äù These hyper-specific rules are meaningless for new examples. Retrain on a different sample, and the tree structure changes completely. High variance.</p>
<p>To visualize variance, imagine training the same model architecture on multiple random subsamples from the same distribution. A low-variance model produces similar predictions across subsamples‚Äîit has identified the consistent patterns. A high-variance model produces wildly different predictions‚Äîeach subsample leads to different memorized details. Variance measures how much the learned function fluctuates with training set perturbations.</p>
<p>The signal of overfitting is a large gap between training and test performance. Training error is low (the model fits the training data well), but test error is high (it doesn‚Äôt generalize). This divergence indicates the model is learning dataset-specific details rather than transferable patterns.</p>
<p><img  src="/eng-ai/_astro/04-diagram.DHKnoa9a_gDyk8.svg" alt="Overfitting: When Models Are Too Complex diagram" width="500" height="300" loading="lazy" decoding="async"></p>
<p>The diagram shows the bias-variance tradeoff. As model complexity increases, training error decreases (the model fits training data better). But test error follows a U-curve: initially decreasing (reducing bias), then increasing (increasing variance). The optimal model complexity minimizes test error.</p>
<p>Common causes of overfitting:</p>
<ul>
<li><strong>Model too complex</strong>: Too many parameters relative to training data.</li>
<li><strong>Training too long</strong>: The model continues fitting training data past the point of generalization.</li>
<li><strong>Insufficient regularization</strong>: No penalty for complexity, allowing memorization.</li>
<li><strong>Noisy or mislabeled data</strong>: The model fits errors as if they were patterns.</li>
</ul>
<p>Fixing overfitting requires controlling complexity: use a simpler model, add regularization, collect more data, or stop training early. The goal is to constrain the model enough that it learns patterns but not so much that it memorizes specifics.</p>
<h2 id="why-you-cant-eliminate-both-ch4">Why You Can‚Äôt Eliminate Both</h2>
<p>The bias-variance tradeoff is fundamental: reducing one increases the other. You cannot simultaneously have a simple model (low variance) and a highly flexible model (low bias). You must choose where on the spectrum to operate.</p>
<p><strong>Increasing model complexity:</strong></p>
<ul>
<li><strong>Reduces bias</strong>: More flexible models can approximate more complex functions.</li>
<li><strong>Increases variance</strong>: More parameters mean more sensitivity to training data.</li>
</ul>
<p><strong>Decreasing model complexity:</strong></p>
<ul>
<li><strong>Increases bias</strong>: Simpler models make stronger assumptions that may be wrong.</li>
<li><strong>Reduces variance</strong>: Fewer parameters mean more stability across different training sets.</li>
</ul>
<p>This tradeoff is mathematical. The expected error of a model can be decomposed as:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Expected¬†Error</mtext><mo>=</mo><msup><mtext>Bias</mtext><mn>2</mn></msup><mo>+</mo><mtext>Variance</mtext><mo>+</mo><mtext>Irreducible¬†Error</mtext></mrow><annotation encoding="application/x-tex">\text{Expected Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Expected¬†Error</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9707em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord text"><span class="mord">Bias</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8873em;"><span style="top:-3.1362em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">Variance</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">Irreducible¬†Error</span></span></span></span></span></span>
<p>Where:</p>
<ul>
<li><strong>Bias</strong>: Error from incorrect modeling assumptions.</li>
<li><strong>Variance</strong>: Error from sensitivity to specific training samples.</li>
<li><strong>Irreducible error</strong>: Noise in the data-generating process that no model can eliminate.</li>
</ul>
<p>This decomposition reveals the tradeoff explicitly. As you increase model complexity:</p>
<ul>
<li>Bias¬≤ decreases: the model can fit more complex patterns, reducing systematic errors.</li>
<li>Variance increases: the model has more degrees of freedom, becoming sensitive to training noise.</li>
<li>Irreducible error remains constant: it‚Äôs a property of the problem, not the model.</li>
</ul>
<p>The total error is the sum. The optimal model complexity is the one that minimizes this sum‚Äîthe <strong>sweet spot</strong> where bias and variance are balanced.</p>
<p>Visualize the tradeoff as a curve: on the left (simple models), bias dominates‚Äîthe model can‚Äôt capture the true function. On the right (complex models), variance dominates‚Äîthe model overfits to noise. In the middle, total error is minimized. This is the Goldilocks principle: not too simple, not too complex, just right.</p>
<p>Because bias and variance are both components of error, minimizing total error requires balancing them. The optimal model is not the one with zero bias or zero variance‚Äîit‚Äôs the one that minimizes their sum.</p>
<p>In practice, this means:</p>
<ul>
<li><strong>With limited data</strong>: Prefer simpler models. High-variance models will overfit because there aren‚Äôt enough examples to constrain them. Accept some bias to avoid catastrophic variance.</li>
<li><strong>With abundant data</strong>: Use more complex models. Data reduces variance, so you can afford more flexibility to reduce bias. This is why deep learning requires massive datasets‚Äîneural networks are high-variance models that need data to prevent memorization.</li>
</ul>
<p>An intriguing modern discovery complicates this picture: the <strong>double descent phenomenon</strong>. Classical theory predicts test error follows a U-curve (the diagram above). But with very large overparameterized models (more parameters than training examples), test error can decrease again. After the overfitting regime, continuing to increase model size improves generalization. This ‚Äúdouble descent‚Äù curve suggests that models large enough to interpolate training data perfectly can still generalize if optimization finds simple solutions within the space of perfect fits. This is an active research area, but practically: modern deep learning often uses models far larger than classical theory would recommend.</p>
<p>The tradeoff also explains why ensembles work (Chapter 9). Averaging multiple high-variance models reduces variance without increasing bias, moving toward the optimal tradeoff point.</p>
<h2 id="how-modern-ml-fights-the-tradeoff-ch4">How Modern ML Fights the Tradeoff</h2>
<p>Modern machine learning uses several strategies to navigate the bias-variance tradeoff:</p>
<p><strong>1. More Data</strong></p>
<p>Data is the most effective way to reduce variance without increasing bias. More training examples constrain the model, preventing it from overfitting to noise. With limited data, even moderate model complexity causes high variance. With abundant data, you can use very complex models without overfitting. This is why deep learning exploded once large datasets became available‚Äîneural networks are high-variance models that require massive data to regularize.</p>
<p><strong>2. Regularization</strong></p>
<p>Regularization explicitly penalizes model complexity in the loss function. L2 regularization adds a penalty <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi><mo>‚àë</mo><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\lambda \sum w_i^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0728em;vertical-align:-0.2587em;"></span><span class="mord mathnormal">Œª</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-symbol small-op" style="position:relative;top:0em;">‚àë</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> to the loss, forcing weights to stay small unless they‚Äôre necessary. L1 regularization drives weights to zero, producing sparse models. Dropout randomly disables neurons during training, preventing co-adaptation. These techniques reduce variance by limiting flexibility. The regularization path‚Äîtracking training and test error as you vary <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">Œª</span></span></span></span>‚Äîreveals the bias-variance tradeoff empirically: strong regularization increases bias, weak regularization increases variance.</p>
<p>Regularization enforces compression (Chapter 3). By penalizing complexity, it forces the model to find simpler explanations of the data, which generalize better. This connects the compression view and the bias-variance view: good compression means low variance.</p>
<p><strong>3. Cross-Validation</strong></p>
<p>Cross-validation estimates test error without a separate test set by repeatedly training on different subsets of data and testing on held-out portions. This lets you tune hyperparameters (model complexity, regularization strength) to minimize estimated test error‚Äîfinding the sweet spot in the bias-variance tradeoff. K-fold cross-validation splits data into k folds, trains on k-1, tests on the remaining fold, and repeats k times. The average test error across folds estimates generalization performance.</p>
<p><strong>4. Early Stopping</strong></p>
<p>Training neural networks past the point of optimal generalization causes overfitting. Early stopping monitors validation error and stops training when it stops improving. This prevents the model from fitting training noise once it has learned the signal. Early stopping is a form of regularization: it limits the model‚Äôs effective capacity by restricting training iterations.</p>
<p><strong>5. Data Augmentation</strong></p>
<p>Creating synthetic training examples‚Äîrotating images, paraphrasing text, adding noise‚Äîeffectively increases data size without collecting new samples. This reduces variance by exposing the model to more variations, making it less sensitive to specific training examples. Augmentation teaches invariances: a rotated cat is still a cat. This reduces variance without increasing bias.</p>
<p><strong>6. Ensembling</strong></p>
<p>Averaging predictions from multiple models reduces variance. If each model has independent errors, the average cancels out the noise. Bagging (bootstrap aggregating) trains many models on random subsamples and averages predictions‚Äîit reduces variance. Boosting trains models sequentially, each correcting errors of previous models‚Äîit reduces bias. Random forests (Chapter 9) use bagging to convert high-variance decision trees into low-variance ensembles.</p>
<p><strong>7. Architecture Design</strong></p>
<p>Neural network architectures encode inductive biases‚Äîassumptions about the problem structure. Convolutional networks assume spatial locality (nearby pixels are related). Recurrent networks assume sequential dependencies. Attention mechanisms assume relevance-weighted aggregation. These biases constrain the hypothesis space, reducing variance while keeping bias manageable if the assumptions are correct. Architecture choice is a form of regularization through structure rather than explicit penalties.</p>
<h2 id="engineering-takeaway-ch4">Engineering Takeaway</h2>
<p>The bias-variance tradeoff explains most machine learning failures and suggests how to fix them.</p>
<p><strong>Diagnose by comparing training and test error.</strong> This is the single most important diagnostic for ML models:</p>
<ul>
<li><strong>High training error, high test error</strong>: Underfitting (high bias). The model is too simple. Increase model capacity, add features, reduce regularization, or train longer.</li>
<li><strong>Low training error, high test error</strong>: Overfitting (high variance). The model memorizes training data. Add regularization, collect more data, simplify the model, or use early stopping.</li>
<li><strong>Low training error, low test error</strong>: Good fit. The model has found the right balance. Monitor for distribution shift over time.</li>
</ul>
<p><strong>Use validation sets to tune hyperparameters.</strong> You cannot see overfitting by looking at training error alone. A separate validation set (or cross-validation) estimates how the model will perform on unseen data. Use validation error to tune model complexity, regularization strength, learning rate, and architecture choices. The model that minimizes validation error is at the sweet spot of the bias-variance tradeoff.</p>
<p><strong>Prioritize more data over better models.</strong> If you‚Äôre overfitting, getting more training data is often more effective than tuning the model. Data directly reduces variance by constraining what the model can learn. Algorithmic improvements offer diminishing returns compared to 10x-ing your dataset. Before trying a fancier algorithm, ask: can I collect or generate more training examples?</p>
<p><strong>Regularization is not optional in production.</strong> Almost all production models use regularization to prevent overfitting. The strength of regularization (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">Œª</span></span></span></span>) is a hyperparameter you tune on validation data. Too much regularization causes underfitting; too little causes overfitting. Find the middle ground empirically. Use cross-validation to find the optimal regularization strength systematically.</p>
<p><strong>Understand the tradeoff for your data regime.</strong> If you have limited data (hundreds to thousands of examples), bias-variance tradeoff is sharp. Small increases in model complexity cause large increases in variance. Use simpler models and strong regularization. If you have massive data (millions of examples), the tradeoff is gentler. Data suppresses variance, allowing more complex models. Scale model capacity with data size.</p>
<p><strong>Think in tradeoffs, not absolutes.</strong> There‚Äôs no such thing as a universally ‚Äúgood‚Äù model. A model is good or bad relative to the amount of data you have, the complexity of the problem, and the cost of different types of errors. Always ask: where should I be on the bias-variance spectrum for this problem? The answer depends on your data size, problem complexity, and deployment constraints.</p>
<p><strong>Monitor generalization continuously in production.</strong> Even after deployment, models can drift into overfitting or underfitting as the data distribution changes. Monitor test metrics in production. If performance degrades, retrain with recent data or adjust model complexity. The bias-variance tradeoff is not static‚Äîit changes as your data evolves.</p>
<p>The lesson: All machine learning is a negotiation between bias and variance. You cannot eliminate both. The art of machine learning is finding the model complexity that minimizes their sum for your specific data and problem. Master this tradeoff, and you understand most of what matters in applied ML.</p>
<hr>
<h2 id="references-and-further-reading-ch4">References and Further Reading</h2>
<p><strong>Understanding the Bias-Variance Tradeoff</strong> ‚Äì Scott Fortmann-Roe
<a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></p>
<p>This is one of the clearest visual explanations of the bias-variance tradeoff available. Fortmann-Roe uses interactive diagrams to show how bias and variance contribute to error and how they trade off as model complexity increases. Reading this will give you intuition for why all ML failures come down to being on the wrong side of this tradeoff. Essential reading for anyone learning machine learning.</p>
<p><strong>The Elements of Statistical Learning, Chapter 7</strong> ‚Äì Hastie, Tibshirani, Friedman
<a href="https://hastie.su.domains/ElemStatLearn/">https://hastie.su.domains/ElemStatLearn/</a></p>
<p>This is the canonical textbook treatment of model selection and the bias-variance tradeoff. Chapter 7 covers bootstrap methods, cross-validation, and the decomposition of error into bias and variance. It‚Äôs mathematical but readable. Understanding this chapter gives you the statistical foundation for choosing model complexity and evaluating generalization.</p>
<p><strong>Ensemble Methods in Machine Learning</strong> ‚Äì Thomas Dietterich (2000)
<a href="https://link.springer.com/chapter/10.1007/3-540-45014-9_1">https://link.springer.com/chapter/10.1007/3-540-45014-9_1</a></p>
<p>This paper explains how ensemble methods (bagging, boosting, stacking) navigate the bias-variance tradeoff. Bagging reduces variance by averaging high-variance models. Boosting reduces bias by sequentially correcting errors. The paper provides theoretical analysis and empirical results showing why ensembles outperform single models. Understanding this connects the bias-variance tradeoff to one of the most effective techniques in practice‚Äîcombining multiple models to get the best of both worlds.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part1/03-models-are-compression-machines" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Models Are Compression Machines</span> </a> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Features: How Machines See the World</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#underfitting-when-models-are-too-simple-ch4" data-astro-cid-xvrfupwn>Underfitting: When Models Are Too Simple</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#overfitting-when-models-are-too-complex-ch4" data-astro-cid-xvrfupwn>Overfitting: When Models Are Too Complex</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-you-cant-eliminate-both-ch4" data-astro-cid-xvrfupwn>Why You Can‚Äôt Eliminate Both</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#how-modern-ml-fights-the-tradeoff-ch4" data-astro-cid-xvrfupwn>How Modern ML Fights the Tradeoff</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch4" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch4" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>