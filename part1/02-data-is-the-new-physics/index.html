<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 2: Data Is the New Physics | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part1/02-data-is-the-new-physics/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part1/02-data-is-the-new-physics/"><meta property="og:title" content="Chapter 2: Data Is the New Physics | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part1/02-data-is-the-new-physics/"><meta name="twitter:title" content="Chapter 2: Data Is the New Physics | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part1" data-astro-cid-ilhxcym7>Part I: Foundations</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Data Is the New Physics</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-2-data-is-the-new-physics">Chapter 2: Data Is the New Physics</h1>
<h2 id="why-models-dont-discover-laws-ch2">Why Models Don‚Äôt Discover Laws</h2>
<p>Physics discovers laws. Newton‚Äôs law of gravitation, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mi>G</mi><mfrac><mrow><msub><mi>m</mi><mn>1</mn></msub><msub><mi>m</mi><mn>2</mn></msub></mrow><msup><mi>r</mi><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">F = G \frac{m_1 m_2}{r^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0565em;vertical-align:-0.345em;"></span><span class="mord mathnormal">G</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7115em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>, is a compact mathematical statement that describes how all masses attract each other, everywhere, always. It‚Äôs universal, causal, and predictive. Once discovered, it applies to situations never observed before‚Äîthe motion of distant planets, the trajectory of satellites, the formation of galaxies.</p>
<p>Machine learning does not work this way. Machine learning models do not discover laws‚Äîthey approximate functions. A model trained to predict housing prices has learned a statistical relationship between features (square footage, location, bedrooms) and prices in the training data. It has not discovered the underlying economics of supply and demand, construction costs, or urban planning that actually determine prices. It has only learned the patterns that happened to occur in the data.</p>
<p>This distinction is fundamental. Physics explains why things happen. Machine learning predicts what will happen based on what has happened before. Physics seeks generalizable principles. Machine learning seeks predictive accuracy within a data distribution.</p>
<p>Consider a model predicting when a user will click on an ad. The model might learn that users on mobile devices in the evening are more likely to click. But it doesn‚Äôt know why‚Äîmaybe people browse casually in the evening, maybe they‚Äôre bored, maybe the lighting makes screens more comfortable to read. The model just knows the correlation exists in the training data.</p>
<p>If the pattern changes‚Äîif a new app shifts user behavior, or if ad placement algorithms evolve‚Äîthe model‚Äôs predictions degrade. It hasn‚Äôt learned a principle that transcends the data distribution. It has learned the data distribution itself.</p>
<p>The failure to discover universal laws manifests in systematic ways. A face recognition model trained predominantly on one demographic fails on others. It hasn‚Äôt learned universal principles of facial geometry‚Äîit has learned the statistical patterns in its training set. A language model trained on English text generates coherent English but fails on code-switched text mixing languages. The patterns it learned are dataset-specific, not universal.</p>
<p>Newton‚Äôs laws work on Earth and on Mars. A machine learning model trained on Earth data has no guarantee of working on Mars data. This isn‚Äôt a bug‚Äîit‚Äôs the fundamental nature of learning from data rather than discovering principles. Machine learning trades generality for practicality: we accept dataset-specific patterns because discovering universal laws is often impossible, and dataset-specific patterns are good enough when the dataset is representative.</p>
<p>This is why machine learning models require continuous retraining. The world changes. User behavior evolves. Market conditions shift. A model trained last year may be obsolete today, not because it was poorly designed, but because the patterns it learned no longer hold. Physics doesn‚Äôt have this problem‚Äîgravity hasn‚Äôt changed.</p>
<h2 id="why-more-data-beats-better-code-ch2">Why More Data Beats Better Code</h2>
<p>In machine learning, the quality and quantity of data matter more than the sophistication of the algorithm. A simple model trained on a million examples will typically outperform a complex model trained on a thousand examples. This is counterintuitive for engineers trained to believe that better algorithms solve problems, but it‚Äôs one of the most empirically validated findings in machine learning.</p>
<p>Consider machine translation. Early systems used rule-based approaches: linguists hand-coded grammatical transformations between languages. These systems were sophisticated but brittle‚Äîthey worked for the language pairs and constructions they were designed for, but failed on anything unexpected.</p>
<p>Then statistical machine translation emerged. Instead of rules, these systems learned from large corpora of translated text. They didn‚Äôt understand grammar or semantics‚Äîthey just counted co-occurrence patterns. ‚ÄúLe chien‚Äù appears near ‚Äúthe dog‚Äù in aligned French-English texts, so the model learns to translate one to the other.</p>
<p>These statistical models, despite being algorithmically simpler, outperformed rule-based systems once enough parallel text became available. Google‚Äôs systems improved dramatically not by inventing new algorithms, but by using the entire web as training data. More data revealed more patterns‚Äîidioms, rare constructions, domain-specific terminology‚Äîthat no linguist could have anticipated.</p>
<p>The same pattern repeats across domains. Speech recognition improved with more transcribed audio. Image classification improved with ImageNet‚Äôs millions of labeled images. Recommendation systems improved with more user interaction data. In each case, the algorithmic innovations mattered less than the scale of the data.</p>
<p>ImageNet illustrates this effect concretely. Released in 2009, ImageNet provided 1.2 million labeled images across 1,000 categories‚Äîorders of magnitude larger than previous datasets. This scale enabled convolutional neural networks to learn robust visual features that generalized across contexts. AlexNet‚Äôs 2012 breakthrough on ImageNet wasn‚Äôt primarily an algorithmic innovation‚Äîit was the combination of CNNs (known since the 1990s) with sufficient data and compute. The data unlocked the model‚Äôs potential.</p>
<p>Modern language models demonstrate the same scaling law. GPT-2 (2019) had 1.5 billion parameters and was trained on 40GB of text. GPT-3 (2020) had 175 billion parameters and was trained on 570GB of text. The algorithmic differences were minor‚Äîboth were Transformer-based autoregressive models. The performance difference was dramatic: GPT-3 could perform tasks GPT-2 couldn‚Äôt, primarily because it had seen vastly more data.</p>
<p>The Chinchilla scaling law (2022) made this precise: optimal performance requires scaling model size and training data equally. If you double compute budget, you should roughly double both parameters and training tokens. Prior models had been undertrained‚Äîthey had sufficient parameters but insufficient data. Chinchilla, with 70B parameters trained on 1.4 trillion tokens, outperformed much larger models trained on less data. The lesson: data matters as much as model capacity.</p>
<p>Why does more data help so much? Because data reduces uncertainty. With limited data, many functions fit the observations‚Äîyou can‚Äôt tell which patterns are real and which are noise. With abundant data, the true patterns emerge consistently across examples, while noise averages out. The model can learn finer distinctions, rarer patterns, and more robust representations.</p>
<p>This has a practical implication: if your model isn‚Äôt performing well, your first instinct should be to get more data, not to tune hyperparameters or try a fancier algorithm. More data gives the model more signal to learn from. Algorithmic improvements offer diminishing returns compared to doubling or 10x-ing your training set.</p>
<h2 id="noise-vs-signal-ch2">Noise vs Signal</h2>
<p>Data is never perfect. Every dataset contains both signal‚Äîthe true patterns you want to learn‚Äîand noise‚Äîrandom variations, measurement errors, and irrelevant correlations. Learning means extracting the signal while ignoring the noise. This is harder than it sounds.</p>
<p><strong>Signal</strong> is the systematic, repeatable pattern. In housing price data, the signal includes relationships like ‚Äúlarger houses cost more‚Äù and ‚Äúhouses near parks command a premium.‚Äù These patterns hold across many examples and generalize to new data.</p>
<p><strong>Noise</strong> is the random variation. Two identical houses might sell for slightly different prices depending on the buyer‚Äôs urgency, the season, negotiation skills, or luck. This variation is real but unpredictable‚Äîit can‚Äôt be learned because it doesn‚Äôt repeat.</p>
<p>The challenge is that models can‚Äôt automatically distinguish signal from noise. Both show up as patterns in the data. A model can easily overfit to noise if the noise happens to correlate with outcomes in the training set by chance. With limited data, spurious patterns look just as valid as real ones.</p>
<p>Consider a medical diagnosis model trained on 100 patients. Suppose 3 patients with rare last names happened to have the disease. The model might learn that certain last names predict the disease‚Äînot because of any biological mechanism, but because of random chance. With more data, this pattern would disappear (it‚Äôs noise), but with 100 examples, it looks like signal.</p>
<p>This is exacerbated by <strong>sampling bias</strong>‚Äîwhen your training data doesn‚Äôt represent the full population. Surveys suffer from this: people who respond to surveys differ systematically from those who don‚Äôt (response bias). Medical studies trained on clinical trial volunteers may not generalize to the broader patient population‚Äîvolunteers tend to be healthier, more compliant, and from different demographics than typical patients.</p>
<p>Web scraping introduces sampling bias because the data you can scrape reflects who uses the platform and how they use it, not the broader population. A sentiment analysis model trained on Twitter data learns patterns from Twitter‚Äôs specific user demographics, which skew younger and more politically engaged than the general population. Applying this model to customer feedback from your product may fail because your customers have different communication styles.</p>
<p>If you train a hiring model on historical data from a company that predominantly hired from certain schools, the model learns that those schools predict success. But this might reflect historical hiring bias, not actual predictive value. The model can‚Äôt tell the difference between ‚Äúpeople from School X perform better‚Äù (signal) and ‚Äúpeople from School X were hired more often due to bias‚Äù (sampling artifact).</p>
<p><strong>Selection bias</strong> occurs when your data collection process systematically excludes certain outcomes. A model predicting customer lifetime value trained only on customers who completed registration misses patterns about why people abandon registration. A credit scoring model trained only on approved loans (because you only observe default rates for loans you granted) systematically underestimates risk for marginal applications you rejected.</p>
<p><strong>Label noise</strong> is another critical issue. Real-world labels are often ambiguous or inconsistent. In content moderation, different annotators disagree on whether content violates policies‚Äîwhat looks like hate speech to one person might look like political discourse to another. This inter-annotator disagreement means your training labels contain errors. The model will try to fit these errors as if they were signal.</p>
<p>Even with good labels, <strong>data quality issues</strong> degrade signal. Missing values (users who don‚Äôt fill in age fields), outliers (data entry errors like houses listed at $1), duplicates (the same example appearing multiple times, artificially inflating its importance), and inconsistencies (addresses formatted differently) all inject noise. Cleaning data to remove these issues is often the highest-leverage work in a machine learning project.</p>
<p><strong>Measurement error</strong> is another source of noise. If your labels are incorrect‚Äîspam emails mislabeled as legitimate, or vice versa‚Äîthe model learns to reproduce those errors. Garbage in, garbage out. Data quality matters more than data quantity if the data is systematically wrong.</p>
<p>The implication: clean, representative data is more valuable than vast amounts of noisy, biased data. You can‚Äôt learn signal that isn‚Äôt there. If your data is biased, your model will be biased. If your labels are wrong, your model will learn the wrong patterns. Data engineering‚Äîcollecting, cleaning, and validating data‚Äîis often the most important part of building ML systems.</p>
<h2 id="irreducible-error-ch2">Irreducible Error</h2>
<p>Some things cannot be predicted, no matter how much data you have or how sophisticated your model is. There is irreducible error‚Äîrandomness inherent in the world that cannot be eliminated by better prediction.</p>
<p>Consider predicting tomorrow‚Äôs weather. Meteorology is a mature science with vast amounts of data, powerful models, and deep understanding of atmospheric physics. Yet forecasts beyond 7-10 days are unreliable. Why? Because weather is a chaotic system. Tiny differences in initial conditions‚Äîunmeasurable fluctuations in temperature or pressure‚Äîamplify over time, making long-term prediction impossible.</p>
<p>This isn‚Äôt a failure of modeling. It‚Äôs a fundamental property of the system. No amount of data will let you predict next month‚Äôs weather with certainty because the system is inherently unpredictable beyond a certain time horizon.</p>
<p>The same applies to many machine learning problems. Predicting which specific users will click on an ad is fundamentally uncertain‚Äîhuman behavior has random components that can‚Äôt be captured by features. Predicting whether a specific loan will default is uncertain‚Äîlife events (job loss, illness, divorce) are unpredictable. Predicting stock prices is uncertain‚Äîmarkets reflect the collective unpredictability of millions of actors.</p>
<p>Flipping a fair coin has 50% irreducible error. No model can predict the outcome better than chance because the outcome is determined by physical randomness (exact force, air resistance, rotation) that can‚Äôt be measured precisely enough. Even if you could measure everything, quantum uncertainty imposes fundamental limits. Some processes are simply random.</p>
<p>In contrast, some prediction tasks have low irreducible error but high difficulty. Predicting whether an image contains a cat has low irreducible error‚Äîhumans agree nearly 100% of the time, meaning the ‚Äútrue‚Äù label is well-defined. The challenge is extracting the signal (learning what ‚Äúcat‚Äù means), not dealing with randomness. Predicting stock prices has high irreducible error‚Äîeven perfect information about the past doesn‚Äôt determine the future because prices depend on future information and aggregate human decisions.</p>
<p>This concept is formalized as <strong>Bayes error rate</strong>‚Äîthe lowest possible error achievable by any predictor, even with infinite data and perfect knowledge of the data distribution. It represents the irreducible error inherent in the problem. If human experts disagree 20% of the time on whether a medical image shows a tumor, the Bayes error rate is at least 20%‚Äîno model can do better than the ground truth that defines ‚Äúcorrect.‚Äù</p>
<p>This irreducible error sets a ceiling on what models can achieve. If the best possible model can only predict with 80% accuracy due to inherent randomness, you won‚Äôt achieve 95% accuracy by trying harder. You need to accept that uncertainty is part of the problem.</p>
<p>The total error in a model‚Äôs predictions can be decomposed into three sources:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Total¬†Error</mtext><mo>=</mo><msup><mtext>Bias</mtext><mn>2</mn></msup><mo>+</mo><mtext>Variance</mtext><mo>+</mo><mtext>Irreducible¬†Error</mtext></mrow><annotation encoding="application/x-tex">\text{Total Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">Total¬†Error</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9707em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord text"><span class="mord">Bias</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8873em;"><span style="top:-3.1362em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">Variance</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">Irreducible¬†Error</span></span></span></span></span></span>
<ul>
<li><strong>Bias</strong>: Error from incorrect assumptions in the model (e.g., assuming a linear relationship when it‚Äôs nonlinear).</li>
<li><strong>Variance</strong>: Error from sensitivity to training data (e.g., overfitting to noise).</li>
<li><strong>Irreducible error</strong>: Error from randomness in the true data-generating process.</li>
</ul>
<p>You can reduce bias by using more flexible models. You can reduce variance by using more data or regularization. But you cannot reduce irreducible error. It‚Äôs a property of the problem, not the solution.</p>
<p>In practice, this means you must design systems that operate under uncertainty. Don‚Äôt expect perfect predictions. Build confidence intervals. Use probabilistic outputs rather than deterministic classifications. Route uncertain cases to human review. Accept that some errors are unavoidable and design your system to be robust to them.</p>
<h2 id="engineering-takeaway-ch2">Engineering Takeaway</h2>
<p>Data is the foundation of machine learning systems. The model is secondary. If you have good data, even a simple model will work. If your data is poor, no amount of algorithmic sophistication will save you.</p>
<p><strong>Invest in data infrastructure and pipelines.</strong> Building pipelines to collect, store, version, and serve data is often more important than choosing algorithms. Companies with better data infrastructure‚Äîlogging, tracking, labeling, versioning‚Äîbuild better models. Invest in instrumentation that captures ground truth labels, user feedback, and edge cases. Data versioning ensures reproducibility: you should be able to reproduce any model by knowing exactly what data it was trained on.</p>
<p><strong>Prioritize data quality over quantity.</strong> A thousand clean, correctly labeled, representative examples are worth more than a million noisy, biased examples. Before scaling data collection, ensure your data is trustworthy. Check for label errors, sampling biases, and data leakage (where the model accidentally sees information it shouldn‚Äôt have at inference time). Audit your data for systematic errors‚Äîduplicates, missing values, outliers, inconsistencies‚Äîand clean them before training.</p>
<p><strong>Monitor data distribution in production.</strong> If your training data comes from one distribution (e.g., users in the US) but you deploy to another (users in Europe), the model will underperform. Collect data from the same distribution you‚Äôll deploy to. Deploy monitoring to detect distribution shift: track summary statistics of input features over time. When the model starts seeing data unlike its training set, retrain with recent data.</p>
<p><strong>Use data augmentation to increase effective data size.</strong> If collecting more real data is expensive, augment existing data. For images: rotate, crop, adjust brightness. For text: synonym replacement, back-translation. For time series: add noise, warp time axis. Augmentation teaches the model invariances (a cat rotated is still a cat) and reduces overfitting without requiring new labeled examples.</p>
<p><strong>Apply active learning when labeling is expensive.</strong> Don‚Äôt label data randomly. Use the model to identify examples it‚Äôs most uncertain about, then label those. This targets labeling effort where it reduces error most. Active learning can achieve the same performance with 10x less labeled data by focusing on the decision boundary rather than labeling redundant examples.</p>
<p><strong>Engineer features from domain knowledge.</strong> The raw data you have might not be the data your model needs. Transforming raw data into useful features‚Äîextracting time of day from timestamps, combining fields to create interaction terms, binning continuous variables‚Äîis often the difference between a mediocre model and a great one. Engineers who understand the domain can create features that make the signal clearer and reduce the model‚Äôs need to discover everything from scratch.</p>
<p><strong>Default to ‚Äúget more data‚Äù over ‚Äúimprove algorithm.‚Äù</strong> If your model isn‚Äôt performing well, your first move should be: can I get more training data? Can I label more examples? Can I collect more diverse examples to reduce bias? Algorithmic improvements have diminishing returns. Data improvements scale. A simple model on 10x data usually beats a sophisticated model on current data.</p>
<p>The lesson: Machine learning is not about finding clever algorithms. It‚Äôs about having good data and extracting its patterns reliably. The algorithm is just the tool. The data is the raw material. No tool can build something great from poor raw materials.</p>
<hr>
<h2 id="references-and-further-reading-ch2">References and Further Reading</h2>
<p><strong>The Unreasonable Effectiveness of Data</strong> ‚Äì Alon Halevy, Peter Norvig, Fernando Pereira (2009)
<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf">https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf</a></p>
<p>This paper from Google researchers demonstrates that simple models trained on massive datasets outperform sophisticated models trained on small datasets. The authors provide evidence from machine translation, speech recognition, and other domains where scaling data‚Äînot improving algorithms‚Äîdrove progress. Reading this will fundamentally shift how you prioritize work in machine learning projects.</p>
<p><strong>The Lack of A Priori Distinctions Between Learning Algorithms</strong> ‚Äì David Wolpert (1996)
<a href="https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf">https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf</a></p>
<p>This is the formal statement of the ‚ÄúNo Free Lunch‚Äù theorem, which proves that no machine learning algorithm is universally better than any other across all possible problems. The implication: the quality of your data and how well it represents the problem matters more than which algorithm you choose. Algorithms matter only relative to specific problem structures. Understanding this prevents cargo-culting‚Äîcopying algorithms that worked elsewhere without understanding whether they fit your data.</p>
<p><strong>ImageNet: A Large-Scale Hierarchical Image Database</strong> ‚Äì Jia Deng et al. (2009)
<a href="https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf">https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf</a></p>
<p>ImageNet provided the first truly large-scale labeled image dataset (1.2 million images, 1,000 categories) and catalyzed the deep learning revolution. This paper demonstrates how dataset scale unlocks model capabilities‚Äîprior algorithms couldn‚Äôt leverage large data, and prior datasets couldn‚Äôt train powerful models. ImageNet bridged this gap and enabled CNNs to learn robust visual features. The ImageNet challenge drove five years of rapid progress in computer vision, showing how shared benchmark datasets accelerate research.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Why Intelligence Is Not Magic</span> </a> <a href="/eng-ai/part1/03-models-are-compression-machines" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Models Are Compression Machines</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-models-dont-discover-laws-ch2" data-astro-cid-xvrfupwn>Why Models Don‚Äôt Discover Laws</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-more-data-beats-better-code-ch2" data-astro-cid-xvrfupwn>Why More Data Beats Better Code</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#noise-vs-signal-ch2" data-astro-cid-xvrfupwn>Noise vs Signal</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#irreducible-error-ch2" data-astro-cid-xvrfupwn>Irreducible Error</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch2" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch2" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>