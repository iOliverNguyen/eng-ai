<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 3: Models Are Compression Machines | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part1/03-models-are-compression-machines/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part1/03-models-are-compression-machines/"><meta property="og:title" content="Chapter 3: Models Are Compression Machines | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part1/03-models-are-compression-machines/"><meta name="twitter:title" content="Chapter 3: Models Are Compression Machines | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part1" data-astro-cid-ilhxcym7>Part I: Foundations</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Models Are Compression Machines</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-3-models-are-compression-machines">Chapter 3: Models Are Compression Machines</h1>
<h2 id="what-a-trained-model-really-is-ch3">What a Trained Model Really Is</h2>
<p>A trained machine learning model is a compressed representation of the training data. It distills millions of training examples into a set of parameters‚Äîweights, biases, decision thresholds‚Äîthat capture the essential patterns while discarding the noise and details. This compression is not a side effect of learning; it is learning.</p>
<p>Consider a linear regression model trained on 100,000 housing price examples. The dataset might be hundreds of megabytes: addresses, sale dates, square footages, prices. The trained model is just a handful of numbers‚Äîmaybe 10 weights and a bias. These 11 numbers encode what the model learned from 100,000 examples.</p>
<p>This is radical compression. The model has gone from 100,000 specific facts (this house sold for this price) to 11 general rules (square footage contributes this much to price, location contributes that much). The model cannot reproduce the training data exactly‚Äîit has thrown away the details. But it can approximate the patterns well enough to make useful predictions on new examples.</p>
<p>The size of a model relative to its training data is a measure of compression ratio. A neural network with 1 million parameters trained on 1 billion examples has compressed the data by a factor of 1,000x. A decision tree with 50 leaf nodes trained on 10,000 examples has compressed by 200x. The compression ratio reflects how much generalization is happening: higher compression means more patterns are being abstracted away from specific examples.</p>
<p>Modern language models demonstrate extreme compression. GPT-3 has 175 billion parameters (each a 32-bit float, totaling ~700GB uncompressed) and was trained on roughly 300 billion tokens (about 2TB of text). The compression ratio is roughly 3x‚Äîthe model has distilled 2TB of text into 700GB of weights. But this understates the compression: GPT-3 can generate coherent text on topics not in its training data, meaning it has learned compressible patterns (grammar, facts, reasoning styles) rather than memorizing strings.</p>
<p>Different architectures impose different compression constraints. Convolutional neural networks (CNNs) for vision have built-in assumptions about spatial locality‚Äînearby pixels are correlated. This architectural prior reduces the parameter count needed to model images compared to fully connected networks. Recurrent networks for sequences assume temporal dependencies. These architectural choices are forms of compression: they restrict the hypothesis space to functions that align with domain structure, enabling better generalization from less data.</p>
<p>This perspective‚Äîlearning as compression‚Äîis not just a metaphor. It‚Äôs a formal framework from information theory. The Minimum Description Length (MDL) principle formalizes this: the best model is the one that minimizes the total description length of the model plus the data given the model. A simple model with poor fit requires many bits to encode the residual errors. A complex model that fits perfectly requires many bits to encode its parameters. The optimal model balances these: enough complexity to capture patterns, enough simplicity to avoid encoding noise.</p>
<h2 id="pattern-discovery-as-compression-ch3">Pattern Discovery as Compression</h2>
<p>Compression works by finding regularities. A file compressor looks for repeated sequences‚Äîif ‚Äúthe‚Äù appears 1,000 times in a document, the compressor encodes it once and references it repeatedly. A machine learning model does something similar: it finds patterns that recur across examples and encodes them as parameters.</p>
<p>Consider learning to predict the next element in a sequence: 2, 4, 6, 8, 10, ‚Ä¶</p>
<p>A naive encoding stores each number explicitly: 5 numbers, each requiring a few bytes. But if you recognize the pattern‚Äî‚Äústart at 2, add 2 each time‚Äù‚Äîyou can encode the entire sequence with just two numbers: the starting value and the increment. This is compression through pattern discovery.</p>
<p>Now consider a less obvious sequence: 3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9, ‚Ä¶</p>
<p>This is the digits of œÄ. If you know the rule‚Äîcompute œÄ and extract digits‚Äîyou can compress the sequence into an algorithm. But if you don‚Äôt recognize this pattern, you‚Äôre stuck storing each digit individually. The sequence looks random even though it‚Äôs not.</p>
<p>This notion is formalized by Kolmogorov complexity: the complexity of a string is the length of the shortest program that generates it. The sequence 2, 4, 6, 8, 10 has low Kolmogorov complexity (short program: <code>for i in range(5): print(2*(i+1))</code>). The digits of œÄ also have low complexity (there‚Äôs a formula for computing œÄ). A truly random sequence has high complexity‚Äîit cannot be compressed; the shortest program is ‚Äúprint these specific digits.‚Äù</p>
<p>Machine learning models do this automatically. They search for compressible patterns in data. When they find them, they encode them as parameters. A language model trained on English text learns that ‚Äúthe‚Äù is common, that ‚ÄúNew York‚Äù often appears together, that verbs follow subjects. These patterns allow the model to compress text‚Äîto represent it with fewer parameters than the raw character sequence would require.</p>
<p>Language models perform next-word prediction, which is equivalent to compression. Given text ‚ÄúThe cat sat on the ___‚Äù, a model assigns high probability to ‚Äúmat‚Äù and low probability to ‚Äútheorem.‚Äù This probability distribution compresses the data: common continuations are assigned short codes (high probability), rare continuations get long codes (low probability). The better the model predicts, the more it compresses. Lossless compression algorithms like gzip use this principle explicitly‚Äîthey build a probability model and encode symbols with lengths inverse to their predicted probability.</p>
<p>Machine learning is lossy compression. Unlike gzip, which can perfectly reconstruct the original data, a trained model discards details. It cannot reproduce each training example exactly‚Äîit has forgotten the specifics and retained only the patterns. This is not a bug; it‚Äôs essential. Lossless compression memorizes. Lossy compression generalizes. By discarding instance-specific noise, the model retains only the transferable signal.</p>
<p>The quality of compression reflects the quality of learning. If the model compresses well‚Äîcaptures the patterns with few parameters‚Äîit has learned something general. If it compresses poorly‚Äîrequires many parameters to fit the data‚Äîit‚Äôs memorizing rather than learning.</p>
<h2 id="occams-razor-ch3">Occam‚Äôs Razor</h2>
<p>Occam‚Äôs Razor is the principle that simpler explanations are more likely to be true. In machine learning, this translates to: simpler models generalize better. A model with fewer parameters is less likely to overfit because it cannot encode complex, dataset-specific idiosyncrasies. It‚Äôs forced to find broad patterns that transfer to new data.</p>
<p>This is why regularization works. Regularization adds a penalty to the loss function that punishes model complexity. For a linear model, L2 regularization (ridge regression) penalizes large weights:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mtext>total</mtext></msub><mo>=</mo><msub><mi>L</mi><mtext>data</mtext></msub><mo>+</mo><mi>Œª</mi><munder><mo>‚àë</mo><mi>i</mi></munder><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">L_{\text{total}} = L_{\text{data}} + \lambda \sum_{i} w_i^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">total</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">data</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.3277em;vertical-align:-1.2777em;"></span><span class="mord mathnormal">Œª</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">‚àë</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mtext>data</mtext></msub></mrow><annotation encoding="application/x-tex">L_{\text{data}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">data</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the error on training data and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">Œª</span></span></span></span> controls the strength of the penalty. This forces the model to keep weights small unless they‚Äôre truly necessary to fit the data. The result is a simpler model that generalizes better.</p>
<p>L1 regularization (lasso) uses absolute values instead: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi><msub><mo>‚àë</mo><mi>i</mi></msub><mi mathvariant="normal">‚à£</mi><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">‚à£</mi></mrow><annotation encoding="application/x-tex">\lambda \sum_i |w_i|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="mord mathnormal">Œª</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">‚àë</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">‚à£</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">‚à£</span></span></span></span>. This encourages sparsity‚Äîmany weights are driven to exactly zero, effectively removing features from the model. Sparse models are interpretable and fast: you can ignore zero-weight features entirely. L1 is useful when you have many features and suspect most are irrelevant.</p>
<p>Dropout, used in neural networks, randomly disables neurons during training. This prevents the network from relying on any single neuron‚Äîit must learn redundant representations. Dropout acts as regularization by reducing the effective capacity of the network. At test time, dropout is off, but the redundancy learned during training makes predictions robust.</p>
<p>Early stopping is another form of regularization. Even without explicit penalties, training a model for too many iterations causes overfitting‚Äîit starts memorizing training examples rather than learning patterns. Early stopping monitors validation error and stops training when it stops improving, even if training error is still decreasing. This prevents the model from using its full capacity to overfit.</p>
<p>Cross-validation provides a systematic way to tune regularization strength. You split data into folds, train on some folds, validate on others, and repeat. For each setting of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">Œª</span></span></span></span>, you measure average validation error across folds. The <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">Œª</span></span></span></span> that minimizes validation error balances underfitting (too much regularization) and overfitting (too little regularization). Cross-validation finds the right compression level: enough complexity to capture signal, enough simplicity to ignore noise.</p>
<p>Why does simplicity improve generalization? Because complex models can fit spurious patterns‚Äîpatterns that happened to occur in the training data by chance but don‚Äôt reflect the underlying process. A complex model with many parameters can bend and twist to fit every quirk of the training set, including the noise. A simple model cannot. It‚Äôs constrained to find only the strongest, most consistent patterns.</p>
<p>Consider fitting a polynomial to data points. A degree-1 polynomial (a line) is simple but might underfit. A degree-10 polynomial is complex and can fit every training point exactly‚Äîbut it will wildly oscillate between points, fitting noise rather than signal. A degree-3 polynomial might strike the right balance: flexible enough to capture the true curve, simple enough to ignore noise.</p>
<p><img  src="/eng-ai/_astro/03-diagram.ehMiBSXk_JH7w0.svg" alt="Occam&#38;#x27;s Razor diagram" width="500" height="350" loading="lazy" decoding="async"></p>
<p>The diagram shows three models: a simple linear model that underfits, a complex polynomial that overfits by passing through every point, and a moderate model that captures the true pattern without fitting noise. The best model achieves good compression‚Äîit captures the signal with reasonable complexity.</p>
<p>Occam‚Äôs Razor is not just a philosophical preference‚Äîit‚Äôs a statistical necessity. Given limited data, you must choose the simplest model consistent with the observations because simpler models make fewer assumptions and are thus more likely to transfer to unseen data.</p>
<h2 id="overfitting-as-memorization-ch3">Overfitting as Memorization</h2>
<p>Overfitting occurs when a model memorizes the training data instead of learning general patterns. The model achieves perfect training accuracy but poor test accuracy because it has encoded dataset-specific details that don‚Äôt transfer.</p>
<p>Think of a student memorizing answers to practice problems without understanding the underlying concepts. They‚Äôll ace the practice test but fail on new questions that require applying the concepts in unfamiliar ways. The student has compressed nothing‚Äîthey‚Äôve stored each example verbatim.</p>
<p>This is what happens when models are too complex relative to the amount of training data. A neural network with 1 million parameters trained on 100 examples will overfit catastrophically. It has enough capacity to memorize all 100 examples exactly, including every bit of noise, without learning anything generalizable.</p>
<p>Consider a decision tree trained without depth limits on a small dataset. The tree will grow until each leaf contains a single training example‚Äîperfect training accuracy, zero compression. Each leaf encodes a rule like ‚Äúif feature1=0.5 and feature2=3.2 and feature3=1.1, then class=A.‚Äù These rules are utterly specific to the training data and won‚Äôt generalize. A shallower tree forced to group similar examples learns broader rules that transfer better.</p>
<p>Memorization happens when:</p>
<ol>
<li><strong>Model capacity exceeds data size</strong>: Too many parameters, too few examples.</li>
<li><strong>Training runs too long</strong>: Even a well-sized model can overfit if trained until it perfectly fits every training example.</li>
<li><strong>Noise is present</strong>: If the data contains randomness or mislabeled examples, the model can memorize these errors.</li>
</ol>
<p>The signal that overfitting is occurring is divergence between training and validation performance. Training error keeps decreasing (the model is fitting the training data better), but validation error stops decreasing or starts increasing (the model is not generalizing). This divergence indicates memorization: the model is learning patterns specific to the training set.</p>
<p>Interestingly, very large modern neural networks sometimes escape this pattern. The <strong>double descent phenomenon</strong> (Nakkiran et al., 2019) shows that test error can decrease again after the overfitting regime if you make the model large enough. The classic U-shaped bias-variance curve (small models underfit, large models overfit) becomes double-descent: small models underfit, medium models overfit, but very large models can generalize well again. This happens because overparameterized models have many solutions that fit the training data, and optimization implicitly finds solutions that generalize‚Äîa form of implicit regularization.</p>
<p>This doesn‚Äôt invalidate Occam‚Äôs Razor‚Äîit reveals that model ‚Äúcomplexity‚Äù isn‚Äôt just parameter count. Very large models trained with SGD, dropout, and batch normalization have implicit constraints that enforce simplicity despite their size. The effective capacity (how complex functions the training procedure actually learns) is smaller than the nominal capacity (how complex functions the architecture could represent).</p>
<p>Preventing overfitting requires limiting model complexity relative to data:</p>
<ul>
<li><strong>Regularization</strong>: Penalize complexity in the loss function (L1, L2 penalties, weight decay).</li>
<li><strong>Early stopping</strong>: Stop training when validation error stops improving, even if training error is still decreasing.</li>
<li><strong>Data augmentation</strong>: Create more training examples by applying transformations (rotation, cropping for images; paraphrasing for text).</li>
<li><strong>Dropout and noise injection</strong>: Force the model to learn robust representations that don‚Äôt rely on specific neurons or features.</li>
<li><strong>Architecture choices</strong>: Use simpler models when data is limited.</li>
</ul>
<p>The fundamental tradeoff is between fitting the training data and compressing it. Perfect fit means memorization. Imperfect but parsimonious fit means compression‚Äîand compression generalizes.</p>
<h2 id="engineering-takeaway-ch3">Engineering Takeaway</h2>
<p>Understanding learning as compression changes how you approach model design and debugging.</p>
<p><strong>Match model capacity to data size.</strong> Don‚Äôt use a 100-million parameter neural network on 10,000 training examples. The model will overfit. Use simpler models (linear, shallow trees) when data is limited. Scale model capacity as data scales. The rule of thumb: you need roughly 10x as many training examples as parameters to avoid overfitting without strong regularization.</p>
<p><strong>Apply regularization systematically.</strong> Almost all production models use regularization‚ÄîL2 penalties, dropout, weight decay, early stopping. These techniques prevent memorization by penalizing complexity. Tune regularization strength (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œª</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">Œª</span></span></span></span>) on a validation set: stronger regularization means simpler models that might underfit; weaker regularization means complex models that might overfit. Use cross-validation to find the right balance.</p>
<p><strong>Monitor training vs validation loss continuously.</strong> If training loss is much lower than validation loss, you‚Äôre overfitting. The model is memorizing rather than compressing. Increase regularization, reduce model capacity, or collect more data. If both losses are high, you‚Äôre underfitting‚Äîthe model is too simple. Increase capacity or use better features. The gap between training and validation loss is your diagnostic for compression quality.</p>
<p><strong>Leverage compression for transfer learning.</strong> Models that compress well‚Äîthat learn general patterns rather than memorizing specifics‚Äîtransfer better to new domains. This is why pretraining works: a language model trained on billions of words compresses the patterns of language, and these patterns transfer to specific tasks like sentiment analysis or translation. Good compression is good representation. Pretrained models are compressed knowledge that you can fine-tune with little task-specific data.</p>
<p><strong>Compress models for deployment.</strong> After training, many parameters contribute little to predictions. Model compression techniques reduce size and speed up inference without hurting accuracy:</p>
<ul>
<li><strong>Pruning</strong>: Remove weights with small magnitude (often 50-90% of weights without accuracy loss).</li>
<li><strong>Quantization</strong>: Reduce precision from 32-bit floats to 8-bit integers (4x smaller, faster).</li>
<li><strong>Knowledge distillation</strong>: Train a small model to mimic a large model‚Äôs outputs (compress the compressed representation).
These techniques make models practical for mobile devices and real-time systems.</li>
</ul>
<p><strong>Use compression as a debugging tool.</strong> If your model has 1 million parameters but achieves the same performance as a 10,000 parameter model, it‚Äôs not compressing well‚Äîit‚Äôs learning redundant or irrelevant patterns. Simplify the architecture. If your model compresses well (good validation performance with few parameters), it has discovered meaningful structure. Inspect what it learned‚Äîvisualize weights, analyze feature importance‚Äîto understand the patterns.</p>
<p><strong>Expect implicit regularization in modern deep learning.</strong> Large neural networks trained with SGD, batch normalization, and dropout often generalize better than classical theory predicts. They have implicit biases toward simple solutions despite their nominal complexity. This is still an active research area, but practically: don‚Äôt be afraid to use large models if you have compute and data‚Äîimplicit regularization provides compression.</p>
<p>The lesson: Learning is compression. Models that compress data well‚Äîcapturing patterns with few parameters‚Äîgeneralize well. Models that memorize data‚Äîrequiring many parameters to fit specific examples‚Äîdo not. Design systems that favor compression over memorization, and you‚Äôll build models that generalize.</p>
<hr>
<h2 id="references-and-further-reading-ch3">References and Further Reading</h2>
<p><strong>Kolmogorov Complexity and Algorithmic Information Theory</strong> ‚Äì Ming Li and Paul Vit√°nyi
<a href="https://homepages.cwi.nl/~paulv/papers/info.pdf">https://homepages.cwi.nl/~paulv/papers/info.pdf</a></p>
<p>Kolmogorov complexity formalizes the idea that learning is compression. It defines the complexity of a dataset as the length of the shortest program that generates it. Learning means finding that program. This paper connects information theory, compression, and machine learning in a rigorous framework. Reading this will give you a theoretical foundation for why simpler models generalize better.</p>
<p><strong>Occam‚Äôs Razor</strong> ‚Äì Kevin Murphy, Section 1.2 in Machine Learning: A Probabilistic Perspective
<a href="https://probml.github.io/pml-book/">https://probml.github.io/pml-book/</a></p>
<p>Murphy‚Äôs textbook provides an accessible introduction to Occam‚Äôs Razor in the context of Bayesian machine learning. He explains how the principle of parsimony emerges naturally from probability theory: simpler models are preferred unless the data strongly justifies complexity. This connects philosophical intuition (simplicity) to mathematical formalism (Bayesian model selection).</p>
<p><strong>Deep Double Descent: Where Bigger Models and More Data Hurt</strong> ‚Äì Preetum Nakkiran et al. (2019)
<a href="https://arxiv.org/abs/1912.02292">https://arxiv.org/abs/1912.02292</a></p>
<p>This paper documents the double descent phenomenon‚Äîtest error decreases, then increases (classic overfitting), then decreases again as model size grows. This challenges conventional wisdom about the bias-variance tradeoff and reveals that very large models can generalize well despite having capacity to memorize. Understanding this phenomenon is essential for modern deep learning, where overparameterized models are standard. It shows that the relationship between model complexity and generalization is more nuanced than classical theory suggests.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part1/02-data-is-the-new-physics" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Data Is the New Physics</span> </a> <a href="/eng-ai/part1/04-bias-variance-tradeoff" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>The Bias-Variance Tradeoff</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#what-a-trained-model-really-is-ch3" data-astro-cid-xvrfupwn>What a Trained Model Really Is</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#pattern-discovery-as-compression-ch3" data-astro-cid-xvrfupwn>Pattern Discovery as Compression</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#occams-razor-ch3" data-astro-cid-xvrfupwn>Occam‚Äôs Razor</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#overfitting-as-memorization-ch3" data-astro-cid-xvrfupwn>Overfitting as Memorization</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch3" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch3" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>