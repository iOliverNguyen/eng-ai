<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 5: Features: How Machines See the World | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part1/05-features-how-machines-see-the-world/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part1/05-features-how-machines-see-the-world/"><meta property="og:title" content="Chapter 5: Features: How Machines See the World | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part1/05-features-how-machines-see-the-world/"><meta name="twitter:title" content="Chapter 5: Features: How Machines See the World | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part1" data-astro-cid-ilhxcym7>Part I: Foundations</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Features: How Machines See the World</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-5-features-how-machines-see-the-world">Chapter 5: Features: How Machines See the World</h1>
<h2 id="why-raw-data-is-unusable-ch5">Why Raw Data Is Unusable</h2>
<p>Machine learning models do not operate on raw reality. They operate on numbers‚Äîvectors of features that represent reality in a form the model can process. The quality of these features determines the ceiling on what the model can learn. No amount of algorithmic sophistication can compensate for poor features.</p>
<p>Consider image classification. An image is a grid of pixels, each with RGB color values. For a 256√ó256 image, that‚Äôs 196,608 numbers. But these numbers, as presented, encode very little about what the image contains. Pixel (142, 87) being red tells you almost nothing about whether the image contains a dog or a cat. The information is there, but it‚Äôs not accessible to simple models.</p>
<p>A linear model cannot learn directly from pixels. It would need to learn that certain patterns of pixel values (shapes, textures, edges) correspond to object categories. But pixels don‚Äôt encode shapes‚Äîthey encode colors at specific coordinates. The relationship between ‚Äúthis pixel is red‚Äù and ‚Äúthis image contains a dog‚Äù is extraordinarily complex, involving global spatial structure, lighting, perspective, and occlusion.</p>
<p>The same problem occurs with text. A sentence is a sequence of characters or words. But character codes (‚Äúa‚Äù = 97, ‚Äúb‚Äù = 98) tell you nothing about meaning. The model needs features that capture semantics: ‚Äúdog‚Äù and ‚Äúpuppy‚Äù are similar; ‚Äúgood‚Äù and ‚Äúbad‚Äù are opposites. Raw token IDs don‚Äôt encode these relationships.</p>
<p>Audio faces similar challenges. A waveform is a sequence of amplitude values over time. But to recognize speech, the model needs features representing phonemes, intonation, speaker characteristics‚Äînot raw pressure measurements. Speech recognition systems typically convert waveforms to spectrograms (time-frequency representations) that make phonetic patterns visible. Raw audio is a 1D signal; spectrograms are 2D images where patterns (vowels, consonants, intonation) form recognizable shapes.</p>
<p>Time series data presents a related problem. Raw sensor readings‚Äîtemperature, pressure, acceleration‚Äîcapture instantaneous values. But patterns often emerge over windows of time: daily cycles, weekly trends, anomalies relative to baselines. Models need features that aggregate and compare across time: moving averages, variance over windows, autocorrelations, trend directions.</p>
<p>High-dimensional raw data also suffers from the <strong>curse of dimensionality</strong>. As dimensions increase, data becomes sparse‚Äîmost of the space is empty, and examples are far apart. A linear model in 196,608 dimensions (pixel space) has so many degrees of freedom that it easily overfits. Feature engineering reduces dimensionality by extracting the relevant structure, making data denser and more learnable.</p>
<p>This is why feature engineering dominated classical machine learning. Practitioners spent most of their effort transforming raw data into representations that made patterns obvious. This transformation‚Äîfrom raw data to features‚Äîis where learning actually begins.</p>
<h2 id="what-a-feature-is-ch5">What a Feature Is</h2>
<p>A feature is a measurable property of the data that is relevant to the prediction task. Features translate raw data into a representation where patterns are accessible to models. Good features make learning easy. Bad features make learning impossible, no matter how sophisticated the model.</p>
<p>For structured data‚Äîtables with columns‚Äîfeatures are often given: age, income, purchase history. But even here, feature engineering improves performance. You might create:</p>
<ul>
<li><strong>Derived features</strong>: age¬≤ (to capture nonlinear effects), income/debt ratio</li>
<li><strong>Interaction features</strong>: age √ó income (different effects at different life stages)</li>
<li><strong>Temporal features</strong>: days since last purchase, purchase frequency</li>
<li><strong>Aggregations</strong>: average purchase value over the last 30 days</li>
</ul>
<p>For unstructured data‚Äîimages, text, audio‚Äîfeature engineering is more critical. Classical approaches manually designed features that captured domain knowledge.</p>
<p><strong>Image features:</strong></p>
<ul>
<li><strong>Edges</strong>: Detect boundaries between regions using filters (Sobel, Canny).</li>
<li><strong>Textures</strong>: Measure local patterns using Gabor filters or local binary patterns.</li>
<li><strong>Color histograms</strong>: Distribution of colors in the image.</li>
<li><strong>HOG (Histogram of Oriented Gradients)</strong>: Count edge orientations in local regions, capturing object shape.</li>
<li><strong>SIFT/SURF</strong>: Scale-invariant keypoint descriptors that identify distinctive local features.</li>
</ul>
<p>These features transform pixels into higher-level representations: ‚Äúthis region has strong vertical edges,‚Äù ‚Äúthis texture is smooth,‚Äù ‚Äúthis keypoint is distinctive.‚Äù A linear model can then learn that certain combinations of edges and textures indicate a dog.</p>
<p><strong>Text features:</strong></p>
<ul>
<li><strong>Bag of words</strong>: Count how often each word appears, ignoring order.</li>
<li><strong>TF-IDF</strong>: Weight words by how distinctive they are (frequent in this document, rare overall).</li>
<li><strong>N-grams</strong>: Capture short sequences (‚ÄúNew York,‚Äù ‚Äúnot good‚Äù).</li>
<li><strong>Word embeddings</strong>: Dense vectors where similar words have similar vectors.</li>
</ul>
<p>These features transform text from character sequences into numerical representations that preserve semantic relationships.</p>
<p>The key insight: features define the <strong>hypothesis space</strong>‚Äîthe set of functions the model can learn. If the features don‚Äôt encode the relevant information, the model cannot learn the pattern, no matter how complex it is. Features transform the input space (pixels, characters) into a <strong>feature space</strong> where patterns are more accessible. Ideally, feature space makes data linearly separable‚Äîdifferent classes cluster in different regions, so a linear model can separate them.</p>
<p>The <strong>kernel trick</strong> (used in SVMs) takes this idea further: it implicitly maps data to a very high-dimensional feature space where linear separation becomes possible without explicitly computing the features. This shows that representation‚Äîthe space you learn in‚Äîmatters more than the model you use.</p>
<p>Conversely, with the right features, even simple models work well. This is why feature engineering was the highest-leverage activity in classical machine learning.</p>
<h2 id="manual-vs-learned-features-ch5">Manual vs Learned Features</h2>
<p>Classical machine learning separated feature engineering from model training. A human expert designed features based on domain knowledge, and the model learned to combine those features. This two-stage process worked well but was labor-intensive and required deep expertise.</p>
<p>Deep learning changed this by learning features automatically. Neural networks do end-to-end learning: raw data goes in, predictions come out, and intermediate layers learn useful features without human intervention. This is representation learning.</p>
<p><strong>Classical ML pipeline:</strong></p>
<ol>
<li>Human expert designs features based on domain knowledge.</li>
<li>Model learns to combine features (linear weights, tree splits).</li>
<li>Performance depends critically on feature quality.</li>
</ol>
<p><strong>Deep learning pipeline:</strong></p>
<ol>
<li>Raw data is fed to the network.</li>
<li>Early layers learn low-level features (edges, textures).</li>
<li>Middle layers learn mid-level features (parts, patterns).</li>
<li>Late layers learn high-level features (concepts, categories).</li>
<li>Final layer makes predictions.</li>
</ol>
<p>The difference is profound. In classical ML, the model learns a function of fixed features. In deep learning, the model learns both the features and the function. This flexibility allows neural networks to discover representations humans never considered.</p>
<p>For images, convolutional neural networks (CNNs) learn hierarchical features:</p>
<ul>
<li><strong>Layer 1</strong>: Detects edges at different orientations.</li>
<li><strong>Layer 2</strong>: Combines edges into shapes and textures.</li>
<li><strong>Layer 3</strong>: Combines shapes into parts (eyes, ears, wheels).</li>
<li><strong>Layer 4</strong>: Combines parts into objects (faces, cars).</li>
</ul>
<p>No human programmed these features. The network learned them by minimizing classification loss on labeled images. The features emerged because they were useful for the task.</p>
<p>Why do learned features often beat manually engineered features? Several reasons:</p>
<ol>
<li><strong>Adaptation to data</strong>: Learned features optimize for the specific dataset and task, discovering patterns humans might miss. Manual features encode general intuitions that may not align perfectly with the data.</li>
<li><strong>Discovery of unexpected patterns</strong>: Networks find features humans wouldn‚Äôt think to design. ImageNet-trained CNNs learn to detect patterns (fur textures, specific shapes) that are predictive but not obvious.</li>
<li><strong>Joint optimization</strong>: Learned features and final classifier are optimized together (end-to-end), ensuring features are maximally useful for the task. Manual features are fixed before training, potentially discarding relevant information.</li>
<li><strong>Scalability</strong>: Once a neural network architecture works, it scales to more data without additional human effort. Manual feature engineering requires expert time for each new domain.</li>
</ol>
<p>However, learned features require substantial data. With limited data, manual features incorporating domain knowledge often outperform end-to-end learning because they inject prior knowledge that compensates for data scarcity.</p>
<p><img  src="/eng-ai/_astro/05-diagram.KFdkcyK4_Z1Rrbs4.svg" alt="Manual vs Learned Features diagram" width="500" height="320" loading="lazy" decoding="async"></p>
<p>The diagram shows how neural networks learn hierarchical features. Early layers learn simple features (edges), middle layers learn combinations (shapes), and later layers learn abstract concepts. The representation space transforms from mixed and unstructured (raw pixels) to separated and structured (learned features).</p>
<p>This automatic feature learning is why deep learning succeeded where classical ML struggled on perceptual tasks. Human experts couldn‚Äôt design features good enough to capture the complexity of natural images, speech, or language. Neural networks could.</p>
<h2 id="hierarchies-of-representation-ch5">Hierarchies of Representation</h2>
<p>The key to deep learning‚Äôs success is hierarchy. Features are learned in layers, where each layer builds on the previous one. Early layers learn simple, general features. Later layers learn complex, task-specific features. This mirrors how human perception works.</p>
<p>When you see a face, your visual system processes it hierarchically:</p>
<ol>
<li>Photoreceptors detect light and dark.</li>
<li>V1 neurons detect edges and orientations.</li>
<li>V2 neurons detect shapes and contours.</li>
<li>V4 neurons detect object parts.</li>
<li>IT cortex neurons recognize whole objects and faces.</li>
</ol>
<p>Neural networks learn similar hierarchies. A CNN trained on ImageNet develops detectors for:</p>
<ul>
<li><strong>Layer 1</strong>: Horizontal edges, vertical edges, diagonal edges, color blobs. These are general features that appear in almost any image.</li>
<li><strong>Layer 2</strong>: Corners formed by edge combinations, circles, simple textures (stripes, dots). These compose edges into basic shapes.</li>
<li><strong>Layer 3</strong>: Complex patterns, recurring textures (fur, fabric, water), repeated elements (windows on buildings). These compose shapes into distinctive patterns.</li>
<li><strong>Layer 4</strong>: Object parts‚Äîwheels, eyes, faces, text, windows. These compose patterns into recognizable components.</li>
<li><strong>Layer 5</strong>: Whole objects‚Äîcars (wheels + windows + body), dogs (face + fur + body), buildings (walls + windows). These compose parts into complete concepts.</li>
</ul>
<p>These features are composable. A ‚Äúface‚Äù feature is built from ‚Äúeye,‚Äù ‚Äúnose,‚Äù and ‚Äúmouth‚Äù features, which are built from edge and shape features. This <strong>compositionality</strong> makes learning efficient: you learn low-level features once and reuse them to build many high-level concepts. Instead of learning 1,000 object detectors from scratch, you learn a small set of shared low-level features and compose them differently for each object.</p>
<p>The same principle applies to text. A language model learns:</p>
<ul>
<li><strong>Character/token level</strong>: Spelling patterns, common prefixes/suffixes, character transitions.</li>
<li><strong>Word level</strong>: Syntax, part-of-speech, word co-occurrence, morphology.</li>
<li><strong>Phrase level</strong>: Common expressions (‚Äúin spite of‚Äù), grammatical structures, idioms.</li>
<li><strong>Sentence level</strong>: Semantic relationships, context, syntactic dependencies.</li>
<li><strong>Document level</strong>: Topics, themes, discourse structure, argument flow.</li>
</ul>
<p>Higher layers build meaning by composing simpler patterns. The word ‚Äúbank‚Äù has multiple meanings (financial institution, river edge), but sentence-level features resolve ambiguity from context.</p>
<p><strong>Why depth matters</strong>: Deeper models learn more abstract representations. A 3-layer network might learn edges ‚Üí shapes ‚Üí simple objects. A 50-layer network learns edges ‚Üí textures ‚Üí patterns ‚Üí parts ‚Üí assemblies ‚Üí objects ‚Üí scenes ‚Üí abstract concepts. Depth allows the network to build hierarchies of concepts, where each layer refines and abstracts the previous layer‚Äôs representations.</p>
<p>Empirically, deep networks outperform shallow wide networks with the same number of parameters. This suggests hierarchy‚Äîcomposing features through layers‚Äîis more powerful than learning a flat function.</p>
<p>This is why transfer learning works: features learned on one task (ImageNet classification) transfer to other tasks (medical imaging, satellite analysis) because the low-level features (edges, textures) are general. You can use a pretrained network‚Äôs early layers as-is and only retrain later layers for your specific task.</p>
<h2 id="engineering-takeaway-ch5">Engineering Takeaway</h2>
<p>Features determine what a model can learn. Investing in better features pays off more than investing in better algorithms.</p>
<p><strong>Preprocess and normalize systematically.</strong> Raw data is rarely in the right form for learning. Normalization (scaling features to similar ranges, e.g., z-score or min-max) prevents some features from dominating due to scale differences. Encoding categorical variables (one-hot for low cardinality, embeddings for high cardinality) makes them usable. Handling missing values (imputation with mean/median/model predictions, or masking) prevents failures. Time spent on preprocessing is time well spent‚Äîpoor preprocessing is a common cause of training instability and poor performance.</p>
<p><strong>Feature engineering still matters, especially for tabular data.</strong> Even with deep learning, domain knowledge helps. For tabular data (customer records, sensor logs, financial transactions), engineered features (ratios, aggregations, time-based features, interaction terms) often outperform raw columns, even with neural networks. Deep learning excels on perceptual data (images, text, audio) where structure is implicit. For structured data where features have explicit meaning, manual engineering remains valuable.</p>
<p><strong>Use data augmentation as feature-space expansion.</strong> For images, data augmentation (rotation, cropping, color jittering, flipping) creates variation that helps learning. This implicitly teaches the model invariances: a rotated cat is still a cat. For text, augmentation includes synonym replacement, back-translation, paraphrasing. For tabular data, consider noise injection or SMOTE (synthetic minority oversampling). Augmentation is a form of regularization that reduces overfitting by expanding the effective training set.</p>
<p><strong>Leverage embeddings as learned dense representations.</strong> Word embeddings (Word2Vec, GloVe) and contextualized embeddings (BERT, GPT) are features learned by neural networks on large text corpora. These features capture semantic relationships better than hand-designed features like TF-IDF or bag-of-words. The same principle extends to other domains: node embeddings for graphs, user/item embeddings for recommendation, image embeddings for retrieval. Embeddings compress sparse, high-dimensional data (vocabulary, user IDs) into dense, low-dimensional vectors where similar entities are close.</p>
<p><strong>Use transfer learning to leverage pretrained features.</strong> Pretrained models (ResNet for images, BERT for text, Wav2Vec for audio) have already learned good features on large datasets (ImageNet, web text, speech corpora). You can use these features for your specific task by fine-tuning (continuing training on your data) or using the model as a feature extractor (freezing early layers, training only final layers). This is faster and more effective than training from scratch, especially with limited data. Transfer learning works because learned features are general‚Äîlow-level features transfer broadly, high-level features transfer to similar tasks.</p>
<p><strong>Monitor feature distributions in production for drift detection.</strong> In production, feature distributions can shift (concept drift). If input features change‚Äîusers behave differently, sensors degrade, markets shift‚Äîmodel performance degrades even if the model itself hasn‚Äôt changed. Monitor feature statistics (mean, variance, quantiles, entropy) over time and compare to training distribution. Significant divergence signals that retraining is needed. Feature drift often precedes performance degradation, giving early warning.</p>
<p><strong>Design interpretability into feature engineering.</strong> To explain why a model made a prediction, you need to understand what features it‚Äôs using. For classical models, this means keeping features interpretable: ‚Äúage > 30‚Äù is interpretable, a complex polynomial of age is not. For neural networks, use interpretability tools (saliency maps show which pixels mattered, attention weights show which words mattered) to understand what patterns the network learned. Interpretability is easier with meaningful features than with raw data.</p>
<p>The lesson: Features are the interface between reality and models. Good features make patterns obvious; bad features hide patterns. Classical ML required manual feature engineering based on domain expertise. Deep learning automates it through representation learning, discovering features from data. But in both cases, the quality of features‚Äîwhether designed or learned‚Äîis what determines success. Invest in understanding and improving features, and model performance will follow.</p>
<hr>
<h2 id="references-and-further-reading-ch5">References and Further Reading</h2>
<p><strong>Feature Engineering for Machine Learning</strong> ‚Äì Alice Zheng and Amanda Casari
<a href="https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/">https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/</a></p>
<p>This book is a practical guide to feature engineering for structured data. It covers handling categorical variables, numerical transformations, text features, and time-series features with concrete examples and code. Reading this will teach you the techniques practitioners use to improve model performance through better features. Essential for anyone working with tabular data or building features for classical ML.</p>
<p><strong>Representation Learning: A Review and New Perspectives</strong> ‚Äì Yoshua Bengio, Aaron Courville, Pascal Vincent (2013)
<a href="https://arxiv.org/abs/1206.5538">https://arxiv.org/abs/1206.5538</a></p>
<p>This paper surveys representation learning‚Äîthe idea that models should learn features rather than rely on hand-engineering. Bengio explains why deep learning works: hierarchical feature learning, compositionality, and distributed representations. The paper connects classical feature engineering, autoencoders, RBMs, and modern deep learning, providing both theoretical foundations and empirical insights. Reading this gives you the conceptual framework for understanding why neural networks discover useful features automatically.</p>
<p><strong>Visualizing and Understanding Convolutional Networks</strong> ‚Äì Matthew Zeiler and Rob Fergus (2013)
<a href="https://arxiv.org/abs/1311.2901">https://arxiv.org/abs/1311.2901</a></p>
<p>This paper visualizes what features CNNs learn at each layer. Zeiler and Fergus use deconvolution to project activations back to pixel space, revealing that early layers detect edges and colors, middle layers detect textures and patterns, and late layers detect object parts and whole objects. Reading this (and studying the figures) will give you concrete intuition for hierarchical feature learning in neural networks. It makes abstract concepts (representation learning, hierarchical features) visually concrete.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part1/04-bias-variance-tradeoff" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>The Bias-Variance Tradeoff</span> </a> <a href="/eng-ai/part2" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Part II: Classical Machine Learning</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-raw-data-is-unusable-ch5" data-astro-cid-xvrfupwn>Why Raw Data Is Unusable</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#what-a-feature-is-ch5" data-astro-cid-xvrfupwn>What a Feature Is</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#manual-vs-learned-features-ch5" data-astro-cid-xvrfupwn>Manual vs Learned Features</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#hierarchies-of-representation-ch5" data-astro-cid-xvrfupwn>Hierarchies of Representation</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch5" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch5" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>