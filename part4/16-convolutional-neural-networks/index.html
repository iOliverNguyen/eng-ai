<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 16: Convolutional Neural Networks | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part4/16-convolutional-neural-networks/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part4/16-convolutional-neural-networks/"><meta property="og:title" content="Chapter 16: Convolutional Neural Networks | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part4/16-convolutional-neural-networks/"><meta name="twitter:title" content="Chapter 16: Convolutional Neural Networks | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part4" data-astro-cid-ilhxcym7>Part IV: Deep Architectures</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Convolutional Neural Networks</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-16-convolutional-neural-networks">Chapter 16: Convolutional Neural Networks</h1>
<h2 id="how-machines-see-ch16">How Machines See</h2>
<h2 id="why-pixels-are-not-independent-ch16">Why Pixels Are Not Independent</h2>
<p>A fully connected neural network treats each input independently. For an image with 224√ó224 pixels and 3 color channels (RGB), that‚Äôs 150,528 input values. Every neuron in the first hidden layer connects to all 150,528 pixels. With just 1,000 neurons in the first layer, that‚Äôs over 150 million parameters‚Äîbefore we‚Äôve even started building a deep network.</p>
<p>This architecture ignores the fundamental structure of images: <strong>spatial locality</strong>. Images have structure. Nearby pixels are strongly correlated‚Äîthey‚Äôre part of the same edge, texture, or object. A pixel‚Äôs meaning depends on its neighbors. An isolated red pixel means nothing; a cluster of red pixels forming a shape conveys information. A pixel at position (100, 100) has more in common with pixels at (99, 100) and (100, 101) than with a pixel at (200, 200).</p>
<p>Fully connected layers don‚Äôt exploit this structure. They have to learn spatial relationships from scratch across millions of parameters. They treat a pixel in the top-left corner as equally related to all other pixels, forcing the network to discover that nearby pixels matter more‚Äîa waste of parameters and data.</p>
<p><strong>Convolutional Neural Networks (CNNs)</strong> solve this by building spatial structure into the architecture. They use convolution operations that process local regions, explicitly encoding the prior that nearby pixels are related. This inductive bias dramatically reduces parameters, improves generalization, and makes learning visual features tractable.</p>
<h2 id="convolutions-ch16">Convolutions</h2>
<p>A convolutional layer applies a small learned filter (typically 3√ó3, 5√ó5, or 7√ó7) across the entire image. The filter slides from left to right, top to bottom, computing a dot product at each position. The result is a feature map‚Äîa grid showing where the pattern was detected.</p>
<p>For a 3√ó3 filter with weights <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> applied to an image region <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span>, the output at that location is:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = \sum_{i=0}^{2} \sum_{j=0}^{2} W_{ij} X_{ij} + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.2149em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8011em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">‚àë</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8011em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">‚àë</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span> is a learned bias term. This operation slides across the image with a stride (typically 1 or 2 pixels), producing a 2D output called a feature map or activation map.</p>
<p><strong>Example: Vertical Edge Detection</strong></p>
<p>Consider a simple vertical edge detector:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>W</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">W = \begin{bmatrix}
-1 &#x26; 0 &#x26; 1 \\
-1 &#x26; 0 &#x26; 1 \\
-1 &#x26; 0 &#x26; 1
\end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.6em;vertical-align:-1.55em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.667em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.667em" height="3.600em" viewBox="0 0 667 3600"><path d="M403 1759 V84 H666 V0 H319 V1759 v0 v1759 h347 v-84
H403z M403 1759 V0 H319 V1759 v0 v1759 h84z"></path></svg></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">‚àí</span><span class="mord">1</span></span></span><span style="top:-3.01em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">‚àí</span><span class="mord">1</span></span></span><span style="top:-1.81em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">‚àí</span><span class="mord">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.01em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-1.81em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-3.01em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-1.81em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.667em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.667em" height="3.600em" viewBox="0 0 667 3600"><path d="M347 1759 V0 H0 V84 H263 V1759 v0 v1759 H0 v84 H347z
M347 1759 V0 H263 V1759 v0 v1759 h84z"></path></svg></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span></span></span></span></span>
<p>This filter responds strongly to vertical edges‚Äîplaces where the intensity changes from left (negative weights) to right (positive weights). When centered on a vertical edge, the negative weights multiply dark pixels on the left and positive weights multiply bright pixels on the right, producing a large positive value. On uniform regions or horizontal edges, the response is small.</p>
<p>A CNN learns dozens or hundreds of filters automatically. Each filter specializes in detecting a different pattern: horizontal edges, diagonal edges, corners, color blobs, textures. The network discovers which patterns matter for the task by adjusting filter weights during training.</p>
<p><strong>Parameters and Weight Sharing</strong></p>
<p>The key innovation: the same filter is applied everywhere. A 3√ó3 filter has only 9 weights (plus 1 bias), regardless of image size. For a layer with 64 filters on a 224√ó224 image, that‚Äôs 64 √ó (9 + 1) = 640 parameters. Compare this to a fully connected layer: 150,528 √ó 1,000 = 150 million parameters for the same setup.</p>
<p>This weight sharing has two benefits:</p>
<ol>
<li><strong>Parameter efficiency</strong>: Dramatically fewer parameters means less data needed to train and lower risk of overfitting.</li>
<li><strong>Translation invariance</strong>: Because the same filter is applied everywhere, the network automatically handles objects appearing at different positions. A cat in the top-left and a cat in the bottom-right both activate the same filters.</li>
</ol>
<p><img  src="/eng-ai/_astro/16-diagram-1.2Gu3fTwA_1dthd4.svg" alt="Why Pixels Are Not Independent diagram" width="500" height="350" loading="lazy" decoding="async"></p>
<p>The diagram shows how convolution works: a small filter slides across the input image, computing dot products at each position to produce a feature map. The same filter (orange) is applied at all positions.</p>
<h2 id="hierarchies-of-vision-from-edges-to-objects-ch16">Hierarchies of Vision: From Edges to Objects</h2>
<p>CNNs are typically deep, with many convolutional layers stacked sequentially. Each layer learns features of increasing abstraction by combining features from the previous layer. This mirrors how the human visual system processes information hierarchically.</p>
<p><strong>Layer 1: Low-Level Features</strong></p>
<p>The first convolutional layer learns simple patterns that appear universally in natural images:</p>
<ul>
<li>Edges at different orientations (horizontal, vertical, diagonal)</li>
<li>Color blobs and gradients</li>
<li>Simple textures (dots, lines)</li>
</ul>
<p>These features are general‚Äîthey appear in almost all images, regardless of content. A horizontal edge detector is useful whether you‚Äôre looking at cats, cars, or buildings.</p>
<p><strong>Layer 2: Mid-Level Features</strong></p>
<p>The second layer combines layer 1 features into more complex patterns:</p>
<ul>
<li>Corners (combinations of perpendicular edges)</li>
<li>Curves and circles</li>
<li>Simple textures (grid patterns, waves)</li>
<li>Color combinations</li>
</ul>
<p>These features require specific spatial arrangements of edges. A corner needs two edges meeting at a point. The network learns these combinations automatically.</p>
<p><strong>Layer 3-4: High-Level Features</strong></p>
<p>Deeper layers combine mid-level features into object parts and patterns:</p>
<ul>
<li>Furry textures (combinations of fine-scale patterns)</li>
<li>Metallic surfaces (specific reflectance patterns)</li>
<li>Object parts: eyes, ears, wheels, windows</li>
<li>Repeated structures (bricks, tiles, text)</li>
</ul>
<p><strong>Layer 5+: Object and Scene Representations</strong></p>
<p>The deepest layers represent whole objects and scenes:</p>
<ul>
<li>Specific object categories: dogs, cats, cars, buildings</li>
<li>Scene types: indoors, outdoors, urban, natural</li>
<li>Abstract concepts: ‚Äúdangerous,‚Äù ‚Äúvaluable‚Äù (task-dependent)</li>
</ul>
<p>By this point, spatial information has been heavily compressed. The representation encodes ‚Äúwhat‚Äù (is there a dog?) rather than precise ‚Äúwhere‚Äù (which pixels contain the dog?).</p>
<p><img  src="/eng-ai/_astro/16-diagram-2.DDikfL5y_FcIPC.svg" alt="Hierarchies of Vision: From Edges to Objects diagram" width="500" height="320" loading="lazy" decoding="async"></p>
<p>The diagram shows CNN architecture with increasing abstraction. Early layers learn simple patterns with small receptive fields; later layers combine them into complex concepts with large receptive fields. Pooling progressively reduces spatial resolution.</p>
<p><strong>Pooling: Reducing Spatial Resolution</strong></p>
<p>Between convolutional layers, CNNs typically use pooling to downsample feature maps. The most common is <strong>max pooling</strong>: divide the feature map into non-overlapping 2√ó2 regions and take the maximum value in each region. This reduces dimensions by 2x in each spatial dimension.</p>
<p>Pooling serves several purposes:</p>
<ol>
<li><strong>Computational efficiency</strong>: Smaller feature maps mean fewer parameters and faster computation in subsequent layers.</li>
<li><strong>Translation invariance</strong>: Taking the max over a region makes the representation less sensitive to small shifts. If a feature appears anywhere in the 2√ó2 region, it‚Äôs detected.</li>
<li><strong>Abstraction</strong>: Pooling discards precise spatial information (‚Äúthe edge is at pixel (47, 93)‚Äù) but preserves existential information (‚Äúthere‚Äôs an edge in this general area‚Äù). This lossy compression focuses the network on ‚Äúwhat‚Äù rather than ‚Äúwhere.‚Äù</li>
</ol>
<h2 id="batch-normalization-enabling-deep-networks-ch16">Batch Normalization: Enabling Deep Networks</h2>
<p>Deep CNNs (beyond ~10 layers) historically struggled to train without careful initialization and learning rate tuning. The problem: <strong>internal covariate shift</strong>‚Äîas network parameters change during training, the distribution of layer inputs shifts, making optimization unstable.</p>
<p><strong>Batch Normalization</strong> (Ioffe &#x26; Szegedy, 2015) solved this by normalizing activations within each mini-batch. For each feature map channel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span>, batch norm computes:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>c</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>z</mi><mi>c</mi></msub><mo>‚àí</mo><msub><mi>Œº</mi><mi>c</mi></msub></mrow><msqrt><mrow><msubsup><mi>œÉ</mi><mi>c</mi><mn>2</mn></msubsup><mo>+</mo><mi>œµ</mi></mrow></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\hat{z}_c = \frac{z_c - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3903em;vertical-align:-1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em;"><span style="top:-2.1784em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9316em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">œÉ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7401em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">œµ</span></span></span><span style="top:-2.8916em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.28em" viewBox="0 0 400000 1296" preserveAspectRatio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.3084em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">Œº</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:1.13em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œº</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">\mu_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Œº</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>œÉ</mi><mi>c</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_c^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">œÉ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> are the mean and variance of activations across the batch, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">œµ</span></span></span></span> prevents division by zero. Then scale and shift with learned parameters:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>c</mi></msub><mo>=</mo><msub><mi>Œ≥</mi><mi>c</mi></msub><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>c</mi></msub><mo>+</mo><msub><mi>Œ≤</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">y_c = \gamma_c \hat{z}_c + \beta_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">Œ≥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0556em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">Œ≤</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>
<p><strong>Why it works:</strong></p>
<ol>
<li><strong>Faster convergence</strong>: Normalized activations stay in sensitive regions of activation functions (sigmoid, tanh don‚Äôt saturate)</li>
<li><strong>Higher learning rates</strong>: Stable activations allow training with larger learning rates without divergence</li>
<li><strong>Regularization</strong>: Batch statistics add noise (activations depend on which examples are in the batch), acting as a regularizer</li>
</ol>
<p><strong>Where to place:</strong> Standard practice: Conv ‚Üí BatchNorm ‚Üí Activation (ReLU). Some architectures place it after activation, but before is now more common.</p>
<p><strong>Inference mode:</strong> During training, use batch statistics (mean/variance computed from current batch). During inference, use running statistics (exponential moving average collected during training) since batches may be size 1 or small.</p>
<p><strong>Impact:</strong> Batch normalization enabled training networks 100+ layers deep (ResNet-152, DenseNet-264). Before batch norm, networks beyond ~20 layers were extremely difficult to train. After batch norm, depth became the standard way to improve accuracy.</p>
<p><strong>Production tip:</strong> Always use batch normalization in CNNs deeper than ~10 layers. It‚Äôs not optional‚Äîit‚Äôs essential for training stability and achieving good performance. Without it, you‚Äôll struggle with vanishing gradients, slow convergence, and poor generalization.</p>
<h2 id="modern-cnn-architectures-ch16">Modern CNN Architectures</h2>
<p>Early CNNs (AlexNet, VGG) were simple stacks: Conv ‚Üí ReLU ‚Üí Pool, repeated many times. Modern CNNs use sophisticated architectural patterns that dramatically improve both accuracy and efficiency.</p>
<p><strong>ResNet (2015): Skip Connections</strong></p>
<p>ResNet introduced residual connections: instead of learning <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>, learn the residual <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">F(x) = H(x) - x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>. The output is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">H(x) = F(x) + x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>‚Äîthe input is added directly to the layer output.</p>
<p>Why this matters: Skip connections create identity paths for gradients to flow backward without attenuation. This solves vanishing gradients in very deep networks (50, 101, 152 layers). ResNet-50 remains a production standard for vision tasks.</p>
<p><strong>DenseNet (2017): Dense Connections</strong></p>
<p>DenseNet connects every layer to every subsequent layer within a block. Instead of one skip connection, create many‚Äîeach layer receives inputs from all previous layers, maximizing feature reuse.</p>
<p>Benefits: Fewer parameters (less redundancy), better gradient flow, stronger feature propagation. Tradeoff: higher memory usage during training (must store all intermediate features).</p>
<p><strong>EfficientNet (2019): Compound Scaling</strong></p>
<p>EfficientNet uses neural architecture search to find the optimal balance of depth (number of layers), width (number of channels), and resolution (input image size). Instead of scaling one dimension, scale all three simultaneously with a compound coefficient.</p>
<p>Result: State-of-the-art accuracy with 10√ó fewer parameters and 10√ó less compute than previous models. EfficientNet-B0 through B7 provide a family of models trading accuracy for efficiency.</p>
<p><strong>MobileNet / EfficientNet: Depthwise Separable Convolutions</strong></p>
<p>Standard convolution mixes spatial and channel dimensions. Depthwise separable convolution splits this: first apply spatial convolution per channel, then mix channels with 1√ó1 convolutions.</p>
<p>Benefit: 9√ó fewer parameters for 3√ó3 convolutions. Critical for mobile deployment where model size and inference speed matter.</p>
<p><strong>When to use which:</strong></p>
<ul>
<li><strong>ResNet-50</strong>: General-purpose workhorse, good accuracy, moderate cost</li>
<li><strong>EfficientNet</strong>: Best accuracy-efficiency tradeoff, use for deployment</li>
<li><strong>MobileNet</strong>: Mobile and edge devices, prioritize speed over accuracy</li>
<li><strong>DenseNet</strong>: Research, maximum parameter efficiency</li>
</ul>
<p><strong>Connection to representation learning (Chapter 15):</strong> All these architectures learn hierarchical features‚Äîedges ‚Üí textures ‚Üí parts ‚Üí objects. The architectural innovations (skip connections, dense connections, efficient convolutions) improve how well these hierarchies are learned, but the principle remains the same.</p>
<p><strong>Production reality:</strong> Don‚Äôt design custom architectures. Use pretrained ResNet, EfficientNet, or Vision Transformers and fine-tune. Custom architectures rarely outperform well-tuned standard architectures unless you have domain-specific requirements.</p>
<h2 id="translation-invariance-why-cnns-recognize-objects-anywhere-ch16">Translation Invariance: Why CNNs Recognize Objects Anywhere</h2>
<p>A fundamental property of convolution with weight sharing is translation invariance: the network responds to patterns regardless of their position in the image. If you train a CNN to recognize cats, it works whether the cat is in the center, top-left, bottom-right, or partially cropped‚Äîanywhere in the image.</p>
<p>This happens automatically because:</p>
<ol>
<li><strong>Same filters everywhere</strong>: The cat-detecting filters in layer 3 are applied to all spatial positions.</li>
<li><strong>Hierarchical pooling</strong>: Pooling progressively abstracts away precise location, making the final representation encode ‚Äúcat present‚Äù rather than ‚Äúcat at position (x, y).‚Äù</li>
</ol>
<p>This is crucial for real-world vision systems. Objects appear at arbitrary positions. Cameras have different fields of view. Users crop photos differently. CNNs handle this naturally without requiring training examples at every possible position.</p>
<p>However, translation invariance is not perfect in practice:</p>
<ul>
<li><strong>Position biases</strong>: Real datasets have biases (objects are often centered), and networks can learn these biases.</li>
<li><strong>Boundary effects</strong>: Patterns near image edges have less context and may be detected differently.</li>
<li><strong>Data augmentation helps</strong>: Random crops during training improve translation invariance by exposing the network to objects at varied positions.</li>
</ul>
<h2 id="failure-modes-and-limitations-ch16">Failure Modes and Limitations</h2>
<p>Despite their success, CNNs have limitations:</p>
<p><strong>1. Not truly scale-invariant</strong></p>
<p>While translation-invariant, CNNs are not inherently scale-invariant. A CNN trained on cats at typical sizes might struggle with a cat that‚Äôs unusually small (far away) or large (extreme close-up). The filters have fixed sizes‚Äîa 3√ó3 filter detecting whiskers works at one scale but not all scales.</p>
<p>Solutions include multi-scale architectures (process the image at different resolutions) and data augmentation (random zooming during training).</p>
<p><strong>2. Limited to spatial/grid-structured data</strong></p>
<p>Convolution assumes data lies on a regular grid (images, videos, grids). For non-grid data‚Äîgraphs, point clouds, text with long-range dependencies‚Äîconvolution is less natural. Other architectures (Graph Neural Networks, Transformers) are better suited.</p>
<p><strong>3. Computationally expensive</strong></p>
<p>While more efficient than fully connected networks, CNNs still require significant compute for deep architectures on high-resolution images. Modern CNNs like EfficientNet balance accuracy and efficiency through neural architecture search, but deployment on mobile devices or embedded systems remains challenging.</p>
<p><strong>4. Adversarial vulnerability</strong></p>
<p>CNNs are famously vulnerable to adversarial examples‚Äîtiny, imperceptible perturbations that cause confident misclassifications. This brittleness suggests CNNs don‚Äôt ‚Äúunderstand‚Äù images the way humans do‚Äîthey exploit statistical patterns that can be fooled.</p>
<h2 id="engineering-takeaway-ch16">Engineering Takeaway</h2>
<p>CNNs revolutionized computer vision by encoding spatial inductive biases directly into the architecture. Understanding their design principles helps you use them effectively and know when to choose alternatives.</p>
<p><strong>CNNs embed priors about images</strong></p>
<p>Convolution assumes spatial locality and translation invariance. These assumptions are correct for natural images, making CNNs vastly more data-efficient than fully connected networks. But for non-spatial data (tabular data, time series without local structure), these priors don‚Äôt help‚Äîuse fully connected or other architectures.</p>
<p><strong>Pretrained models are standard</strong></p>
<p>Training CNNs from scratch requires massive labeled datasets (ImageNet: 1.2M images, 1000 classes) and substantial compute (days on GPUs). In practice, use pretrained models (ResNet, EfficientNet, Vision Transformers) and fine-tune the final layers for your specific task. Transfer learning works because early layers learn general visual features (edges, textures) that transfer across tasks.</p>
<p><strong>Architecture matters more than you think</strong></p>
<p>Early CNNs (AlexNet, VGG) were simple stacks of convolutions and pooling. Modern CNNs (ResNet, DenseNet, EfficientNet) use skip connections, bottleneck layers, and careful scaling to improve both accuracy and efficiency. Architecture choices‚Äîdepth, width, filter sizes, pooling strategies‚Äîsignificantly affect performance. Use well-validated architectures rather than designing from scratch.</p>
<p><strong>Data augmentation teaches invariances and prevents overfitting.</strong> CNNs overfit easily with limited data. Data augmentation artificially increases dataset size by creating modified versions of training images:</p>
<p>Common augmentations:</p>
<ul>
<li><strong>Random crops</strong>: Sample different regions, teaching position invariance</li>
<li><strong>Horizontal flips</strong>: Mirror images (but not vertical‚Äîsky is usually up)</li>
<li><strong>Rotations</strong>: ¬±15-30 degrees for natural images</li>
<li><strong>Color jitter</strong>: Vary brightness, contrast, saturation (lighting invariance)</li>
<li><strong>Cutout/Random erasing</strong>: Mask random patches (robustness to occlusion)</li>
<li><strong>Mixup</strong>: Blend two images and labels (smooths decision boundaries)</li>
</ul>
<p>Effectiveness: Typical 2-5√ó effective dataset size increase, translating to 5-10% accuracy improvement. Augmentation only during training, not inference.</p>
<p>Modern approaches: <strong>AutoAugment</strong> and <strong>RandAugment</strong> use learned policies‚Äîtraining finds optimal augmentation strategies automatically for each dataset. More effective than hand-crafted augmentation but requires more compute.</p>
<p>Connection to generalization (Chapter 4): Augmentation reduces overfitting by teaching the model that these transformations don‚Äôt change the label. A rotated cat is still a cat. This inductive bias improves generalization without requiring more real data.</p>
<p>Production tip: Augmentation is not optional‚Äîit‚Äôs mandatory for training CNNs on datasets &#x3C; 100k images. Without it, networks memorize training data and fail on test data. Always augment unless you have millions of examples.</p>
<p><strong>Batch normalization is non-negotiable for deep CNNs.</strong> CNNs deeper than ~10 layers require batch normalization to train effectively. Without it, internal covariate shift causes vanishing gradients, slow convergence, and training instability. Batch norm normalizes activations per channel within each mini-batch, stabilizing training and enabling higher learning rates. ResNet, EfficientNet, and all modern CNNs use batch norm after every conv layer. Impact: enabled training networks 100+ layers deep. Production tip: Place batch norm between convolution and activation (Conv ‚Üí BatchNorm ‚Üí ReLU). Always use it for deep networks‚Äîit‚Äôs the difference between training successfully and failing.</p>
<p><strong>CNNs are being challenged by Transformers for large-scale vision.</strong> Vision Transformers (ViTs) now match or exceed CNN performance on many tasks by treating images as sequences of patches and using attention instead of convolution. CNNs still dominate for small datasets (better inductive biases with less data), edge deployment (lower compute and memory), and tasks requiring precise spatial reasoning. But for large-scale vision with abundant data (billions of images), Transformers are increasingly preferred. Tradeoff: CNNs need less data but have lower ceiling, Transformers need more data but scale better. Modern trend: hybrid architectures combine convolution (early layers) and attention (late layers).</p>
<p><strong>Production deployment requires optimization</strong></p>
<p>Inference with CNNs can be expensive. Techniques for deployment:</p>
<ul>
<li><strong>Pruning</strong>: Remove redundant filters and weights</li>
<li><strong>Quantization</strong>: Use int8 instead of float32 for weights/activations</li>
<li><strong>Knowledge distillation</strong>: Train a smaller ‚Äústudent‚Äù network to mimic a large ‚Äúteacher‚Äù</li>
<li><strong>Architecture search</strong>: Find efficient architectures (MobileNet, EfficientNet) that balance accuracy and speed</li>
</ul>
<p>The lesson: CNNs are not just ‚Äúneural networks for images.‚Äù They‚Äôre architectures that encode specific assumptions about spatial structure. These assumptions make them extraordinarily effective for vision but also constrain where they‚Äôre applicable. Understanding the inductive biases‚Äîconvolution as local pattern detection, pooling as hierarchical abstraction, weight sharing as translation invariance‚Äîlets you reason about when CNNs are the right tool and when alternatives are better.</p>
<hr>
<h2 id="references-and-further-reading-ch16">References and Further Reading</h2>
<p><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong> ‚Äì Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (2012)
<a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a></p>
<p>AlexNet sparked the deep learning revolution by winning ImageNet 2012 with a CNN that crushed previous methods. This paper showed that deep CNNs, trained on GPUs with large datasets, could solve real-world vision tasks at unprecedented accuracy. Reading this gives historical context for why CNNs changed AI and how architecture design (ReLU, dropout, data augmentation, GPU training) enabled their success.</p>
<p><strong>Deep Residual Learning for Image Recognition</strong> ‚Äì Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015)
<a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></p>
<p>ResNet introduced skip connections (residual connections), enabling training of extremely deep networks (50, 101, 152 layers) without degradation. He et al. showed that depth improves accuracy when done correctly, and skip connections allow gradients to flow easily through very deep networks. ResNet remains one of the most influential CNN architectures. Understanding why skip connections work (gradient flow, identity mappings) helps you reason about modern architectures.</p>
<p><strong>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</strong> ‚Äì Alexey Dosovitskiy et al. (2020)
<a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></p>
<p>Vision Transformers (ViT) showed that pure attention-based architectures can match or exceed CNNs on vision tasks when trained on large datasets. This paper challenged the assumption that convolution is necessary for vision and demonstrated that Transformers‚Äô flexibility makes them universal across modalities. Reading this explains the current shift from CNNs to Transformers in large-scale vision systems.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part4" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Part IV: Deep Architectures</span> </a> <a href="/eng-ai/part4/17-recurrent-neural-networks" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Recurrent Neural Networks</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#how-machines-see-ch16" data-astro-cid-xvrfupwn>How Machines See</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-pixels-are-not-independent-ch16" data-astro-cid-xvrfupwn>Why Pixels Are Not Independent</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#convolutions-ch16" data-astro-cid-xvrfupwn>Convolutions</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#hierarchies-of-vision-from-edges-to-objects-ch16" data-astro-cid-xvrfupwn>Hierarchies of Vision: From Edges to Objects</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#batch-normalization-enabling-deep-networks-ch16" data-astro-cid-xvrfupwn>Batch Normalization: Enabling Deep Networks</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#modern-cnn-architectures-ch16" data-astro-cid-xvrfupwn>Modern CNN Architectures</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#translation-invariance-why-cnns-recognize-objects-anywhere-ch16" data-astro-cid-xvrfupwn>Translation Invariance: Why CNNs Recognize Objects Anywhere</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#failure-modes-and-limitations-ch16" data-astro-cid-xvrfupwn>Failure Modes and Limitations</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch16" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch16" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>