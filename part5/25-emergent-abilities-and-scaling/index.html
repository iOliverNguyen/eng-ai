<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 25: Emergent Abilities and Scaling | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part5/25-emergent-abilities-and-scaling/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part5/25-emergent-abilities-and-scaling/"><meta property="og:title" content="Chapter 25: Emergent Abilities and Scaling | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part5/25-emergent-abilities-and-scaling/"><meta name="twitter:title" content="Chapter 25: Emergent Abilities and Scaling | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part5" data-astro-cid-ilhxcym7>Part V: Language Models</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Emergent Abilities and Scaling</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-25-emergent-abilities-and-scaling">Chapter 25: Emergent Abilities and Scaling</h1>
<h2 id="why-bigger-models-do-new-things-ch25">Why Bigger Models Do New Things</h2>
<h2 id="scaling-laws-the-physics-of-language-models-ch25">Scaling Laws: The Physics of Language Models</h2>
<p>Language model performance improves predictably with scale. Increase model size, training data, or compute, and loss decreases following a power law. This relationship‚Äî<strong>scaling laws</strong>‚Äîmakes larger models not just quantitatively better but qualitatively different.</p>
<p>The empirical relationship (Kaplan et al., 2020):</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo><mo>‚àù</mo><msup><mi>N</mi><mrow><mo>‚àí</mo><msub><mi>Œ±</mi><mi>N</mi></msub></mrow></msup><mo separator="true">,</mo><mspace width="1em"></mspace><mi>L</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>‚àù</mo><msup><mi>D</mi><mrow><mo>‚àí</mo><msub><mi>Œ±</mi><mi>D</mi></msub></mrow></msup><mo separator="true">,</mo><mspace width="1em"></mspace><mi>L</mi><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo><mo>‚àù</mo><msup><mi>C</mi><mrow><mo>‚àí</mo><msub><mi>Œ±</mi><mi>C</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">L(N) \propto N^{-\alpha_N}, \quad L(D) \propto D^{-\alpha_D}, \quad L(C) \propto C^{-\alpha_C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àù</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0713em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8213em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚àí</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">Œ±</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567em;margin-left:-0.0037em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àù</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0713em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8213em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚àí</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">Œ±</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567em;margin-left:-0.0037em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àù</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8213em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8213em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚àí</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">Œ±</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567em;margin-left:-0.0037em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>
<p>Where:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span> is the cross-entropy loss on held-out data</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> is the number of model parameters</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span> is the dataset size (number of tokens)</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> is the compute used during training (measured in FLOPs)</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>N</mi></msub><mo>‚âà</mo><mn>0.076</mn></mrow><annotation encoding="application/x-tex">\alpha_N \approx 0.076</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6331em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.076</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>D</mi></msub><mo>‚âà</mo><mn>0.095</mn></mrow><annotation encoding="application/x-tex">\alpha_D \approx 0.095</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6331em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.095</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>C</mi></msub><mo>‚âà</mo><mn>0.050</mn></mrow><annotation encoding="application/x-tex">\alpha_C \approx 0.050</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6331em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.050</span></span></span></span> are empirically determined exponents</li>
</ul>
<p>These power laws hold across orders of magnitude. A 10√ó increase in parameters reduces loss by a predictable amount. A 100√ó increase produces proportionally greater improvement. The relationships are smooth and consistent, enabling forecasting: experiments with small models predict large model performance.</p>
<p>In log-log space, scaling laws appear as straight lines:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>log</mi><mo>‚Å°</mo><mi>L</mi><mo>=</mo><mo>‚àí</mo><mi>Œ±</mi><mi>log</mi><mo>‚Å°</mo><mi>N</mi><mo>+</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">\log L = -\alpha \log N + c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">‚àí</span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span></span>
<p>This linearity is remarkable. Most systems exhibit diminishing returns or saturation‚Äîperformance improves rapidly at first, then plateaus. Language models don‚Äôt plateau within observed ranges. Doubling compute continues to reduce loss, suggesting no near-term ceiling on performance gains.</p>
<p><img  src="/eng-ai/_astro/25-diagram-1.DgvprXfQ_ZonOi4.svg" alt="Scaling Laws: The Physics of Language Models diagram" width="550" height="330" loading="lazy" decoding="async"></p>
<p>The diagram shows the empirical scaling relationship: loss decreases as a power law with model size. In log-log space, the relationship is linear. Each generation of models (GPT-1 ‚Üí GPT-2 ‚Üí GPT-3 ‚Üí PaLM) follows the same trend, validating the predictability of scaling.</p>
<p><strong>Why scaling laws matter</strong>: They transform AI development from alchemy to engineering. Before scaling laws, improving models required algorithmic innovations‚Äînew architectures, training tricks, clever regularization. Scaling laws show that <strong>size is sufficient</strong>: just make the model bigger, train on more data, use more compute. Performance improvements are nearly guaranteed. This shifts strategy from ‚Äúfind the right algorithm‚Äù to ‚Äúinvest in scale.‚Äù</p>
<p>The practical implication: forecasting. Train a 1B parameter model and measure its loss. Scaling laws predict the loss of a 100B parameter model trained the same way. This enables cost-benefit analysis‚Äîis the 100√ó compute investment worth the predicted performance gain? For applications where marginal improvements matter (search, translation, assistants), the answer is often yes.</p>
<h2 id="phase-transitions-when-abilities-appear-suddenly-ch25">Phase Transitions: When Abilities Appear Suddenly</h2>
<p>Scaling laws describe smooth improvement in loss, but some capabilities don‚Äôt scale smoothly‚Äîthey appear suddenly at specific model sizes. These <strong>emergent abilities</strong> are tasks the model couldn‚Äôt perform at small scale but can perform at large scale, with the transition happening abruptly.</p>
<p>Examples:</p>
<ul>
<li><strong>Multi-step reasoning</strong>: GPT-2 (1.5B) fails at problems requiring multiple reasoning steps. GPT-3 (175B) succeeds on some, GPT-4 on most. The ability doesn‚Äôt scale gradually‚Äîit‚Äôs nearly absent below a threshold, then present above it.</li>
<li><strong>Arithmetic</strong>: Small models (&#x3C;1B parameters) can‚Äôt reliably add large numbers. Larger models (>10B) can, with accuracy improving sharply around 10B parameters.</li>
<li><strong>Translation</strong>: Small models translate common language pairs poorly. Scaling to 100B+ parameters unlocks reliable translation, even for rare language pairs.</li>
</ul>
<p>These abilities emerge because the model crosses a <strong>capability threshold</strong>. Below the threshold, the model has insufficient capacity to compress the patterns required for the task. Above the threshold, capacity suffices, and training data provides enough signal to learn the pattern.</p>
<p><img  src="/eng-ai/_astro/25-diagram-2._MtM9sad_2mxMWX.svg" alt="Phase Transitions: When Abilities Appear Suddenly diagram" width="550" height="350" loading="lazy" decoding="async"></p>
<p>The diagram shows emergent abilities appearing suddenly at specific model sizes. Unlike smooth loss reduction, task accuracy jumps from near-zero to high performance at capability thresholds. Different tasks have different thresholds‚Äîarithmetic emerges earlier than multi-step reasoning.</p>
<p><strong>Why emergence happens</strong>: Tasks vary in complexity. Simple tasks (completing common phrases) require minimal capacity and data‚Äîsmall models suffice. Complex tasks (multi-step reasoning, rare translations) require large capacity to compress the patterns and substantial data to provide signal. Below the capacity threshold, the model can‚Äôt represent the solution; above it, the model can learn the task given sufficient data.</p>
<p>This creates a discontinuous relationship between scale and capability. Loss decreases smoothly (more capacity = better compression), but specific tasks unlock suddenly (sufficient capacity for the pattern). The result: larger models don‚Äôt just do things better‚Äîthey do things smaller models can‚Äôt do at all.</p>
<h2 id="few-shot-learning-learning-from-context-ch25">Few-Shot Learning: Learning from Context</h2>
<p>Perhaps the most surprising emergent ability is <strong>few-shot learning</strong>: the model learns tasks from examples provided in the prompt, without any parameter updates. This <strong>in-context learning</strong> transforms language models into universal function approximators.</p>
<p>Example:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Translate English to French:</span></span>
<span class="line"><span>sea otter => loutre de mer</span></span>
<span class="line"><span>peppermint => menthe poivr√©e</span></span>
<span class="line"><span>plush girafe => girafe peluche</span></span>
<span class="line"><span>cheese =></span></span>
<span class="line"><span></span></span></code></pre>
<p>The model continues with ‚Äúfromage‚Äù (French for cheese). It inferred the task (English‚ÜíFrench translation) from the examples in the prompt, applied the pattern, and generated the correct output‚Äîall without fine-tuning.</p>
<p>This capability emerges at scale. GPT-2 (1.5B) struggles with few-shot learning. GPT-3 (175B) excels: given 0-5 examples, it performs tasks it was never explicitly trained on. Larger models learn more effectively from fewer examples‚ÄîGPT-4 often succeeds with 1-shot or even 0-shot (just the task description, no examples).</p>
<p><strong>Why in-context learning works</strong>: During pretraining, the model sees countless patterns where a task is demonstrated then applied. Web pages explain concepts then give examples. Documentation shows function signatures then usage. Forums ask questions, then provide answers. The model learns the meta-pattern: ‚Äúwhen text shows [task format] followed by [examples], generate [application of the pattern].‚Äù</p>
<p>The model doesn‚Äôt ‚Äúunderstand‚Äù it‚Äôs doing few-shot learning‚Äîit‚Äôs predicting plausible continuations based on patterns in training data. But those patterns happen to include task demonstration ‚Üí task execution, so the model learns to perform tasks from prompts.</p>
<p><strong>Chain-of-thought prompting</strong> extends in-context learning to multi-step reasoning. Instead of asking for direct answers, prompt the model to show its reasoning:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Problem: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.</span></span>
<span class="line"><span>Each can has 3 tennis balls. How many tennis balls does he have now?</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Let's think step by step:</span></span>
<span class="line"><span>- Roger starts with 5 balls</span></span>
<span class="line"><span>- He buys 2 cans</span></span>
<span class="line"><span>- Each can has 3 balls, so 2 cans have 2 √ó 3 = 6 balls</span></span>
<span class="line"><span>- Total: 5 + 6 = 11 balls</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Answer: 11</span></span>
<span class="line"><span></span></span></code></pre>
<p>This prompting strategy significantly improves performance on math, logic, and reasoning tasks. The model generates intermediate steps, which serve as additional context for producing the final answer. Chain-of-thought turns next-token prediction into a reasoning process: each predicted token provides context that improves subsequent predictions.</p>
<p>Chain-of-thought only works at scale. Small models produce nonsensical reasoning chains. Large models (>100B parameters) produce coherent, often correct chains. This is another emergent ability: the capacity to generate useful intermediate reasoning steps appears suddenly above a threshold.</p>
<h2 id="generalization-why-llms-transfer-knowledge-ch25">Generalization: Why LLMs Transfer Knowledge</h2>
<p>Language models trained on internet text generalize to tasks they‚Äôve never seen explicitly. This <strong>zero-shot transfer</strong> happens because pretraining data implicitly contains task patterns.</p>
<p>A model trained on web text sees:</p>
<ul>
<li>Question-answering: forums, FAQs, Quora, Stack Overflow</li>
<li>Summarization: article abstracts, TL;DRs, executive summaries</li>
<li>Translation: multilingual websites, bilingual documents</li>
<li>Code generation: GitHub repositories with comments and implementations</li>
<li>Dialogue: chat logs, Reddit threads, customer service transcripts</li>
</ul>
<p>These aren‚Äôt labeled datasets but naturally occurring examples. The model learns these formats as statistical patterns. When prompted with ‚ÄúQ: ‚Ä¶ A:‚Äù, it predicts text matching the question-answer pattern seen during pretraining. Zero-shot transfer works because pretraining data is so diverse that most tasks appear implicitly.</p>
<p><strong>Why generalization improves with scale</strong>: Larger models have more capacity to compress diverse patterns. A small model must prioritize‚Äîit can learn common tasks but not rare ones. A large model compresses common tasks <em>and</em> rare edge cases. The result: larger models generalize better. GPT-2 fails at most zero-shot tasks. GPT-3 succeeds at many. GPT-4 succeeds at most.</p>
<p>This explains the power of foundation models: they‚Äôre trained on such diverse data that they generalize to countless downstream tasks without task-specific training. The model hasn‚Äôt seen explicit ‚Äúsentiment classification‚Äù training data, but it‚Äôs seen enough text discussing emotions that it can classify sentiment zero-shot. It hasn‚Äôt been trained on ‚Äúcode debugging,‚Äù but it‚Äôs seen enough code and explanations that it can debug zero-shot.</p>
<p>Generalization is compression-driven. The model can‚Äôt memorize all training data, so it compresses‚Äîextracts patterns. Those patterns happen to generalize because they capture the underlying structure of language and tasks described in text.</p>
<h2 id="engineering-takeaway-ch25">Engineering Takeaway</h2>
<p>Scaling transforms language models from narrow predictors to general-purpose systems. Understanding scaling laws, emergent abilities, and in-context learning is essential for leveraging modern AI effectively.</p>
<p><strong>Bigger models are qualitatively different, not just quantitatively better</strong></p>
<p>A 10√ó larger model doesn‚Äôt just perform 10% better‚Äîit unlocks entirely new capabilities. Multi-step reasoning, complex arithmetic, chain-of-thought prompting, and few-shot learning emerge at scale. These aren‚Äôt architectural innovations; they‚Äôre consequences of capacity. When designing systems, assume larger models will do things smaller models can‚Äôt. Plan for emergent abilities: applications that seem impossible with current models may become trivial with the next generation. Scale changes the problem space.</p>
<p><strong>Scaling is expensive but predictable</strong></p>
<p>Training GPT-4-scale models costs tens of millions of dollars. But scaling laws make outcomes predictable: invest 10√ó compute, get a quantifiable performance improvement. This transforms AI development from research (uncertain outcomes) to engineering (predictable ROI). For organizations, this means scaling is a viable strategy‚Äînot a gamble. Budget for larger models based on forecasted performance gains from scaling laws. The economics favor scale: the fixed cost of training is high, but the marginal cost of inference is low, and the model serves millions of users.</p>
<p><strong>Few-shot learning reduces need for task-specific fine-tuning</strong></p>
<p>With large models, many applications don‚Äôt require fine-tuning. Provide examples in the prompt, and the model adapts. This is cheaper (no training cost, no labeled data) and faster (immediate deployment). The trade-off: few-shot performance is lower than fine-tuned performance, and prompt engineering requires iteration. For applications where fine-tuning is expensive or data-limited, few-shot learning is a powerful alternative. As models scale, few-shot performance approaches fine-tuned performance, reducing the need for task-specific training.</p>
<p><strong>In-context learning is free but limited by context window</strong></p>
<p>In-context learning happens at inference time‚Äîno parameter updates, no training. Just provide examples in the prompt. This makes it incredibly flexible: the same model adapts to different tasks dynamically. The constraint: context window size. Current models support 4K‚Äì128K tokens. Long-context models (1M+ tokens) enable even more in-context learning. For production systems, maximize in-context learning by designing prompts that fit essential examples and instructions within the context window. This reduces deployment complexity compared to maintaining fine-tuned models for every task.</p>
<p><strong>Emergent abilities justify the cost of training large models</strong></p>
<p>Smaller models are cheaper to train but lack critical capabilities. A 1B parameter model can‚Äôt do multi-step reasoning, arithmetic, or complex translation. A 100B parameter model can. The cost difference is 100√ó, but the capability difference is qualitative, not quantitative. For applications requiring these abilities, there‚Äôs no substitute for scale. Organizations building AI products must decide: train/use large models with emergent abilities (high cost, high capability) or use smaller models with limited abilities (low cost, limited capability). For many applications, the emergent abilities justify the cost.</p>
<p><strong>Prompting becomes programming</strong></p>
<p>With large models, prompts are code. Chain-of-thought prompting, few-shot examples, instruction formatting‚Äîthese are programming constructs. Engineering effective prompts (Part VI) is now a critical skill. The model‚Äôs behavior is controlled entirely by the input prompt, making prompt design as important as algorithm design in traditional software. Production systems invest heavily in prompt engineering: iterating on formats, testing variations, optimizing for task performance. Understanding how prompts interact with emergent abilities (in-context learning, chain-of-thought) enables building sophisticated applications without fine-tuning.</p>
<p><strong>Compute is the bottleneck for frontier models</strong></p>
<p>Training frontier models requires clusters of tens of thousands of GPUs running for months. The bottleneck isn‚Äôt algorithms or data‚Äîit‚Äôs compute. Scaling laws show exactly how much compute is needed for a target performance level. Organizations with sufficient compute can train state-of-the-art models; those without must use smaller models or API access. The practical implication: AI development increasingly favors organizations with massive compute resources. For most practitioners, accessing frontier models via APIs (OpenAI, Anthropic, Google) is more viable than training from scratch. Understanding scaling laws helps evaluate trade-offs: when is it worth training your own model vs. using a provider‚Äôs API?</p>
<p>The lesson: Scale is the dominant factor in language model capability. Scaling laws make performance improvements predictable‚Äîinvest in size, get better models. But scale doesn‚Äôt just improve performance; it unlocks emergent abilities that smaller models lack entirely. Few-shot learning, chain-of-thought reasoning, and zero-shot generalization appear suddenly at specific model sizes, transforming what‚Äôs possible. Modern AI strategy revolves around scale: training or accessing the largest models feasible, leveraging emergent abilities through prompt engineering, and planning for future capabilities as models continue to grow. Understanding scaling‚Äîwhy it works, what it unlocks, and how to leverage it‚Äîis essential for building and deploying AI systems that push the frontier of what‚Äôs possible.</p>
<hr>
<h2 id="references-and-further-reading-ch25">References and Further Reading</h2>
<p><strong>Scaling Laws for Neural Language Models</strong> ‚Äì Jared Kaplan, Sam McCandlish, Tom Henighan, et al. (2020)
<a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a></p>
<p>Kaplan et al. empirically characterized how language model performance scales with model size, dataset size, and compute. They showed that loss follows predictable power laws across orders of magnitude, enabling forecasting: small-scale experiments predict large-scale performance. The paper quantifies the trade-offs between model size and training data, providing a framework for compute-optimal training. This work transformed AI development from alchemy to engineering‚Äîperformance improvements with scale are nearly guaranteed. The paper‚Äôs insights justified massive investments in training GPT-3 and subsequent models. Reading this explains why scaling became the dominant strategy in AI and how to predict performance gains from increased compute.</p>
<p><strong>Emergent Abilities of Large Language Models</strong> ‚Äì Jason Wei, Yi Tay, Rishi Bommasani, et al. (2022)
<a href="https://arxiv.org/abs/2206.07682">https://arxiv.org/abs/2206.07682</a></p>
<p>Wei et al. documented abilities that appear suddenly at specific model sizes rather than scaling smoothly. They identified dozens of emergent abilities‚Äîmulti-step reasoning, translation, arithmetic‚Äîthat are absent in models below a threshold but present above it. The paper shows that scale doesn‚Äôt just improve performance; it unlocks qualitatively new capabilities. This explains why GPT-3 can do tasks GPT-2 can‚Äôt, and why GPT-4 outperforms GPT-3 not just marginally but categorically. Understanding emergent abilities is critical for planning AI applications: current models may lack necessary capabilities, but the next generation may possess them. Reading this clarifies what scale buys beyond lower loss‚Äîentirely new capabilities.</p>
<p><strong>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</strong> ‚Äì Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. (2022)
<a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a></p>
<p>Wei et al. showed that prompting large models to generate intermediate reasoning steps (chain-of-thought) dramatically improves performance on complex reasoning tasks. Instead of directly answering, the model shows its work‚Äîbreaking problems into steps, which serve as context for subsequent predictions. This simple prompting strategy unlocks reasoning-like behavior in models trained only on next-token prediction. The paper demonstrates that emergent abilities (here, multi-step reasoning) can be amplified through clever prompting. Chain-of-thought is now standard in production systems for math, logic, and complex question answering. Reading this clarifies how to leverage large models effectively: prompting strategies can unlock capabilities that seem absent without the right input format.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part5/24-rlhf" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>RLHF</span> </a> <a href="/eng-ai/part6" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Part VI: AI Systems</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-bigger-models-do-new-things-ch25" data-astro-cid-xvrfupwn>Why Bigger Models Do New Things</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#scaling-laws-the-physics-of-language-models-ch25" data-astro-cid-xvrfupwn>Scaling Laws: The Physics of Language Models</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#phase-transitions-when-abilities-appear-suddenly-ch25" data-astro-cid-xvrfupwn>Phase Transitions: When Abilities Appear Suddenly</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#few-shot-learning-learning-from-context-ch25" data-astro-cid-xvrfupwn>Few-Shot Learning: Learning from Context</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#generalization-why-llms-transfer-knowledge-ch25" data-astro-cid-xvrfupwn>Generalization: Why LLMs Transfer Knowledge</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch25" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch25" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>