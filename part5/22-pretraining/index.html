<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 22: Pretraining | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part5/22-pretraining/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part5/22-pretraining/"><meta property="og:title" content="Chapter 22: Pretraining | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part5/22-pretraining/"><meta name="twitter:title" content="Chapter 22: Pretraining | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part5" data-astro-cid-ilhxcym7>Part V: Language Models</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Pretraining</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-22-pretraining">Chapter 22: Pretraining</h1>
<h2 id="learning-from-the-internet-ch22">Learning from the Internet</h2>
<h2 id="self-supervised-learning-without-labels-ch22">Self-Supervised Learning Without Labels</h2>
<p>Language model pretraining is self-supervised learning: training without human-provided labels. Instead of labeled datasets‚Äî‚Äúthis image is a cat,‚Äù ‚Äúthis email is spam‚Äù‚Äîthe training data itself provides the supervision.</p>
<p>The next token in a sequence is the label. Given ‚ÄúThe capital of France is‚Äù, the label is ‚ÄúParis‚Äù‚Äînot added by humans but extracted from the text itself. Every document becomes millions of training examples: each position provides a context (preceding tokens) and a target (the next token). A single sentence ‚ÄúThe cat sat on the mat‚Äù yields six training examples:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Context: "The"              Target: "cat"</span></span>
<span class="line"><span>Context: "The cat"          Target: "sat"</span></span>
<span class="line"><span>Context: "The cat sat"      Target: "on"</span></span>
<span class="line"><span>Context: "The cat sat on"   Target: "the"</span></span>
<span class="line"><span>Context: "The cat sat on the" Target: "mat"</span></span>
<span class="line"><span></span></span></code></pre>
<p>Self-supervision eliminates the need for manual annotation. Labeling data is expensive: humans must read, understand, and categorize each example. For image classification, experts label thousands of images. For machine translation, bilingual speakers align sentence pairs. These datasets are limited by cost and human effort‚ÄîImageNet has ~1 million images, translation corpora rarely exceed tens of millions of sentence pairs.</p>
<p>Language modeling bypasses this bottleneck. Any text provides supervision‚Äîbooks, articles, websites, code, conversations. The internet contains trillions of words, all freely available (legally or otherwise). No humans need to label what comes next; the text itself contains the answer. This abundance of free supervision is why language models scale beyond supervised methods.</p>
<p>The trade-off: self-supervised learning uses a proxy objective. The model optimizes next-token prediction, not the actual task (question answering, translation, summarization). The bet is that learning to predict text forces the model to internalize patterns useful for downstream tasks. Chapter 21 explained why this works: prediction requires compression, and compression captures structure. Pretraining bets that text prediction is a rich enough objective to learn general language understanding.</p>
<h2 id="why-scale-matters-coverage-of-reality-ch22">Why Scale Matters: Coverage of Reality</h2>
<p>The internet contains discussions of science, history, politics, fiction, code, recipes, instructions, arguments, and nonsense. Pretraining on this diversity exposes the model to vastly more concepts, patterns, and edge cases than any curated dataset.</p>
<p><strong>Coverage determines capability</strong>. A model trained on 100M tokens sees limited vocabulary, few examples of rare constructions, and narrow domains. A model trained on 500B tokens (GPT-3) encounters:</p>
<ul>
<li>Common words tens of millions of times (strong statistical signal)</li>
<li>Rare words thousands of times (sufficient to learn embeddings)</li>
<li>Technical jargon across domains (medicine, law, engineering)</li>
<li>Multiple languages (enabling multilingual understanding)</li>
<li>Diverse reasoning patterns (math proofs, legal arguments, code logic)</li>
<li>Edge cases and exceptions (idioms, sarcasm, historical events)</li>
</ul>
<p>More data means more knowledge gets compressed into parameters. A model can only predict ‚ÄúParis is the capital of France‚Äù if it saw enough instances to encode that association. For common facts (capitals of major countries), thousands of examples suffice. For rare facts (obscure historical events), few examples exist‚Äîthe model may never learn them or overgeneralize from insufficient data.</p>
<p><strong>Data diversity enables generalization</strong>. Training on Wikipedia alone produces models good at encyclopedia-style text but poor at conversations, code, or creative writing. Training on diverse sources‚Äîweb crawls, books, GitHub, forums‚Äîproduces models that generalize across domains. The model learns patterns common to all text (grammar, factual structure, reasoning) while also learning domain-specific conventions (code syntax, formal vs informal tone).</p>
<p>The scale of modern pretraining is staggering:</p>
<ul>
<li><strong>GPT-2</strong> (2019): 1.5B parameters, 40GB text (~10B tokens)</li>
<li><strong>GPT-3</strong> (2020): 175B parameters, ~300GB text (~500B tokens)</li>
<li><strong>PaLM</strong> (2022): 540B parameters, ~780B tokens</li>
<li><strong>LLaMA 2</strong> (2023): 70B parameters, 2 trillion tokens</li>
</ul>
<p>Each generation trains on more data. Why? Because performance continues to improve. Scaling laws (Chapter 25) show predictable improvements with data size‚Äîdouble the data, reduce the loss by a consistent factor. There‚Äôs no sign of saturation: more data keeps helping.</p>
<p><img  src="/eng-ai/_astro/22-diagram-1.DH-nuu8j_1tfjyK.svg" alt="Why Scale Matters: Coverage of Reality diagram" width="550" height="320" loading="lazy" decoding="async"></p>
<p>The diagram shows pretraining as compression: diverse training data (web text, books, code) is compressed into model parameters, which encode grammar, facts, reasoning patterns‚Äîand biases. The model learns whatever statistical regularities exist in the data, both useful and harmful.</p>
<h2 id="token-diversity-and-concept-learning-ch22">Token Diversity and Concept Learning</h2>
<p>A language model can only predict tokens it has seen during training. If ‚Äúquantum chromodynamics‚Äù never appears in the training data, the model won‚Äôt predict it‚Äîit‚Äôs outside the learned vocabulary.</p>
<p>This creates a coverage problem for rare concepts. Common words (‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúcat‚Äù) appear millions of times, providing strong signal. Technical terms (‚Äúglioblastoma‚Äù, ‚Äúrecombinase‚Äù, ‚Äúmerkle tree‚Äù) appear rarely, providing weak signal. If a medical term appears only 10 times, the model may not learn its meaning or may overgeneralize from insufficient context.</p>
<p><strong>Token frequency determines learning</strong>. High-frequency tokens (top 1000 words) are learned robustly‚Äîthe model sees them in countless contexts and learns precise embeddings and usage patterns. Mid-frequency tokens (10K‚Äì50K rank) are learned adequately if the dataset is large. Low-frequency tokens (tail of vocabulary) are learned poorly or not at all‚Äîfew examples don‚Äôt provide enough signal to distinguish them from noise.</p>
<p>This explains why larger datasets improve performance on specialized domains. A model trained on 10B tokens may rarely see biochemistry terminology. A model trained on 1T tokens sees biochemistry papers thousands of times, learning domain-specific patterns. The long tail of rare concepts requires massive data to cover adequately.</p>
<p><strong>Multilingual coverage</strong>: English dominates the internet (~60% of web pages), but GPT models learn dozens of languages by exposure to non-English text. The model learns that ‚Äúcat‚Äù (English), ‚Äúchat‚Äù (French), ‚ÄúKatze‚Äù (German) refer to similar concepts through contextual similarity in training data. However, languages with less training data (e.g., low-resource languages like Swahili or Urdu) are learned poorly compared to high-resource languages (English, Spanish, Chinese).</p>
<p>The quality of learned representations correlates directly with data quantity. This creates performance disparities: models excel at English but struggle with low-resource languages, understand common topics better than obscure ones, and perform well on popular domains (general knowledge, programming) but poorly on specialized fields (rare medical conditions, niche legal subfields).</p>
<h2 id="spurious-correlations-and-inherited-biases-ch22">Spurious Correlations and Inherited Biases</h2>
<p>Training data reflects reality‚Äîincluding its biases, misinformation, and problematic patterns. Models learn statistical associations without distinguishing true patterns from spurious correlations.</p>
<p><strong>Biases are statistical regularities</strong>. If training data contains gender stereotypes (‚Äúdoctor‚Äù often appears with ‚Äúhe‚Äù, ‚Äúnurse‚Äù with ‚Äúshe‚Äù), the model learns these associations. Asked to complete ‚ÄúThe doctor entered the room. She ___‚Äù, the model may generate text reflecting stereotyped assumptions because that pattern was statistically common in training data. The model doesn‚Äôt ‚Äúbelieve‚Äù stereotypes‚Äîit predicts based on learned correlations.</p>
<p><strong>Misinformation becomes encoded</strong>. If conspiracy theories or false claims appear frequently in training data, the model learns to predict them as plausible continuations. It can‚Äôt distinguish truth from falsehood without external verification. The model learns that ‚Äúvaccines cause autism‚Äù is a phrase that appears in text, even though it‚Äôs factually false. Prediction doesn‚Äôt require truth‚Äîonly statistical frequency.</p>
<p><strong>Toxic content and harmful patterns</strong>: Internet text includes hate speech, offensive language, and instructions for harmful activities. Models trained on unfiltered web crawls absorb these patterns. A model completing ‚ÄúGroup X people are ___‚Äù may generate offensive completions because such text existed in training data. This is not malice‚Äîit‚Äôs compression of statistical patterns, including harmful ones.</p>
<p>Data curation attempts to mitigate these issues but trades coverage for safety:</p>
<ul>
<li><strong>Filtering toxic content</strong>: Removes explicitly harmful text but reduces dataset size and may introduce new biases (over-filtering innocuous content)</li>
<li><strong>Deduplication</strong>: Removes repeated text (preventing memorization) but may eliminate rare examples needed for coverage</li>
<li><strong>Source selection</strong>: Prioritizing high-quality sources (books, Wikipedia) over low-quality ones (forums, comments) improves average quality but narrows diversity</li>
</ul>
<p>There‚Äôs no perfect solution. Filtering reduces harms but also reduces coverage of reality. Unfiltered data maximizes coverage but includes harmful content. Modern pretraining balances these trade-offs through careful curation‚Äîremoving the most egregious content while preserving diversity. However, biases persist because they‚Äôre woven into human-generated text. Models will reflect their training data‚Äôs values, biases, and errors unless explicitly aligned through post-training (Chapters 23‚Äì24).</p>
<h2 id="the-mathematics-of-data-scaling-ch22">The Mathematics of Data Scaling</h2>
<p>Performance improves with data size following empirical scaling laws (detailed in Chapter 25). The relationship is a power law:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>‚àù</mo><msup><mi>D</mi><mrow><mo>‚àí</mo><mi>Œ±</mi></mrow></msup></mrow><annotation encoding="application/x-tex">L(D) \propto D^{-\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àù</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8213em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8213em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚àí</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">Œ±</span></span></span></span></span></span></span></span></span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span> is test loss, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span> is dataset size (measured in tokens), and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span></span></span></span> is the scaling exponent (typically <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>‚âà</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\alpha \approx 0.1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4831em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.1</span></span></span></span>‚Äì<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.2</mn></mrow><annotation encoding="application/x-tex">0.2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.2</span></span></span></span>). Doubling training data reduces loss by a predictable factor.</p>
<p>In log-log space, this appears as a straight line: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>‚Å°</mo><mi>L</mi><mo>=</mo><mo>‚àí</mo><mi>Œ±</mi><mi>log</mi><mo>‚Å°</mo><mi>D</mi><mo>+</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">\log L = -\alpha \log D + c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">‚àí</span><span class="mord mathnormal" style="margin-right:0.0037em;">Œ±</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span>. The linearity of this relationship enables forecasting: experiments with small datasets predict performance at large scales. This predictability justifies massive investments in compute and data‚Äîperformance gains are nearly guaranteed with scale.</p>
<p><img  src="/eng-ai/_astro/22-diagram-2.DBcz5o5Y_Z2kujrA.svg" alt="The Mathematics of Data Scaling diagram" width="550" height="330" loading="lazy" decoding="async"></p>
<p>The diagram shows the empirical relationship between training data size and loss. In log-log space, the curve is approximately linear (power law). Each successive generation of models trains on more data and achieves lower loss. This predictable scaling justifies continued investment in larger datasets.</p>
<p>However, scaling has diminishing returns: the first 10B tokens provide massive improvement, the next 90B provide moderate improvement, the next 900B provide incremental gains. At some point, the cost of additional data exceeds the value of marginal performance gains. Current models (2023‚Äì2024) train on 1‚Äì2 trillion tokens, approaching the limit of available high-quality text on the internet.</p>
<h2 id="engineering-takeaway-ch22">Engineering Takeaway</h2>
<p>Pretraining on massive datasets is the foundation of modern language models. Understanding data scale, diversity, and curation explains model capabilities and limitations.</p>
<p><strong>Pretraining is expensive but amortizes across all tasks</strong></p>
<p>Training GPT-3 cost an estimated $4‚Äì5 million in compute. Training cutting-edge models costs tens of millions. But once trained, the model serves millions of users and countless applications. Pretraining cost is amortized over all downstream uses‚Äîfine-tuning (Chapter 23), prompting (Part VI), and deployment. This is why foundation models dominate: the fixed cost of pretraining is enormous, but the marginal cost of reusing the model is near zero.</p>
<p><strong>Data quality matters more than quantity at the margin</strong></p>
<p>Early scaling (10B ‚Üí 100B tokens) benefits from any data. Later scaling (500B ‚Üí 2T tokens) saturates on low-quality sources. Adding more Reddit comments provides diminishing returns compared to high-quality books or technical documentation. Modern pretraining curates data sources, prioritizing quality and diversity over raw quantity. Filtering, deduplication, and source selection improve model behavior even if total token count decreases.</p>
<p><strong>Data curation trades coverage for safety</strong></p>
<p>Removing toxic content reduces harmful outputs but narrows the model‚Äôs understanding of the world. Over-filtering can introduce new biases (e.g., removing all mentions of certain topics makes the model ignorant of them, even for benign queries). Curation is a judgment call: what content is harmful enough to exclude vs. valuable enough to include? There‚Äôs no perfect answer. Modern pipelines use automated toxicity classifiers, source reputation, and manual review to balance safety and coverage.</p>
<p><strong>Deduplication prevents memorization and improves generalization</strong></p>
<p>Training data often contains duplicates (identical or near-identical text appearing multiple times). Duplicates cause models to memorize specific sequences rather than learning general patterns. Deduplication (removing repeated text) improves generalization by forcing the model to compress diverse examples rather than memorizing common ones. However, some duplication is benign (common phrases, idioms), and over-aggressive deduplication can remove valid training signal. Practical systems use fuzzy matching (e.g., MinHash, SimHash) to remove near-duplicates while preserving useful repetition.</p>
<p><strong>Compute-optimal training balances model size and data size</strong></p>
<p>The Chinchilla paper (Hoffmann et al., 2022) showed that prior models were undertrained: given a compute budget, it‚Äôs better to train smaller models on more data than larger models on less data. GPT-3 (175B parameters, ~500B tokens) was trained on insufficient data relative to its size. Chinchilla (70B parameters, 1.4T tokens) achieved lower loss with the same compute by reducing model size and increasing data. The optimal ratio: for every doubling of model size, double the amount of training data. This insight shifted pretraining strategies toward longer training runs on massive datasets rather than simply scaling model parameters.</p>
<p><strong>Checkpoints capture knowledge at different training stages</strong></p>
<p>Models improve continuously during training as they process more data. Early checkpoints (e.g., after 10% of training) have learned basic patterns but not specialized knowledge. Late checkpoints (after 90%) have learned nuanced patterns but may overfit to training distribution. Saving intermediate checkpoints allows selecting the model that balances generalization and performance. Some applications prefer earlier checkpoints (less overfitting), others prefer late checkpoints (maximum capability).</p>
<p><strong>Why foundation models work: transfer from pretraining to any task</strong></p>
<p>Pretraining creates general-purpose models. The model hasn‚Äôt seen tasks explicitly but has seen task-like patterns in diverse data. Question-answering appears in forums, FAQ pages, and textbooks. Translation appears in multilingual documents. Code generation appears in GitHub repositories with comments. The model learns these formats as statistical patterns, enabling zero-shot transfer to formal tasks (Chapter 23). Foundation models work because pretraining data is diverse enough to contain implicit examples of most tasks humans care about.</p>
<p>The lesson: Language model capabilities scale with data. More diverse, high-quality training data produces more capable, knowledgeable models. But data isn‚Äôt neutral‚Äîit encodes biases, misinformation, and toxic patterns. Pretraining compresses the internet, including both its wisdom and its flaws. Understanding data curation, coverage, and scaling laws is essential for building and deploying modern language models effectively.</p>
<hr>
<h2 id="references-and-further-reading-ch22">References and Further Reading</h2>
<p><strong>Language Models are Few-Shot Learners</strong> ‚Äì Tom B. Brown, Benjamin Mann, Nick Ryder, et al. (2020)
<a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></p>
<p>The GPT-3 paper demonstrated that scaling model size and training data produces qualitative improvements in capability. Brown et al. trained a 175B parameter model on ~500B tokens and showed it could perform tasks zero-shot or few-shot without fine-tuning‚Äîanswering questions, translating languages, writing code‚Äîpurely from in-context examples. This established scale as a primary driver of performance: bigger models trained on more data unlock emergent abilities. The paper includes detailed ablations showing how data diversity (web text, books, Wikipedia) affects performance across domains. Reading this explains why pretraining on internet-scale data is the foundation of modern AI and how capabilities emerge from scale alone.</p>
<p><strong>Training Compute-Optimal Large Language Models</strong> ‚Äì Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, et al. (2022)
<a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a></p>
<p>The Chinchilla paper challenged conventional scaling wisdom. Hoffmann et al. showed that GPT-3-scale models were undertrained‚Äîgiven fixed compute, it‚Äôs better to train smaller models on more data. Chinchilla (70B parameters, 1.4T tokens) outperformed much larger models (GPT-3, Gopher) by using a compute-optimal training regime. The paper derives scaling laws for optimal model size vs. data size and provides practical guidance: for every doubling of parameters, double training tokens. This reshaped pretraining strategy: modern models train longer on larger datasets rather than simply adding parameters. Understanding compute-optimal scaling clarifies the economics of training large models and why data matters as much as model size.</p>
<p><strong>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</strong> ‚Äì Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell (2021)
<a href="https://dl.acm.org/doi/10.1145/3442188.3445922">https://dl.acm.org/doi/10.1145/3442188.3445922</a></p>
<p>Bender et al. critically examine the costs and risks of large language models. They discuss environmental impact (massive compute requires enormous energy), bias (models inherit training data biases), misinformation (models generate plausible but false text), and misalignment (models optimize prediction, not truth or ethics). The paper argues for responsible scaling: considering societal impacts, transparency in data curation, and alignment research alongside capability improvements. While not a technical paper, it provides essential context on why pretraining choices matter beyond performance metrics. Understanding the dangers and trade-offs of scale is crucial for building AI systems responsibly. Reading this grounds technical decisions in ethical and societal considerations.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part5/21-next-token-prediction" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Next-Token Prediction</span> </a> <a href="/eng-ai/part5/23-fine-tuning" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Fine-Tuning</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#learning-from-the-internet-ch22" data-astro-cid-xvrfupwn>Learning from the Internet</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#self-supervised-learning-without-labels-ch22" data-astro-cid-xvrfupwn>Self-Supervised Learning Without Labels</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-scale-matters-coverage-of-reality-ch22" data-astro-cid-xvrfupwn>Why Scale Matters: Coverage of Reality</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#token-diversity-and-concept-learning-ch22" data-astro-cid-xvrfupwn>Token Diversity and Concept Learning</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#spurious-correlations-and-inherited-biases-ch22" data-astro-cid-xvrfupwn>Spurious Correlations and Inherited Biases</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#the-mathematics-of-data-scaling-ch22" data-astro-cid-xvrfupwn>The Mathematics of Data Scaling</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch22" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch22" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>