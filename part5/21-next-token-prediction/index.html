<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 21: Next-Token Prediction | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part5/21-next-token-prediction/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part5/21-next-token-prediction/"><meta property="og:title" content="Chapter 21: Next-Token Prediction | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part5/21-next-token-prediction/"><meta name="twitter:title" content="Chapter 21: Next-Token Prediction | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"â€º";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>ğŸ“–
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part5" data-astro-cid-ilhxcym7>Part V: Language Models</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Next-Token Prediction</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-21-next-token-prediction">Chapter 21: Next-Token Prediction</h1>
<h2 id="the-engine-of-language-models-ch21">The Engine of Language Models</h2>
<h2 id="language-as-probability-ch21">Language as Probability</h2>
<p>Every language modelâ€”from GPT-3 to Claude to LLaMAâ€”is trained to solve a single task: predict the next word. Thatâ€™s it. No explicit instruction following, no question answering, no code generation. Just predict what comes next in a sequence of text.</p>
<p>This simple objective is the foundation of all modern large language models. Everything these models can doâ€”answering questions, writing code, translating languages, engaging in conversationâ€”emerges from learning to predict the next token in a sequence. Understanding this objective is essential to understanding how language models work and why they behave the way they do.</p>
<p>The architecture is a Transformer (Chapter 20), but the task is next-token prediction. Given a sequence of tokens <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, \ldots, x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">â€¦</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, the model predicts the probability distribution over the next token <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>. The model is trained on billions of sequences, learning statistical patterns that enable it to assign high probability to likely continuations and low probability to unlikely ones.</p>
<h2 id="tokens-the-atoms-of-language-ch21">Tokens: The Atoms of Language</h2>
<p>Before prediction, text must be broken into tokensâ€”the atomic units of language modeling. A token might be a word, a subword, or even a character, depending on the tokenization scheme.</p>
<p><strong>Tokenization</strong> converts raw text into a sequence of integers from a vocabulary. For English, common tokenization schemes like Byte-Pair Encoding (BPE) or WordPiece create vocabularies of 30,000 to 100,000 tokens. Common words get their own token (â€œhelloâ€ â†’ token 4521), while rare words are split into subwords (â€œunhappinessâ€ â†’ â€œunâ€ + â€œhappinessâ€ â†’ tokens 837, 9204).</p>
<p>Why subword tokenization? It balances vocabulary size with coverage. A character-level vocabulary (26 letters + punctuation) is tiny but sequences are long. A word-level vocabulary covers common words but struggles with rare terms and produces massive vocabularies. Subword tokenization compresses common patterns while handling rare words through composition.</p>
<p>Example tokenization for â€œThe cat sat on the matâ€:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Text:   "The"  "cat"  "sat"  "on"  "the"  "mat"</span></span>
<span class="line"><span>Tokens:  464    3797   3332   319    262   3298</span></span>
<span class="line"><span></span></span></code></pre>
<p>The model operates on token IDs, not raw text. The vocabulary size (typically V â‰ˆ 50,000) determines the output dimension: for each input sequence, the model predicts a probability distribution over all V possible next tokens.</p>
<h2 id="the-training-objective-predicting-what-comes-next-ch21">The Training Objective: Predicting What Comes Next</h2>
<p>Given a sequence of tokens <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, \ldots, x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">â€¦</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, the model computes:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">âˆ£</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x_{t+1} | x_1, x_2, \ldots, x_t; \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mord">âˆ£</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">â€¦</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Î¸</span><span class="mclose">)</span></span></span></span></span>
<p>This is a probability distribution over the vocabulary. The model assigns a probability to every possible next token. For â€œThe cat sat on the ___â€, the model might output:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>P("mat" | context)   = 0.24</span></span>
<span class="line"><span>P("floor" | context) = 0.18</span></span>
<span class="line"><span>P("chair" | context) = 0.12</span></span>
<span class="line"><span>P("couch" | context) = 0.08</span></span>
<span class="line"><span>P("roof" | context)  = 0.02</span></span>
<span class="line"><span>...</span></span>
<span class="line"><span>P("quantum" | context) = 0.0001</span></span>
<span class="line"><span></span></span></code></pre>
<p>High probability for plausible continuations, low probability for implausible ones. The modelâ€™s parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î¸</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Î¸</span></span></span></span> (billions of weights in the Transformer) determine these probabilities.</p>
<p><strong>Training</strong> adjusts <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î¸</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Î¸</span></span></span></span> to match reality. The model sees text from the internet: â€œThe cat sat on the mat.â€ It predicts the probability of â€œmatâ€ given â€œThe cat sat on theâ€, then updates its weights to increase that probability. Over billions of examples, the model learns the statistical structure of language.</p>
<p>The loss function is <strong>cross-entropy</strong>:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><mo>âˆ’</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>â¡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">âˆ£</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo separator="true">;</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \log P(x_i | x_1, \ldots, x_{i-1}; \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mord">âˆ’</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">âˆ‘</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">âˆ£</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">â€¦</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">âˆ’</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Î¸</span><span class="mclose">)</span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> is the sequence length. This measures surprise: how unexpected was the actual next token given the modelâ€™s prediction? If the model assigned high probability to the correct token, loss is low. If it assigned low probability (was surprised), loss is high. Training minimizes average surprise over all sequences in the training data.</p>
<p><strong>Autoregressive generation</strong>: The same model that predicts one token can generate entire sequences. Start with a prompt (â€œOnce upon a timeâ€), predict the next token (â€œthereâ€), append it, and repeat. Each token becomes context for predicting the next:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Prompt:  "Once upon a time"</span></span>
<span class="line"><span>Output:  "there" (predicted given prompt)</span></span>
<span class="line"><span>Append:  "Once upon a time there"</span></span>
<span class="line"><span>Output:  "was" (predicted given extended sequence)</span></span>
<span class="line"><span>Append:  "Once upon a time there was"</span></span>
<span class="line"><span>Output:  "a" (predicted given full context)</span></span>
<span class="line"><span>...</span></span>
<span class="line"><span></span></span></code></pre>
<p>This autoregressive processâ€”using model outputs as inputs for the next stepâ€”continues until the model emits a stop token or reaches a length limit. The entire sequence is generated by repeatedly applying the same prediction operation.</p>
<p><img  src="/eng-ai/_astro/21-diagram-1.Cfcuobuz_2uTY6p.svg" alt="The Training Objective: Predicting What Comes Next diagram" width="600" height="280" loading="lazy" decoding="async"></p>
<p>The diagram shows the core process: input tokens flow through the Transformer, which outputs a probability distribution over all vocabulary tokens. The model assigns high probability to plausible continuations and low probability to implausible ones.</p>
<h2 id="why-prediction-creates-understanding-ch21">Why Prediction Creates Understanding</h2>
<p>How does predicting the next word lead to capabilities like question answering, reasoning, and code generation? The model was never explicitly trained on these tasksâ€”it only saw next-token prediction. Yet it performs them.</p>
<p>The answer: to predict well, the model must learn patterns that capture the structure of language and the world it describes.</p>
<p><strong>Grammar and syntax</strong>: To predict likely next words, the model must learn grammatical structure. â€œThe cat satâ€ â†’ â€œonâ€ is likely, â€œThe cat satâ€ â†’ â€œpurpleâ€ is unlikely. The model learns subject-verb agreement, tense, word orderâ€”not through explicit rules but through statistical patterns in billions of examples.</p>
<p><strong>Facts and knowledge</strong>: To predict â€œParis is the capital of ___â€, the model needs to have learned that â€œFranceâ€ is highly probable. This requires encoding factual knowledge about the world. The model doesnâ€™t memorize individual factsâ€”it compresses statistical regularities. If â€œParis is the capital of Franceâ€ appears thousands of times in training data, the association becomes encoded in the modelâ€™s parameters.</p>
<p><strong>Reasoning patterns</strong>: To complete â€œIf X is greater than Y, and Y is greater than Z, then X is ___â€, the model must recognize logical patterns. It learns that â€œgreaterâ€ follows transitivity. Not through formal logic, but through exposure to reasoning-like text where such patterns hold.</p>
<p><strong>Task formats</strong>: Training data includes text formatted as questions and answers, instructions and responses, code and comments. The model learns these formats as statistical patterns. When prompted with â€œQuestion: â€¦ Answer:â€, it predicts text that looks like an answer because thatâ€™s what followed â€œQuestion:â€ in training data.</p>
<p>This is <strong>compression-driven understanding</strong>. The model canâ€™t memorize all training data (trillions of tokens wonâ€™t fit in billions of parameters). It must compressâ€”extract patterns that generalize. The patterns that best compress language are the ones that capture its underlying structure: grammar, facts, reasoning, conventions.</p>
<p>Prediction doesnâ€™t require â€œunderstandingâ€ in a philosophical sense. It requires learning statistical patterns that happen to align with the structure of reality. The model assigns high probability to â€œParisâ€ after â€œcapital of Franceâ€ not because it â€œknowsâ€ geography but because that compression minimizes prediction error on training data.</p>
<h2 id="sampling-and-temperature-controlling-randomness-ch21">Sampling and Temperature: Controlling Randomness</h2>
<p>Given a probability distribution over tokens, how does the model choose which one to output? Several strategies exist, each with trade-offs.</p>
<p><strong>Greedy decoding</strong>: Always pick the highest-probability token. Simple and deterministic but leads to repetitive, boring text. The model falls into loops: â€œI think that I think that I thinkâ€¦â€ because â€œIâ€ has highest probability after â€œthatâ€, and â€œthatâ€ after â€œthinkâ€.</p>
<p><strong>Temperature sampling</strong>: Before selecting, adjust probabilities using a temperature parameter <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span>:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>P</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>â¡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mi>T</mi><mo stretchy="false">)</mo></mrow><mrow><munder><mo>âˆ‘</mo><mi>j</mi></munder><mi>exp</mi><mo>â¡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mi>j</mi></msub><mi mathvariant="normal">/</mi><mi>T</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P_i^{(T)} = \frac{\exp(z_i / T)}{\sum_{j} \exp(z_j / T)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3217em;vertical-align:-0.2769em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.5488em;vertical-align:-1.1218em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">âˆ‘</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:1.1218em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> are the modelâ€™s raw logits (pre-softmax scores). Temperature controls randomness:</p>
<ul>
<li><strong>T = 1</strong>: Original distribution (default)</li>
<li><strong>T â†’ 0</strong>: Peaked distribution (approaches greedy decoding; high-probability tokens dominate)</li>
<li><strong>T > 1</strong>: Flattened distribution (increases probability of lower-ranked tokens)</li>
</ul>
<p>Temperature trades off quality and diversity. Low T produces safe, predictable completions. High T produces creative but potentially nonsensical completions.</p>
<p>Example: â€œThe weather today is ___â€</p>
<p>At T = 0.1 (low temperature, peaked distribution):</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>P("sunny") = 0.85</span></span>
<span class="line"><span>P("cloudy") = 0.10</span></span>
<span class="line"><span>P("rainy") = 0.03</span></span>
<span class="line"><span>P("purple") = 0.0001</span></span>
<span class="line"><span></span></span></code></pre>
<p>â†’ Likely output: â€œsunnyâ€ (highly likely)</p>
<p>At T = 1.0 (default):</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>P("sunny") = 0.42</span></span>
<span class="line"><span>P("cloudy") = 0.28</span></span>
<span class="line"><span>P("rainy") = 0.18</span></span>
<span class="line"><span>P("purple") = 0.001</span></span>
<span class="line"><span></span></span></code></pre>
<p>â†’ Likely output: one of {sunny, cloudy, rainy} (balanced)</p>
<p>At T = 2.0 (high temperature, flat distribution):</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>P("sunny") = 0.30</span></span>
<span class="line"><span>P("cloudy") = 0.25</span></span>
<span class="line"><span>P("rainy") = 0.20</span></span>
<span class="line"><span>P("purple") = 0.08</span></span>
<span class="line"><span></span></span></code></pre>
<p>â†’ Possible output: even unlikely tokens like â€œpurpleâ€ have non-negligible probability</p>
<p><img  src="/eng-ai/_astro/21-diagram-2.63epWw4L_1D1o0c.svg" alt="Sampling and Temperature: Controlling Randomness diagram" width="550" height="300" loading="lazy" decoding="async"></p>
<p><strong>Top-k sampling</strong>: Only consider the k most probable tokens, then sample from the renormalized distribution over those k. This prevents sampling from the long tail of improbable tokens (which can produce nonsense) while allowing diversity among plausible options.</p>
<p><strong>Nucleus (top-p) sampling</strong>: Instead of a fixed k, choose the smallest set of tokens whose cumulative probability exceeds p (e.g., p = 0.9). This adapts to the distribution: peaked distributions use fewer tokens, flat distributions use more. Prevents both repetitive text (low diversity) and nonsense (sampling from very low-probability tokens).</p>
<p>These sampling strategies control the trade-off between <strong>quality</strong> (selecting high-probability, sensible completions) and <strong>diversity</strong> (avoiding repetitive, predictable text). Production systems tune these parameters based on the application: factual question answering uses low temperature (prioritize accuracy), creative writing uses higher temperature (prioritize variety).</p>
<h2 id="engineering-takeaway-ch21">Engineering Takeaway</h2>
<p>Next-token prediction is the foundation of modern language models. Understanding this objective explains their capabilities, limitations, and how to control them effectively.</p>
<p><strong>Next-token prediction is all you need</strong></p>
<p>No task-specific training is required. Models trained solely on next-token prediction can answer questions, write code, translate languages, and engage in conversation. These capabilities emerge from learning to predict textâ€”the objective compresses all tasks that can be expressed in language. This is why pretraining (Chapter 22) on raw internet text produces general-purpose models that transfer to countless applications.</p>
<p><strong>Prompts are control mechanisms</strong></p>
<p>The input to a language model is not a commandâ€”itâ€™s a prefix that steers the probability distribution over completions. â€œTranslate to French:â€ changes the distribution to favor French continuations. â€œOnce upon a timeâ€ triggers story-like patterns. Prompt engineering (Part VI) is the art of constructing prefixes that induce desired probability distributions. The model doesnâ€™t â€œobeyâ€ promptsâ€”it predicts plausible continuations given the prompt as context.</p>
<p><strong>Temperature trades off quality and diversity</strong></p>
<p>Low temperature (T &#x3C; 0.5): Peaked distributions favor high-probability tokens. Use for factual tasks where correctness matters (question answering, code generation). Outputs are safe but repetitive.</p>
<p>High temperature (T > 1.0): Flat distributions increase diversity. Use for creative tasks where variety matters (story writing, brainstorming). Outputs are interesting but may be nonsensical.</p>
<p>Tune temperature based on whether you prioritize accuracy or creativity. Most production systems use T â‰ˆ 0.7â€“1.0 as a reasonable balance.</p>
<p><strong>Top-k and nucleus sampling prevent nonsense</strong></p>
<p>Sampling from the full distribution (including very low-probability tokens) can produce gibberish. Top-k (k â‰ˆ 40â€“50) and nucleus (p â‰ˆ 0.9â€“0.95) sampling restrict attention to plausible tokens while maintaining diversity. These strategies prevent catastrophic failures (nonsensical completions) while avoiding repetitive loops (greedy decoding). Use them in production systems to balance quality and diversity.</p>
<p><strong>Greedy decoding is deterministic but repetitive</strong></p>
<p>Always choosing the highest-probability token produces the same output every time (deterministic) but leads to repetitive loops. Use greedy decoding only when reproducibility is critical and repetition is acceptable (e.g., debugging). For most applications, stochastic sampling with temperature and top-p is preferable.</p>
<p><strong>Evaluation: perplexity measures surprise</strong></p>
<p><strong>Perplexity</strong> quantifies how surprised the model is by test data:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mi>exp</mi><mo>â¡</mo><mo stretchy="false">(</mo><mi mathvariant="script">L</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo>â¡</mo><mrow><mo fence="true">(</mo><mo>âˆ’</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>â¡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">âˆ£</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>â€¦</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Perplexity} = \exp(\mathcal{L}) = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(x_i | x_1, \ldots, x_{i-1})\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Perplexity</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathcal">L</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mord">âˆ’</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">âˆ‘</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">âˆ£</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">â€¦</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">âˆ’</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span>
<p>Lower perplexity means the model assigns higher probability to the actual textâ€”itâ€™s less surprised. Perplexity is the standard metric for evaluating language models during training. A model with perplexity 100 is â€œas surprised as if it had to choose uniformly among 100 equally likely alternativesâ€ at each step. Better models have lower perplexity.</p>
<p><strong>Why language modeling transfers to all tasks</strong></p>
<p>Any task that can be expressed as text can be framed as next-token prediction. Question answering: predict the answer tokens after â€œQ: â€¦ A:â€. Translation: predict French tokens after English text and â€œFrench:â€. Code generation: predict code tokens after a description. The model learns these patterns from training data where such formats appear. This universalityâ€”all language tasks reduce to predictionâ€”is why language modeling is so powerful.</p>
<p>The lesson: Language models are probability engines. They donâ€™t â€œunderstandâ€ in any deep senseâ€”they assign probabilities to token sequences based on statistical patterns learned from training data. But those patterns are rich enough to capture grammar, facts, reasoning, and task conventions. Next-token prediction, applied at scale, compresses the structure of language and the world it describes. Controlling these models means constructing prompts and tuning sampling strategies to shape probability distributions toward desired outputs.</p>
<hr>
<h2 id="references-and-further-reading-ch21">References and Further Reading</h2>
<p><strong>Improving Language Understanding by Generative Pre-Training</strong> â€“ Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (2018)
<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p>
<p>This is the GPT-1 paper that demonstrated language models trained purely on next-token prediction transfer to diverse tasks. Radford et al. showed that unsupervised pretraining on web text, followed by minimal fine-tuning, achieves strong performance on question answering, classification, and inference tasks. This established the paradigm: pretrain on prediction, fine-tune for tasks. The paper introduced the idea that language modeling isnâ€™t just about text generationâ€”itâ€™s a general-purpose learning objective. Understanding GPT-1 clarifies why next-token prediction is sufficient for building capable AI systems.</p>
<p><strong>The Curious Case of Neural Text Degeneration</strong> â€“ Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi (2019)
<a href="https://arxiv.org/abs/1904.09751">https://arxiv.org/abs/1904.09751</a></p>
<p>Holtzman et al. analyzed why greedy decoding and beam search produce repetitive, low-quality text despite high-probability outputs. They introduced nucleus (top-p) sampling, which balances quality and diversity by dynamically adjusting the sampling pool based on cumulative probability. The paper explains sampling strategiesâ€™ impact on generation quality and provides empirical evidence for why stochastic methods outperform deterministic ones. Reading this clarifies how to control language model outputs in production and why temperature and sampling parameters matter.</p>
<p><strong>The Unreasonable Effectiveness of Recurrent Neural Networks</strong> â€“ Andrej Karpathy (2015)
<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>
<p>Karpathyâ€™s blog post (pre-Transformers) demonstrated that character-level language models trained on simple next-character prediction learn surprisingly rich patterns: Shakespeare-like text, Linux source code structure, even LaTeX formatting. Though RNNs are outdated, the core insight remains: prediction objectives drive models to internalize the structure of their training data. This accessible post builds intuition for why next-token prediction is powerfulâ€”itâ€™s not about the architecture (RNNs vs Transformers), itâ€™s about the objective forcing compression of statistical regularities. Reading this provides intuition for what language models learn and why prediction creates â€œunderstanding.â€</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part5" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>â† Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Part V: Language Models</span> </a> <a href="/eng-ai/part5/22-pretraining" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next â†’</span> <span class="nav-title" data-astro-cid-pux6a34n>Pretraining</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#the-engine-of-language-models-ch21" data-astro-cid-xvrfupwn>The Engine of Language Models</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#language-as-probability-ch21" data-astro-cid-xvrfupwn>Language as Probability</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#tokens-the-atoms-of-language-ch21" data-astro-cid-xvrfupwn>Tokens: The Atoms of Language</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#the-training-objective-predicting-what-comes-next-ch21" data-astro-cid-xvrfupwn>The Training Objective: Predicting What Comes Next</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-prediction-creates-understanding-ch21" data-astro-cid-xvrfupwn>Why Prediction Creates Understanding</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#sampling-and-temperature-controlling-randomness-ch21" data-astro-cid-xvrfupwn>Sampling and Temperature: Controlling Randomness</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch21" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch21" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>