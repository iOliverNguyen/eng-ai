<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 26: Prompting as Programming | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part6/26-prompting-as-programming/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part6/26-prompting-as-programming/"><meta property="og:title" content="Chapter 26: Prompting as Programming | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part6/26-prompting-as-programming/"><meta name="twitter:title" content="Chapter 26: Prompting as Programming | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part6" data-astro-cid-ilhxcym7>Part VI: Modern AI Systems</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Prompting as Programming</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-26-prompting-as-programming">Chapter 26: Prompting as Programming</h1>
<h2 id="why-text-is-now-an-api-ch26">Why Text Is Now an API</h2>
<p>Language models don‚Äôt execute code in the traditional sense‚Äîthey generate text. But text is now a programming interface. A carefully constructed prompt can make a model translate languages, debug code, analyze data, write essays, or answer complex questions. The prompt is both the program and the user interface: natural language that specifies computation.</p>
<p>This is a paradigm shift. Traditional programming requires precise syntax, explicit control flow, and formal specifications. Prompt engineering uses natural language to describe desired behavior, relying on the model‚Äôs learned patterns to execute the task. The model interprets intent from text, maps it to internal representations learned during training, and generates appropriate output.</p>
<p>This chapter explains how prompting works, why wording matters, and how to design prompts as engineered artifacts. Understanding prompting is essential for building modern AI systems‚Äîit‚Äôs the primary control mechanism for language models in production.</p>
<h2 id="prompts-as-instructions-steering-probability-distributions-ch26">Prompts as Instructions: Steering Probability Distributions</h2>
<p>A prompt is text that precedes generation. The model uses it as context to predict what comes next. From the model‚Äôs perspective, a prompt is just the beginning of a sequence‚Äîit continues the pattern established by the input.</p>
<p><strong>Example</strong>:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Prompt: "Translate to French: Hello"</span></span>
<span class="line"><span>Model sees: "Translate to French: Hello"</span></span>
<span class="line"><span>Model predicts next token(s): " Bonjour"</span></span>
<span class="line"><span></span></span></code></pre>
<p>The model learned during training that text matching the pattern ‚ÄúTranslate to [language]: [text]‚Äù is often followed by a translation. When the prompt matches this pattern, the model assigns high probability to translations as continuations.</p>
<p>This is how prompts control behavior: by establishing patterns the model recognizes. The prompt shapes the probability distribution over next tokens. A prompt that matches training patterns (questions ‚Üí answers, code ‚Üí explanations, instructions ‚Üí executions) steers generation toward those continuations.</p>
<p><strong>The control mechanism is indirect</strong>. You don‚Äôt specify the exact output‚Äîyou specify context that makes desired outputs probable. The model‚Äôs training data contained countless examples of tasks framed as text patterns. Prompts exploit these patterns: construct input that looks like the beginning of a task, and the model completes it.</p>
<p>This differs fundamentally from traditional programming:</p>
<ul>
<li><strong>Programming</strong>: Explicit algorithm, deterministic execution, precise syntax</li>
<li><strong>Prompting</strong>: Implicit specification, probabilistic generation, flexible natural language</li>
</ul>
<p>Prompting is probabilistic control through learned associations. The better your prompt matches patterns in the training data, the more reliably the model performs the desired task.</p>
<h2 id="context-windows-the-models-working-memory-ch26">Context Windows: The Model‚Äôs Working Memory</h2>
<p>Every language model has a <strong>context window</strong>: the maximum number of tokens it can process as input. This is the model‚Äôs working memory‚Äîeverything that fits in the context window is available for the next prediction. Anything outside is invisible.</p>
<p>Context windows vary by model:</p>
<ul>
<li>GPT-3: 4K tokens (~3,000 words)</li>
<li>GPT-4: 8K-32K tokens (various tiers)</li>
<li>GPT-4 Turbo: 128K tokens (~100,000 words)</li>
<li>Claude 2: 100K tokens</li>
<li>Claude 3: 200K tokens</li>
</ul>
<p>These limits are architectural. The Transformer‚Äôs self-attention mechanism (Chapter 20) computes attention over all positions, requiring <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> memory for sequence length <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>. Longer contexts cost more memory and compute. Extensions (sparse attention, flash attention) push limits, but context windows remain finite.</p>
<p><img  src="/eng-ai/_astro/26-diagram-1.DfrYKdIA_ZXj57o.svg" alt="Context Windows: The Model&#38;#x27;s Working Memory diagram" width="600" height="280" loading="lazy" decoding="async"></p>
<p>The context window forces trade-offs:</p>
<ul>
<li><strong>System prompts</strong>: Instructions persistent across conversation (e.g., ‚ÄúYou are an expert programmer‚Äù). These consume tokens but shape all responses.</li>
<li><strong>Conversation history</strong>: Past messages provide context but accumulate tokens. Long conversations eventually fill the window.</li>
<li><strong>Retrieved content</strong>: RAG (Chapter 27) injects external documents. More documents mean better grounding but consume more tokens.</li>
<li><strong>Available space</strong>: What remains for the user‚Äôs query and the model‚Äôs response.</li>
</ul>
<p><strong>Managing context is critical</strong>. Exceeding the window truncates earlier content‚Äîthe model ‚Äúforgets‚Äù old messages. Production systems implement strategies:</p>
<ul>
<li><strong>Summarization</strong>: Compress old conversation history into summaries</li>
<li><strong>Selective retention</strong>: Keep important messages (system prompt, recent turns), drop less relevant middle turns</li>
<li><strong>Chunking</strong>: Break long documents into retrievable pieces that fit the window</li>
</ul>
<p>Context window size affects capability. Longer windows enable:</p>
<ul>
<li>Analyzing entire codebases</li>
<li>Processing long documents (reports, contracts, research papers)</li>
<li>Maintaining extended conversations without forgetting</li>
<li>Providing more examples in few-shot prompts</li>
</ul>
<p>But longer windows cost more (compute, memory, latency, API pricing). Engineering prompts means balancing what to include vs. what to omit within token limits.</p>
<h2 id="prompt-patterns-zero-shot-few-shot-chain-of-thought-ch26">Prompt Patterns: Zero-Shot, Few-Shot, Chain-of-Thought</h2>
<p>Effective prompting follows patterns that exploit the model‚Äôs training. These patterns shape how the model interprets the task.</p>
<h3 id="zero-shot-prompting">Zero-Shot Prompting</h3>
<p>Provide only the task description, no examples. Relies on the model‚Äôs pretraining to recognize the task.</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Translate to French: The weather is nice today.</span></span>
<span class="line"><span></span></span></code></pre>
<p>‚Üí ‚ÄúLe temps est beau aujourd‚Äôhui.‚Äù</p>
<p>Zero-shot works for tasks the model saw frequently during training (translation, summarization, basic Q&#x26;A). It‚Äôs token-efficient but less reliable for complex or ambiguous tasks.</p>
<h3 id="few-shot-prompting">Few-Shot Prompting</h3>
<p>Provide examples before the task. The model learns the pattern from demonstrations.</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Translate to French:</span></span>
<span class="line"><span>Hello ‚Üí Bonjour</span></span>
<span class="line"><span>Goodbye ‚Üí Au revoir</span></span>
<span class="line"><span>Thank you ‚Üí Merci</span></span>
<span class="line"><span>The weather is nice today. ‚Üí</span></span>
<span class="line"><span></span></span></code></pre>
<p>‚Üí ‚ÄúLe temps est beau aujourd‚Äôhui.‚Äù</p>
<p>Few-shot prompting is in-context learning (Chapter 25): the model learns from examples in the prompt without parameter updates. More examples generally improve performance but consume tokens. Typical few-shot prompts use 1-5 examples.</p>
<p>The quality of examples matters. Clear, consistent examples teach the pattern effectively. Ambiguous or inconsistent examples confuse the model, degrading performance.</p>
<h3 id="chain-of-thought-cot-prompting">Chain-of-Thought (CoT) Prompting</h3>
<p>For reasoning tasks, include intermediate steps in the prompt.</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Problem: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Let's think step by step:</span></span>
<span class="line"><span>1. Roger starts with 5 tennis balls</span></span>
<span class="line"><span>2. He buys 2 cans</span></span>
<span class="line"><span>3. Each can has 3 balls, so 2 cans have 2 √ó 3 = 6 balls</span></span>
<span class="line"><span>4. Total: 5 + 6 = 11 balls</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Answer: 11</span></span>
<span class="line"><span></span></span></code></pre>
<p>Chain-of-thought unlocks multi-step reasoning by making intermediate steps explicit. The model generates reasoning before the answer, using each generated token as context for the next. This prevents the model from ‚Äúguessing‚Äù the answer directly‚Äîit must show its work.</p>
<p>CoT improves performance on:</p>
<ul>
<li>Math word problems</li>
<li>Logic puzzles</li>
<li>Multi-hop question answering</li>
<li>Complex decision-making</li>
</ul>
<p>The prompt can explicitly instruct chain-of-thought: ‚ÄúLet‚Äôs think step by step‚Äù or ‚ÄúExplain your reasoning before answering.‚Äù</p>
<h3 id="role-prompting">Role Prompting</h3>
<p>Frame the model as an expert or persona to shape behavior.</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>You are an experienced Python developer. Debug the following code:</span></span>
<span class="line"><span></span></span>
<span class="line"><span>def factorial(n):</span></span>
<span class="line"><span>    if n = 1:</span></span>
<span class="line"><span>        return 1</span></span>
<span class="line"><span>    return n * factorial(n-1)</span></span>
<span class="line"><span></span></span></code></pre>
<p>‚Üí The model responds as a Python expert, identifying the syntax error (<code>=</code> should be <code>==</code>) and suggesting a fix.</p>
<p>Role prompting works because training data contains text where experts explain concepts, doctors diagnose conditions, teachers answer questions. The model learns associations between roles and appropriate responses.</p>
<p><img  src="/eng-ai/_astro/26-diagram-2.DDsWRhOk_Z1tvvkI.svg" alt="Prompt Patterns: Zero-Shot, Few-Shot, Chain-of-Thought diagram" width="600" height="320" loading="lazy" decoding="async"></p>
<p>The diagram compares prompting strategies: zero-shot (minimal), few-shot (examples), and chain-of-thought (explicit reasoning). Each has trade-offs in tokens, reliability, and capability.</p>
<h2 id="prompt-fragility-why-wording-matters-ch26">Prompt Fragility: Why Wording Matters</h2>
<p>Language models are sensitive to phrasing. Small changes to prompts can produce dramatically different outputs. This <strong>prompt fragility</strong> is a consequence of how models learn: statistical patterns in training data.</p>
<p><strong>Example</strong>:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Prompt 1: "Summarize this article."</span></span>
<span class="line"><span>Output: Concise 2-sentence summary</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Prompt 2: "Write a summary of this article."</span></span>
<span class="line"><span>Output: Verbose 5-paragraph summary</span></span>
<span class="line"><span></span></span></code></pre>
<p>The model interprets ‚Äúsummarize‚Äù and ‚Äúwrite a summary‚Äù differently based on training data distributions. ‚ÄúSummarize‚Äù appears more often in contexts requiring brevity. ‚ÄúWrite a summary‚Äù appears in contexts allowing detail. The model learned these associations and generates accordingly.</p>
<p><strong>Another example</strong>:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Prompt 1: "Is the following review positive or negative? 'The movie was okay.'"</span></span>
<span class="line"><span>Output: "Negative"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Prompt 2: "Classify this review as positive or negative: 'The movie was okay.'"</span></span>
<span class="line"><span>Output: "Neutral/Mixed"</span></span>
<span class="line"><span></span></span></code></pre>
<p>Framing affects interpretation. ‚ÄúIs X or Y?‚Äù suggests a binary choice. ‚ÄúClassify as X or Y‚Äù allows other options. The model‚Äôs behavior changes based on subtle linguistic cues.</p>
<p><strong>Why fragility happens</strong>:</p>
<ol>
<li>
<p><strong>Training data patterns</strong>: The model learned that certain phrasings correlate with certain continuations. Prompt wording determines which patterns activate.</p>
</li>
<li>
<p><strong>Ambiguity</strong>: Natural language is inherently ambiguous. The same request can be phrased countless ways, each with slightly different implications.</p>
</li>
<li>
<p><strong>Probability shaping</strong>: Prompts shift probability distributions. Small changes alter which tokens are likely, cascading through generation.</p>
</li>
<li>
<p><strong>No explicit task understanding</strong>: The model doesn‚Äôt ‚Äúunderstand‚Äù the task‚Äîit pattern-matches and predicts. Different prompts activate different patterns.</p>
</li>
</ol>
<p><strong>Implications for engineering</strong>:</p>
<ul>
<li>Test prompts extensively. A prompt that works on one example may fail on others.</li>
<li>Iterate on wording. Try variations, measure performance, refine.</li>
<li>Document effective prompts. Treat them as code‚Äîversioned, tested, maintained.</li>
<li>Use prompt templates. Standardized formats reduce fragility by constraining variation.</li>
</ul>
<p>Prompt engineering is empirical. There‚Äôs no formula for the perfect prompt‚Äîonly experimentation, testing, and refinement. Successful production systems invest significant effort in prompt optimization.</p>
<h2 id="prompt-engineering-as-software-engineering-ch26">Prompt Engineering as Software Engineering</h2>
<p>Prompts are now engineered artifacts. They specify computation, control behavior, and determine system outcomes. Treating prompts as casual afterthoughts leads to unreliable systems.</p>
<p><strong>Prompts are code</strong>. They should be:</p>
<ul>
<li><strong>Versioned</strong>: Track changes, roll back failures</li>
<li><strong>Tested</strong>: Automated evaluation on test sets</li>
<li><strong>Documented</strong>: Explain intent, edge cases, failure modes</li>
<li><strong>Modular</strong>: Composable templates, reusable components</li>
<li><strong>Reviewed</strong>: Code review for prompts, especially in production</li>
</ul>
<p><strong>Prompt templates</strong> enable reusability:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">TRANSLATION_PROMPT</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#032F62;--shiki-dark:#9ECBFF"> """</span></span>
<span class="line"><span style="color:#032F62;--shiki-dark:#9ECBFF">Translate the following </span><span style="color:#005CC5;--shiki-dark:#79B8FF">{source_lang}</span><span style="color:#032F62;--shiki-dark:#9ECBFF"> text to </span><span style="color:#005CC5;--shiki-dark:#79B8FF">{target_lang}</span><span style="color:#032F62;--shiki-dark:#9ECBFF">:</span></span>
<span class="line"></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">{text}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#032F62;--shiki-dark:#9ECBFF">Translation:"""</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D"># Usage:</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">prompt </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> TRANSLATION_PROMPT</span><span style="color:#24292E;--shiki-dark:#E1E4E8">.format(</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    source_lang</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"English"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    target_lang</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"Spanish"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    text</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"Hello, how are you?"</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p>Templates separate structure (the prompt pattern) from content (variable inputs). This enables testing across many inputs with consistent framing.</p>
<p><strong>Evaluation</strong> is critical. Manually inspecting a few outputs doesn‚Äôt validate a prompt. Production systems require:</p>
<ul>
<li><strong>Automated tests</strong>: Run prompts against test sets, check outputs match expectations</li>
<li><strong>Metrics</strong>: Accuracy, relevance, consistency, safety</li>
<li><strong>A/B testing</strong>: Compare prompt variations on real traffic</li>
<li><strong>Human evaluation</strong>: Sample outputs for quality assessment</li>
</ul>
<p><strong>Failure modes require monitoring</strong>:</p>
<ul>
<li>Prompt injection: User inputs that override instructions</li>
<li>Context overflow: Prompts + inputs exceed context window</li>
<li>Degradation with distribution shift: Prompts optimized on one dataset fail on new data</li>
<li>Safety failures: Prompts that accidentally elicit harmful outputs</li>
</ul>
<p>Production prompts include guardrails:</p>
<ul>
<li>Explicit instructions for edge cases</li>
<li>Safety guidelines (‚ÄúDo not provide medical advice‚Äù)</li>
<li>Output format constraints (‚ÄúReturn only JSON‚Äù)</li>
<li>Fallback behaviors (‚ÄúIf unsure, say ‚ÄòI don‚Äôt know‚Äô‚Äú)</li>
</ul>
<h2 id="engineering-takeaway-ch26">Engineering Takeaway</h2>
<p>Prompting has become the primary interface for controlling language models. Understanding how prompts work‚Äîand how to engineer them effectively‚Äîis now essential for building AI systems.</p>
<p><strong>Prompts are the new code‚Äîtreat them as versioned, tested artifacts</strong></p>
<p>Prompts determine system behavior as much as traditional code. A poorly designed prompt causes failures just like a bug in code. Production systems maintain prompt libraries: versioned, tested, documented collections of prompts for different tasks. Changes go through review, testing, and staged rollout‚Äîjust like code deployments.</p>
<p><strong>Context window is working memory‚Äîdesign prompts to fit essential information</strong></p>
<p>Token limits force prioritization. Include what‚Äôs necessary (system instructions, relevant examples, user query), omit what‚Äôs not. For conversations, summarize or drop old turns. For RAG, retrieve only the most relevant documents. Monitor token usage in production‚Äîhitting limits degrades performance.</p>
<p><strong>Few-shot learning reduces need for fine-tuning but costs tokens</strong></p>
<p>Providing examples in prompts teaches tasks without training. This is cheaper and faster than fine-tuning but uses tokens every inference. The trade-off: few-shot is flexible (change behavior by changing examples) but expensive at scale. Fine-tuning is rigid (requires retraining for changes) but efficient at inference. Choose based on task frequency and update cadence.</p>
<p><strong>Chain-of-thought unlocks reasoning on complex tasks</strong></p>
<p>For tasks requiring multiple steps (math, logic, analysis), instruct the model to show reasoning. ‚ÄúLet‚Äôs think step by step‚Äù or ‚ÄúExplain your reasoning‚Äù significantly improve accuracy on complex problems. CoT is now standard for reasoning-heavy applications (customer support, technical analysis, decision-making).</p>
<p><strong>Prompt templates enable reusability across use cases</strong></p>
<p>Hardcoding prompts is brittle. Templates with placeholders enable consistent behavior across inputs. Build template libraries for common patterns (translation, summarization, Q&#x26;A, code generation). Test templates thoroughly before deploying to production.</p>
<p><strong>Testing prompts is essential‚Äîsubtle changes break behavior</strong></p>
<p>Prompt fragility means testing is not optional. Build test suites: inputs ‚Üí expected outputs. Run tests when prompts change. Track performance metrics over time. A prompt that works initially may degrade as model versions change or data distributions shift. Continuous testing catches regressions.</p>
<p><strong>Why prompt engineering is now a core skill for AI applications</strong></p>
<p>Every AI application built on language models requires prompt engineering. It‚Äôs not a temporary workaround‚Äîit‚Äôs the fundamental control mechanism. Engineers building AI systems must understand prompt patterns, context management, and evaluation. Prompt engineering is software engineering applied to natural language interfaces. Organizations hiring for AI roles now list ‚Äúprompt engineering‚Äù as a required skill alongside traditional software development.</p>
<p>The lesson: Prompts are how we program language models. They‚Äôre text, but they function as code‚Äîspecifying computation, controlling behavior, determining outcomes. Effective prompting requires understanding how models interpret text, testing rigorously, and treating prompts as engineered artifacts. Production AI systems succeed or fail based on prompt quality. Mastering prompting is now essential for building reliable, capable AI applications.</p>
<hr>
<h2 id="references-and-further-reading-ch26">References and Further Reading</h2>
<p><strong>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</strong> ‚Äì Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. (2022)
<a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a></p>
<p>Wei et al. demonstrated that prompting models to generate intermediate reasoning steps dramatically improves performance on complex tasks. Chain-of-thought prompting works by making the model‚Äôs reasoning explicit, allowing each step to inform subsequent predictions. The paper showed this simple technique (adding ‚ÄúLet‚Äôs think step by step‚Äù) unlocks reasoning abilities in large models that were absent in smaller models, making it an emergent capability tied to scale. This work established CoT as a standard technique for reasoning-heavy applications. Understanding chain-of-thought is essential for building AI systems that handle complex problem-solving, multi-step analysis, and decision-making tasks.</p>
<p><strong>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</strong> ‚Äì Pengfei Liu, Weizhe Yuan, Jinlan Fu, et al. (2023)
<a href="https://arxiv.org/abs/2107.13586">https://arxiv.org/abs/2107.13586</a></p>
<p>Liu et al. provide a comprehensive survey of prompting techniques, taxonomies, and best practices. The paper systematically categorizes prompt engineering approaches: few-shot vs. zero-shot, discrete vs. continuous prompts, manually designed vs. automatically generated. It explains why prompting works (exploiting patterns learned during pretraining), when it fails (distribution mismatch, ambiguity), and how to improve it (prompt tuning, calibration). This survey is the definitive reference for understanding the landscape of prompting methods. Reading it clarifies the principles underlying effective prompts and provides a framework for designing robust prompting strategies in production systems.</p>
<p><strong>The Prompt Report: A Systematic Survey of Prompting Techniques</strong> ‚Äì Sander Schulhoff, Michael Ilie, Nishant Balepur, et al. (2024)
<a href="https://arxiv.org/abs/2406.06608">https://arxiv.org/abs/2406.06608</a></p>
<p>This recent comprehensive survey catalogs prompting techniques used in practice, including role prompting, instruction following, meta-prompting, and prompt optimization methods. Schulhoff et al. synthesize research and industry practices, providing actionable guidance for practitioners. The report covers prompt fragility, evaluation strategies, and production considerations (safety, monitoring, versioning). It bridges academic research and industry deployment, making it essential reading for engineers building real AI systems. Understanding the techniques documented here enables building more reliable, robust prompt-based applications.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part6" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Part VI: AI Systems</span> </a> <a href="/eng-ai/part6/27-retrieval-augmented-generation" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Retrieval-Augmented Generation</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-text-is-now-an-api-ch26" data-astro-cid-xvrfupwn>Why Text Is Now an API</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#prompts-as-instructions-steering-probability-distributions-ch26" data-astro-cid-xvrfupwn>Prompts as Instructions: Steering Probability Distributions</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#context-windows-the-models-working-memory-ch26" data-astro-cid-xvrfupwn>Context Windows: The Model‚Äôs Working Memory</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#prompt-patterns-zero-shot-few-shot-chain-of-thought-ch26" data-astro-cid-xvrfupwn>Prompt Patterns: Zero-Shot, Few-Shot, Chain-of-Thought</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#zero-shot-prompting" data-astro-cid-xvrfupwn>Zero-Shot Prompting</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#few-shot-prompting" data-astro-cid-xvrfupwn>Few-Shot Prompting</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#chain-of-thought-cot-prompting" data-astro-cid-xvrfupwn>Chain-of-Thought (CoT) Prompting</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#role-prompting" data-astro-cid-xvrfupwn>Role Prompting</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#prompt-fragility-why-wording-matters-ch26" data-astro-cid-xvrfupwn>Prompt Fragility: Why Wording Matters</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#prompt-engineering-as-software-engineering-ch26" data-astro-cid-xvrfupwn>Prompt Engineering as Software Engineering</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch26" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch26" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>