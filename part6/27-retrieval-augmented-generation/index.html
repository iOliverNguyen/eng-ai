<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 27: Retrieval-Augmented Generation | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part6/27-retrieval-augmented-generation/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part6/27-retrieval-augmented-generation/"><meta property="og:title" content="Chapter 27: Retrieval-Augmented Generation | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part6/27-retrieval-augmented-generation/"><meta name="twitter:title" content="Chapter 27: Retrieval-Augmented Generation | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part6" data-astro-cid-ilhxcym7>Part VI: Modern AI Systems</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Retrieval-Augmented Generation</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-27-retrieval-augmented-generation">Chapter 27: Retrieval-Augmented Generation</h1>
<h2 id="why-models-need-search-ch27">Why Models Need Search</h2>
<p>Language models learn from training data, but that knowledge is frozen at training time. A model trained in 2023 doesn‚Äôt know what happened in 2024. It can‚Äôt access your company‚Äôs internal documents. It can‚Äôt retrieve current stock prices, recent news, or updated regulations. And when asked about unfamiliar topics, models don‚Äôt say ‚ÄúI don‚Äôt know‚Äù‚Äîthey hallucinate plausible-sounding answers.</p>
<p><strong>Retrieval-Augmented Generation (RAG)</strong> solves these problems by augmenting generation with retrieval. Instead of relying solely on the model‚Äôs internalized knowledge, RAG systems:</p>
<ol>
<li>Retrieve relevant information from external sources</li>
<li>Inject retrieved content into the prompt as context</li>
<li>Generate responses grounded in retrieved facts</li>
</ol>
<p>RAG transforms language models from closed systems (limited to training data) to open systems (accessing external knowledge dynamically). This chapter explains why RAG is necessary, how it works, and how to build production RAG systems.</p>
<h2 id="why-llms-forget-training-vs-runtime-knowledge-ch27">Why LLMs Forget: Training vs Runtime Knowledge</h2>
<p>Language models compress training data into parameters during pretraining (Chapter 22). This compression creates a <strong>knowledge cutoff</strong>: the model knows about the world as it existed in the training data, nothing more.</p>
<p><strong>Example failure</strong>:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>User: "Who won the 2024 Olympics men's 100m sprint?"</span></span>
<span class="line"><span>Model (trained on 2023 data): "I don't have information about the 2024 Olympics yet,</span></span>
<span class="line"><span>as my training data only goes up to 2023. However, the 2020 Olympics 100m was won by</span></span>
<span class="line"><span>Marcell Jacobs of Italy..."</span></span>
<span class="line"><span></span></span></code></pre>
<p>The model can‚Äôt know recent events. Its knowledge froze when training ended. For dynamic information (news, stock prices, sports results, product catalogs), this is a fatal limitation.</p>
<p><strong>The hallucination problem</strong>: Models don‚Äôt distinguish known facts from plausible guesses. When uncertain, they generate text that sounds confident but may be false. This is not malice‚Äîit‚Äôs the model‚Äôs optimization objective (Chapter 21): predict plausible next tokens based on learned patterns.</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>User: "What are the health benefits of the fictional herb 'xylophene'?"</span></span>
<span class="line"><span>Model: "Xylophene has been shown to reduce inflammation and improve cognitive function.</span></span>
<span class="line"><span>Studies suggest it may also support cardiovascular health. However, consult a doctor</span></span>
<span class="line"><span>before use..."</span></span>
<span class="line"><span></span></span></code></pre>
<p>The model invented facts about a nonexistent herb because the prompt matched patterns in training data (herb names ‚Üí health benefits). Without external grounding, the model generates plausibly structured fabrications.</p>
<p><strong>Proprietary knowledge</strong>: Models train on public internet data. They don‚Äôt know your company‚Äôs internal documents, customer records, proprietary research, or confidential information. For enterprise applications, this makes bare language models unusable‚Äîthey can‚Äôt answer questions about organization-specific knowledge.</p>
<p>RAG addresses all three limitations:</p>
<ul>
<li><strong>Knowledge cutoff</strong>: Retrieve current information at runtime</li>
<li><strong>Hallucination</strong>: Ground generation in retrieved facts</li>
<li><strong>Proprietary knowledge</strong>: Retrieve from private document stores</li>
</ul>
<h2 id="vector-databases-storing-knowledge-for-retrieval-ch27">Vector Databases: Storing Knowledge for Retrieval</h2>
<p>RAG requires storing documents in a format enabling fast semantic search. Traditional databases support exact matching (SQL: <code>WHERE title = "Annual Report"</code>) or keyword search (full-text search). But semantic search requires finding documents <em>similar in meaning</em> to a query, not just lexically similar.</p>
<p><strong>Vector databases</strong> solve this by storing documents as high-dimensional vectors (embeddings, Chapter 18). Documents semantically similar to the query have vectors close in embedding space, enabling fast similarity search.</p>
<p><strong>The process</strong>:</p>
<ol>
<li><strong>Document encoding</strong>: Split documents into chunks, embed each chunk into a vector</li>
<li><strong>Storage</strong>: Store vectors in a database optimized for similarity search</li>
<li><strong>Query encoding</strong>: Embed the user‚Äôs query into the same vector space</li>
<li><strong>Similarity search</strong>: Find the k most similar document vectors to the query vector</li>
<li><strong>Retrieval</strong>: Return the documents corresponding to the closest vectors</li>
</ol>
<p><strong>Embedding similarity</strong> is typically measured by cosine similarity (same formula from Chapter 18):</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>similarity</mtext><mo stretchy="false">(</mo><mi mathvariant="bold">q</mi><mo separator="true">,</mo><mi mathvariant="bold">d</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold">q</mi><mo>‚ãÖ</mo><mi mathvariant="bold">d</mi></mrow><mrow><mi mathvariant="normal">‚à£</mi><mi mathvariant="bold">q</mi><mi mathvariant="normal">‚à£</mi><mi mathvariant="normal">‚à£</mi><mi mathvariant="bold">d</mi><mi mathvariant="normal">‚à£</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{similarity}(\mathbf{q}, \mathbf{d}) = \frac{\mathbf{q} \cdot \mathbf{d}}{|\mathbf{q}| |\mathbf{d}|}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">similarity</span></span><span class="mopen">(</span><span class="mord mathbf">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3074em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">‚à£</span><span class="mord mathbf">q</span><span class="mord">‚à£‚à£</span><span class="mord mathbf">d</span><span class="mord">‚à£</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathbf">q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚ãÖ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathbf">d</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">q</mi></mrow><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord mathbf">q</span></span></span></span> is the query embedding and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">d</mi></mrow><annotation encoding="application/x-tex">\mathbf{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathbf">d</span></span></span></span> is a document embedding. High cosine similarity (close to 1) indicates semantic relevance.</p>
<p><strong>Vector databases</strong> (Pinecone, Weaviate, FAISS, Chroma, Qdrant) specialize in:</p>
<ul>
<li><strong>Approximate nearest neighbor (ANN) search</strong>: Finding similar vectors quickly (exact search is slow for millions of vectors)</li>
<li><strong>Indexing</strong>: Building data structures (HNSW, IVF) that enable sub-linear search time</li>
<li><strong>Scalability</strong>: Handling billions of vectors, distributed across machines</li>
<li><strong>Metadata filtering</strong>: Combining vector similarity with traditional filters (e.g., ‚Äúfind similar documents from 2024 only‚Äù)</li>
</ul>
<p><img  src="/eng-ai/_astro/27-diagram.CPIKT2Sm_CqcAD.svg" alt="Vector Databases: Storing Knowledge for Retrieval diagram" width="600" height="320" loading="lazy" decoding="async"></p>
<p>The diagram shows the RAG pipeline: query embedding ‚Üí vector search ‚Üí retrieved documents ‚Üí LLM generates response using docs as context. This grounds generation in external knowledge.</p>
<p><strong>Chunking strategy</strong> is critical. Documents are too long to embed as single units‚Äîa 100-page report exceeds context windows and makes poor retrieval targets (too coarse). Chunking splits documents into retrievable pieces.</p>
<p>Common strategies:</p>
<ul>
<li><strong>Fixed-size chunks</strong>: Split every N tokens (e.g., 512 tokens). Simple but breaks mid-sentence.</li>
<li><strong>Sentence/paragraph boundaries</strong>: Split at natural boundaries. Preserves meaning but variable size.</li>
<li><strong>Semantic chunking</strong>: Use NLP to identify topic boundaries. Better semantics, more complex.</li>
</ul>
<p>Chunk size trades off granularity vs. context:</p>
<ul>
<li>Small chunks (100-200 tokens): Precise retrieval, but may lack surrounding context</li>
<li>Large chunks (500-1000 tokens): More context, but less precise, consume more of the context window</li>
</ul>
<p>Production systems often use 300-500 token chunks with overlap (e.g., 50-token overlap between consecutive chunks to preserve continuity).</p>
<h2 id="retrieval-strategies-dense-sparse-hybrid-ch27">Retrieval Strategies: Dense, Sparse, Hybrid</h2>
<p>Vector search (dense retrieval) is powerful but not perfect. <strong>Hybrid retrieval</strong> combines multiple strategies for better performance.</p>
<h3 id="dense-retrieval-embedding-based">Dense Retrieval (Embedding-Based)</h3>
<p>Documents and queries are embedded into a learned vector space. Retrieval uses cosine similarity in that space. This captures semantic meaning‚Äîsynonyms, paraphrases, and conceptual similarity.</p>
<p>Advantages:</p>
<ul>
<li>Semantic matching: ‚ÄúWhat are ML techniques?‚Äù retrieves documents about ‚Äúmachine learning methods‚Äù</li>
<li>Multilingual: Cross-lingual embeddings enable retrieval across languages</li>
<li>Robust to paraphrasing: Different wording, same meaning ‚Üí similar embeddings</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Misses exact matches: Rare entity names or technical terms may not embed well</li>
<li>Computationally expensive: Embedding inference + vector search costs time and resources</li>
</ul>
<h3 id="sparse-retrieval-keyword-based">Sparse Retrieval (Keyword-Based)</h3>
<p>Traditional information retrieval using term frequencies. BM25 is the standard algorithm: ranks documents by how well query keywords match document terms, weighted by term importance.</p>
<p>Advantages:</p>
<ul>
<li>Exact match: Finds documents containing specific entity names, IDs, rare terms</li>
<li>Fast: No embedding inference, just keyword matching</li>
<li>Interpretable: Clear why a document was retrieved (contains query terms)</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Lexical mismatch: Synonyms don‚Äôt match (‚Äúcar‚Äù doesn‚Äôt retrieve ‚Äúautomobile‚Äù)</li>
<li>No semantic understanding: Can‚Äôt handle paraphrases or conceptual queries</li>
</ul>
<h3 id="hybrid-retrieval">Hybrid Retrieval</h3>
<p>Combine dense and sparse retrieval, merging results. Typical approach:</p>
<ol>
<li>Retrieve top-k documents with dense retrieval (semantic matching)</li>
<li>Retrieve top-k documents with sparse retrieval (keyword matching)</li>
<li>Rerank the union using a reranking model (cross-encoder scoring query-document pairs)</li>
</ol>
<p>Hybrid retrieval gets the best of both: semantic understanding from dense retrieval, exact matching from sparse retrieval. Production RAG systems overwhelmingly use hybrid approaches.</p>
<h2 id="grounding-preventing-hallucinations-with-citations-ch27">Grounding: Preventing Hallucinations with Citations</h2>
<p>RAG reduces hallucinations by grounding generation in retrieved documents. But two engineering practices are essential:</p>
<h3 id="instruction-to-use-retrieved-context">Instruction to Use Retrieved Context</h3>
<p>The prompt must explicitly instruct the model to use retrieved documents:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Context:</span></span>
<span class="line"><span>[Retrieved document 1]</span></span>
<span class="line"><span>[Retrieved document 2]</span></span>
<span class="line"><span>...</span></span>
<span class="line"><span></span></span>
<span class="line"><span>User question: {query}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Instructions: Answer the question using only information from the provided context.</span></span>
<span class="line"><span>If the context doesn't contain relevant information, say "I don't have enough</span></span>
<span class="line"><span>information to answer that question."</span></span>
<span class="line"><span></span></span></code></pre>
<p>Without explicit instructions, the model may ignore retrieved context and generate from its parametric knowledge (the hallucination risk remains).</p>
<h3 id="citations">Citations</h3>
<p>Include sources in the response. When the model quotes or paraphrases retrieved content, cite the source:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Response: "The 2024 Olympics men's 100m was won by Noah Lyles with a time of 9.79 seconds.</span></span>
<span class="line"><span>[Source: Olympic Results 2024, Retrieved Aug 10, 2024]"</span></span>
<span class="line"><span></span></span></code></pre>
<p>Citations enable verification: users can check the source to confirm the model‚Äôs claim. This builds trust and catches errors (if the citation doesn‚Äôt support the claim, the user knows to question the output).</p>
<p>Production systems often structure citations as structured metadata:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="json"><code><span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">{</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "response"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"Noah Lyles won the 100m sprint..."</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "sources"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: [</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">    {</span><span style="color:#005CC5;--shiki-dark:#79B8FF">"title"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"Olympic Results 2024"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, </span><span style="color:#005CC5;--shiki-dark:#79B8FF">"url"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"https://..."</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, </span><span style="color:#005CC5;--shiki-dark:#79B8FF">"relevance"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#005CC5;--shiki-dark:#79B8FF">0.94</span><span style="color:#24292E;--shiki-dark:#E1E4E8">}</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">  ]</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">}</span></span>
<span class="line"></span></code></pre>
<h2 id="engineering-takeaway-ch27">Engineering Takeaway</h2>
<p>RAG has become the standard approach for knowledge-intensive applications. Understanding how to build and deploy RAG systems is essential for production AI engineering.</p>
<p><strong>RAG provides fresh knowledge without retraining</strong></p>
<p>Updating model knowledge through retraining costs millions of dollars and weeks of compute. RAG enables knowledge updates by updating the document store‚Äîadd new documents, remove outdated ones. The model remains frozen; knowledge stays current. For applications requiring up-to-date information (news, legal, medical, support), RAG is the only practical approach.</p>
<p><strong>Vector databases enable semantic search at scale</strong></p>
<p>Production RAG systems handle millions of documents. Vector databases index embeddings for sub-linear search time (HNSW, IVF indices). Without specialized databases, similarity search is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>‚Äîintractable at scale. Choose vector databases based on scale (millions vs. billions of documents), latency requirements (real-time vs. batch), and infrastructure (cloud vs. self-hosted). FAISS (Facebook AI) is popular for self-hosted, Pinecone/Weaviate for managed cloud services.</p>
<p><strong>Chunking strategy affects retrieval quality</strong></p>
<p>Too small: Chunks lack context, retrieval misses relevant information because it‚Äôs split across chunks. Too large: Chunks are noisy, contain irrelevant content alongside relevant content, consume context window. The optimal chunk size depends on domain and query types. Test empirically: measure retrieval precision/recall at different chunk sizes. For most applications, 300-500 tokens with 10-20% overlap works well.</p>
<p><strong>Hybrid retrieval outperforms either dense or sparse alone</strong></p>
<p>Dense retrieval excels at semantic queries but misses exact matches. Sparse retrieval excels at specific entities but misses paraphrases. Combining both improves retrieval quality by 10-30% in benchmarks. Production systems use hybrid retrieval by default. The additional complexity (two retrieval passes, result merging) is justified by quality gains.</p>
<p><strong>Citations and grounding reduce hallucinations and build trust</strong></p>
<p>Grounding responses in retrieved documents reduces fabrications but doesn‚Äôt eliminate them‚Äîmodels can still misinterpret or misquote sources. Citations enable verification: users can check whether the response accurately reflects the source. This is critical for high-stakes applications (legal, medical, financial). Structure citations as metadata (title, URL, relevance score) rather than inline text to enable programmatic verification.</p>
<p><strong>RAG beats fine-tuning for knowledge-intensive tasks</strong></p>
<p>Fine-tuning encodes knowledge into model parameters. This works for stable knowledge (grammar, reasoning patterns) but fails for dynamic knowledge (news, product catalogs, customer records). Fine-tuning also risks catastrophic forgetting (Chapter 23). RAG separates knowledge (in the document store) from generation (in the model), enabling independent updates. For knowledge-intensive tasks, RAG is cheaper (no retraining), more flexible (update documents easily), and more accurate (grounds responses in facts).</p>
<p><strong>Why production RAG requires careful engineering</strong></p>
<p>RAG adds complexity: embedding models, vector databases, retrieval strategies, reranking, prompt engineering. Each component can fail. Production RAG systems require:</p>
<ul>
<li><strong>Query rewriting</strong>: Reformulate user queries for better retrieval (expand acronyms, add context)</li>
<li><strong>Reranking</strong>: Score query-document pairs with cross-encoders for precision</li>
<li><strong>Context window management</strong>: Retrieved documents must fit within token limits</li>
<li><strong>Monitoring</strong>: Track retrieval quality (precision, recall), generation quality (accuracy, coherence)</li>
<li><strong>Failover</strong>: Handle retrieval failures gracefully (fall back to model‚Äôs knowledge, warn users)</li>
<li><strong>Security</strong>: Prevent prompt injection via retrieved documents (sanitize content)</li>
</ul>
<p>Building RAG systems is now standard in AI engineering. The pattern‚Äîretrieve, inject context, generate‚Äîapplies across domains: customer support (retrieve past tickets), legal analysis (retrieve case law), medical diagnosis (retrieve research papers), code generation (retrieve documentation). Mastering RAG is essential for production AI applications.</p>
<hr>
<h2 id="references-and-further-reading-ch27">References and Further Reading</h2>
<p><strong>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</strong> ‚Äì Patrick Lewis, Ethan Perez, Aleksandra Piktus, et al. (2020)
<a href="https://arxiv.org/abs/2005.11401">https://arxiv.org/abs/2005.11401</a></p>
<p>Lewis et al. introduced RAG, showing that augmenting language models with retrieval dramatically improves performance on knowledge-intensive tasks. They demonstrated that a smaller model with retrieval outperforms a much larger model without retrieval on open-domain question answering. The paper established the RAG paradigm: separate parametric knowledge (in the model) from non-parametric knowledge (in the document store). This architecture enables updating knowledge without retraining and reduces hallucinations by grounding generation in facts. RAG is now the standard approach for applications requiring accurate, up-to-date knowledge.</p>
<p><strong>Retrieval-Augmented Generation for Large Language Models: A Survey</strong> ‚Äì Yunfan Gao, Yun Xiong, Xinyu Gao, et al. (2023)
<a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a></p>
<p>Gao et al. provide a comprehensive survey of RAG techniques, covering retrieval strategies (dense, sparse, hybrid), indexing methods (vector databases, HNSW, IVF), reranking approaches, and evaluation metrics. The paper synthesizes research and industry practices, offering practical guidance for building production RAG systems. It discusses failure modes (retrieval errors, context window limits, hallucinations despite grounding) and mitigation strategies. This survey is essential reading for engineers deploying RAG in production, providing a roadmap of techniques and trade-offs.</p>
<p><strong>Dense Passage Retrieval for Open-Domain Question Answering</strong> ‚Äì Vladimir Karpukhin, Barlas Oƒüuz, Sewon Min, et al. (2020)
<a href="https://arxiv.org/abs/2004.04906">https://arxiv.org/abs/2004.04906</a></p>
<p>Karpukhin et al. demonstrated that dense retrieval (embedding-based) outperforms traditional sparse retrieval (BM25) for question answering. They showed that training retrieval models end-to-end with question-answer pairs produces embeddings optimized for semantic matching. This work established dense retrieval as the foundation for modern RAG systems. The paper also introduced techniques for scaling retrieval to millions of documents using FAISS, enabling practical deployment. Understanding dense retrieval is fundamental to building effective RAG systems that capture semantic similarity rather than just lexical overlap.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part6/26-prompting-as-programming" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Prompting as Programming</span> </a> <a href="/eng-ai/part6/28-tools-and-function-calling" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Tools and Function Calling</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-models-need-search-ch27" data-astro-cid-xvrfupwn>Why Models Need Search</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-llms-forget-training-vs-runtime-knowledge-ch27" data-astro-cid-xvrfupwn>Why LLMs Forget: Training vs Runtime Knowledge</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#vector-databases-storing-knowledge-for-retrieval-ch27" data-astro-cid-xvrfupwn>Vector Databases: Storing Knowledge for Retrieval</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#retrieval-strategies-dense-sparse-hybrid-ch27" data-astro-cid-xvrfupwn>Retrieval Strategies: Dense, Sparse, Hybrid</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#dense-retrieval-embedding-based" data-astro-cid-xvrfupwn>Dense Retrieval (Embedding-Based)</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#sparse-retrieval-keyword-based" data-astro-cid-xvrfupwn>Sparse Retrieval (Keyword-Based)</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#hybrid-retrieval" data-astro-cid-xvrfupwn>Hybrid Retrieval</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#grounding-preventing-hallucinations-with-citations-ch27" data-astro-cid-xvrfupwn>Grounding: Preventing Hallucinations with Citations</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#instruction-to-use-retrieved-context" data-astro-cid-xvrfupwn>Instruction to Use Retrieved Context</a></li><li class="toc-level-3" data-astro-cid-xvrfupwn><a href="#citations" data-astro-cid-xvrfupwn>Citations</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch27" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch27" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>