<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 30: Memory, Planning, and Long-Term Behavior | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part6/30-memory-and-planning/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part6/30-memory-and-planning/"><meta property="og:title" content="Chapter 30: Memory, Planning, and Long-Term Behavior | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part6/30-memory-and-planning/"><meta name="twitter:title" content="Chapter 30: Memory, Planning, and Long-Term Behavior | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part6" data-astro-cid-ilhxcym7>Part VI: Modern AI Systems</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Memory, Planning, and Long-Term Behavior</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-30-memory-planning-and-long-term-behavior">Chapter 30: Memory, Planning, and Long-Term Behavior</h1>
<p>A language model responds to each prompt as if encountering it for the first time. It has no memory of previous conversations unless they are explicitly included in the current context window. This makes the model stateless: every interaction is independent, every session starts fresh.</p>
<p>But production AI systems need memory. A personal assistant should remember your preferences. A customer service bot should recall past interactions. A code completion tool should learn your coding style. Memory transforms a stateless text predictor into a system with continuity, identity, and the ability to improve over time.</p>
<p>This chapter explains how AI systems maintain memory, how memory enables long-term planning, and why persistent state fundamentally changes what these systems can do and how they behave.</p>
<hr>
<h2 id="short-term-vs-long-term-memory-ch30">Short-Term vs. Long-Term Memory</h2>
<p>AI systems use two distinct types of memory, each with different characteristics and purposes.</p>
<p><strong>Short-term memory</strong> is the context window: the tokens currently loaded into the model‚Äôs attention mechanism. This is working memory‚Äîimmediately accessible but limited in size and duration.</p>
<p>Properties of short-term memory:</p>
<ul>
<li><strong>Limited capacity</strong>: 4K to 200K tokens depending on model (roughly 3K to 150K words)</li>
<li><strong>Perfect recall</strong>: Every token in context is instantly accessible during generation</li>
<li><strong>Ephemeral</strong>: Disappears when the conversation ends or context window fills</li>
<li><strong>Expensive</strong>: Longer context means slower inference and higher cost</li>
</ul>
<p>The context window functions like human working memory: you can hold a few things in mind at once, process them with full attention, but can‚Äôt remember everything you‚Äôve ever experienced this way.</p>
<p><strong>Long-term memory</strong> is external storage: databases, vector stores, file systems that persist information across sessions and beyond context window limits.</p>
<p>Properties of long-term memory:</p>
<ul>
<li><strong>Unbounded capacity</strong>: Can store millions of interactions, documents, facts</li>
<li><strong>Selective retrieval</strong>: Must explicitly search for and load relevant memories</li>
<li><strong>Persistent</strong>: Survives across sessions, devices, model updates</li>
<li><strong>Cheaper at scale</strong>: Storage costs less than keeping everything in context</li>
</ul>
<p>Long-term memory functions like human episodic and semantic memory: you can‚Äôt instantly access everything you‚Äôve ever learned, but you can search your memory for relevant information when needed.</p>
<p><strong>The memory hierarchy in AI systems:</strong></p>
<p><img  src="/eng-ai/_astro/30-diagram-1.j9NG_U01_ZUNl7q.svg" alt="Short-Term vs. Long-Term Memory diagram" width="800" height="500" loading="lazy" decoding="async"></p>
<p><strong>Figure 30.1</strong>: Memory architecture in AI systems. Short-term memory (context window) provides fast, perfect recall but limited capacity. Working memory maintains current state and goals for the session. Long-term memory persists across sessions using vector databases and structured storage, requiring explicit retrieval but offering unbounded capacity.</p>
<p>The key engineering challenge is <strong>memory management</strong>: deciding what to keep in expensive short-term memory, what to offload to long-term storage, and when to retrieve past information.</p>
<hr>
<h2 id="memory-types-episodic-and-semantic-ch30">Memory Types: Episodic and Semantic</h2>
<p>Long-term memory systems distinguish between two types of memory, borrowed from cognitive science:</p>
<p><strong>Episodic memory</strong> stores specific events and experiences: ‚ÄúWhat happened when?‚Äù</p>
<p>Examples:</p>
<ul>
<li>‚ÄúUser asked about vacation policy on March 15‚Äù</li>
<li>‚ÄúFlight booking failed due to payment error at 3:42pm‚Äù</li>
<li>‚ÄúUser prefers morning meetings, dislikes Zoom‚Äù</li>
</ul>
<p>Episodic memories are time-stamped, context-specific, and personal. They answer questions like ‚ÄúWhat did I tell you about my schedule?‚Äù or ‚ÄúWhat happened last time I tried this?‚Äù</p>
<p><strong>Semantic memory</strong> stores general knowledge and patterns: ‚ÄúWhat is true in general?‚Äù</p>
<p>Examples:</p>
<ul>
<li>‚ÄúUser works in software engineering‚Äù</li>
<li>‚ÄúPython uses indentation for blocks‚Äù</li>
<li>‚ÄúEmails should have subject lines‚Äù</li>
</ul>
<p>Semantic memories are timeless facts and rules. They answer questions like ‚ÄúWhat do you know about me?‚Äù or ‚ÄúHow does this work?‚Äù</p>
<p>In practice, AI systems blur these categories. A memory might be ‚ÄúUser prefers ‚Äòasync/await‚Äô over Promises in JavaScript‚Äù (semantic: a general preference) but was learned from ‚ÄúUser rewrote three functions to use async/await on Oct 10‚Äù (episodic: specific events).</p>
<p><strong>Memory storage formats:</strong></p>
<p><strong>Conversation logs</strong> (episodic): Store full transcripts of past interactions.</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="json"><code><span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">{</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "timestamp"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"2024-03-15T10:30:00Z"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "user"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"What's the vacation policy?"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "assistant"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"You have 15 days per year, accruing at 1.25 days per month..."</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "context"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: [</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"discussing benefits"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"onboarding"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">]</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">}</span></span>
<span class="line"></span></code></pre>
<p><strong>Fact extraction</strong> (semantic): Parse conversations into structured facts.</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="json"><code><span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">{</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "type"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"user_preference"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "fact"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"prefers morning meetings"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "confidence"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#005CC5;--shiki-dark:#79B8FF">0.9</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "source"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"conversation on 2024-03-10"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">  "category"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"scheduling"</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">}</span></span>
<span class="line"></span></code></pre>
<p><strong>Embedding-based</strong> (both): Store memories as vectors for semantic search.</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>User said: "I hate long emails"</span></span>
<span class="line"><span>‚Üí Embedded as vector</span></span>
<span class="line"><span>‚Üí Later query: "How should I format this email?"</span></span>
<span class="line"><span>‚Üí Retrieve relevant memory: "User prefers brief emails"</span></span>
<span class="line"><span></span></span></code></pre>
<p>Production systems often combine all three: conversation logs for exact recall, facts for structured queries, embeddings for semantic retrieval.</p>
<hr>
<h2 id="memory-retrieval-finding-relevant-context-ch30">Memory Retrieval: Finding Relevant Context</h2>
<p>The challenge of long-term memory is <strong>retrieval</strong>: given a current query or task, which past memories are relevant?</p>
<p>Unlike short-term memory (where everything in context is equally accessible), long-term memory requires <strong>search</strong>. The system must decide what to fetch from storage and load into the context window.</p>
<p><strong>Retrieval strategies:</strong></p>
<p><strong>Recency-based</strong>: Fetch the N most recent memories.</p>
<ul>
<li>Simple, fast, often effective (recent context is often relevant)</li>
<li>Fails when relevant information is old (first meeting, initial preferences)</li>
</ul>
<p><strong>Keyword matching</strong>: Search for memories containing specific words or phrases.</p>
<ul>
<li>Works for explicit references (‚ÄúWhat did I say about Python?‚Äù)</li>
<li>Fails for semantic similarity (query ‚Äúcode style‚Äù won‚Äôt match memory ‚Äúformatting preferences‚Äù)</li>
</ul>
<p><strong>Semantic search</strong>: Embed query and memories, retrieve by cosine similarity.</p>
<ul>
<li>Captures meaning beyond exact words</li>
<li>Used in most production systems (via vector databases, Chapter 27)</li>
<li>Query: ‚ÄúWhat time should we meet?‚Äù retrieves memory ‚ÄúUser prefers 9am meetings‚Äù</li>
</ul>
<p><strong>Structured queries</strong>: Search extracted facts by category or type.</p>
<ul>
<li>‚ÄúGet all user preferences related to scheduling‚Äù</li>
<li>Fast for specific lookups, requires upfront fact extraction</li>
</ul>
<p><strong>Hybrid retrieval</strong>: Combine multiple strategies.</p>
<ul>
<li>Fetch recent memories (recency)</li>
<li>Fetch semantically similar memories (embedding search)</li>
<li>Fetch explicit references (keyword match)</li>
<li>Merge and rank results</li>
</ul>
<p><strong>Retrieval in action:</strong></p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Current conversation:</span></span>
<span class="line"><span>User: "I need to book a flight for next month's conference"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Retrieval process:</span></span>
<span class="line"><span>1. Semantic search: "booking flights" + "conferences"</span></span>
<span class="line"><span>   ‚Üí Finds memory: "User traveled to MLConf 2023 in Boston"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>2. Fact lookup: category = "travel_preferences"</span></span>
<span class="line"><span>   ‚Üí Finds: "User prefers aisle seats", "User has TSA PreCheck"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>3. Recency filter: last 30 days</span></span>
<span class="line"><span>   ‚Üí Finds: "Conference mentioned: NeurIPS 2024, Vancouver, Dec 10-16"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>4. Load into context:</span></span>
<span class="line"><span>   - "User attending NeurIPS in Vancouver Dec 10-16"</span></span>
<span class="line"><span>   - "Travel preferences: aisle seat, TSA PreCheck"</span></span>
<span class="line"><span>   - "Previous conference: MLConf Boston"</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Agent response:</span></span>
<span class="line"><span>"I'll help book your flight to Vancouver for NeurIPS (Dec 10-16). Based on your preferences, I'll look for aisle seats and include your TSA PreCheck number. Should I search for similar dates as your MLConf trip (arrive day before, leave day after)?"</span></span>
<span class="line"><span></span></span></code></pre>
<p>The agent retrieved relevant memories and incorporated them into planning, making the interaction feel continuous and personalized rather than starting from scratch.</p>
<p><strong>Retrieval challenges:</strong></p>
<ul>
<li><strong>Cold start</strong>: New users have no memory to retrieve</li>
<li><strong>Noise</strong>: Irrelevant memories retrieved alongside relevant ones</li>
<li><strong>Staleness</strong>: Old memories may be outdated (user changed preferences)</li>
<li><strong>Privacy</strong>: Retrieving sensitive information requires access control</li>
<li><strong>Cost</strong>: Every retrieval is a database query and embedding computation</li>
</ul>
<hr>
<h2 id="planning-with-memory-learning-from-experience-ch30">Planning with Memory: Learning from Experience</h2>
<p>Memory enables a powerful capability: <strong>learning from past actions</strong> to improve future planning. An agent can recall what worked, what failed, and what it learned, then apply this experience to new situations.</p>
<p><strong>Example: Code debugging agent with memory</strong></p>
<p>First interaction:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>User: This test fails with "undefined is not a function"</span></span>
<span class="line"><span>Agent: Let me check the code...</span></span>
<span class="line"><span>[Debugging process, finds missing import]</span></span>
<span class="line"><span>Agent: The issue was a missing import statement.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Memory stored:</span></span>
<span class="line"><span>"Error pattern: 'undefined is not a function' ‚Üí likely missing import or typo"</span></span>
<span class="line"><span></span></span></code></pre>
<p>Later interaction with different user:</p>
<pre class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>User: Getting "undefined is not a function" in my React component</span></span>
<span class="line"><span>Agent: [Retrieves memory of similar error pattern]</span></span>
<span class="line"><span>Agent: This error typically indicates a missing import. Let me check your imports first...</span></span>
<span class="line"><span>[Quickly identifies problem]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Memory updated:</span></span>
<span class="line"><span>"Error pattern confirmed in React context: check imports for components and hooks"</span></span>
<span class="line"><span></span></span></code></pre>
<p>The agent learned from experience. The second debugging session was faster because the agent recalled the pattern from the first. This is not model fine-tuning (the model weights didn‚Äôt change)‚Äîit‚Äôs <strong>in-context learning through memory</strong>.</p>
<p><strong>Planning with memory architecture:</strong></p>
<p><img  src="/eng-ai/_astro/30-diagram-2.DEY2TvHr_KqOcz.svg" alt="Planning with Memory: Learning from Experience diagram" width="800" height="450" loading="lazy" decoding="async"></p>
<p><strong>Figure 30.2</strong>: Planning with long-term memory. When given a task, the agent retrieves relevant past experiences, uses them to inform planning, executes the plan, and stores the outcome as new memories. This creates a learning loop where each task improves future performance through accumulated experience.</p>
<p>This architecture enables several powerful behaviors:</p>
<p><strong>Pattern recognition</strong>: ‚ÄúI‚Äôve seen this type of problem before, here‚Äôs what worked‚Äù</p>
<p><strong>Mistake avoidance</strong>: ‚ÄúLast time I tried approach X it failed because Y, so I‚Äôll try Z instead‚Äù</p>
<p><strong>Strategy reuse</strong>: ‚ÄúThis task is similar to task T, I‚Äôll adapt that successful plan‚Äù</p>
<p><strong>Preference adaptation</strong>: ‚ÄúUser corrected me twice on formatting, I‚Äôll remember their preference‚Äù</p>
<p><strong>Context efficiency</strong>: ‚ÄúI don‚Äôt need to ask questions I already know the answers to from past conversations‚Äù</p>
<hr>
<h2 id="alignment-and-identity-how-memory-changes-behavior-ch30">Alignment and Identity: How Memory Changes Behavior</h2>
<p>Persistent memory has a profound effect: it changes not just what the system knows but <strong>who it becomes</strong>. Memory creates continuity, identity, and alignment to specific users or contexts.</p>
<p><strong>Identity through memory.</strong> A system with memory develops a consistent persona:</p>
<ul>
<li>Remembers past statements and maintains consistency</li>
<li>Recalls commitments and follows through</li>
<li>Builds on previous conversations rather than starting fresh</li>
<li>Exhibits preferences learned from interactions</li>
</ul>
<p>This creates the illusion (or reality?) of <strong>continuity of self</strong>. The system behaves as if it is the same entity across sessions because it has access to its own history.</p>
<p><strong>Alignment through adaptation.</strong> Memory enables systems to adapt to individual users:</p>
<ul>
<li>Learn user communication style (formal vs. casual, technical vs. simple)</li>
<li>Adapt to user preferences (level of detail, explanation style)</li>
<li>Remember user feedback (‚Äúlast time you said X was too verbose‚Äù)</li>
<li>Build user-specific knowledge (‚Äúyou work on the authentication service‚Äù)</li>
</ul>
<p>This creates <strong>personalization</strong>: the system becomes aligned with the specific user‚Äôs needs and preferences without model fine-tuning.</p>
<p><strong>Challenges of persistent memory:</strong></p>
<p><strong>Privacy</strong>: Memory stores sensitive information. Who can access it? How long is it kept? Can users delete their memories?</p>
<p><strong>Bias accumulation</strong>: If the system learns from every interaction, harmful patterns can accumulate. A customer service bot might learn to be ruder if customers are rude, creating a negative feedback loop.</p>
<p><strong>Staleness</strong>: Old memories may become incorrect. User preferences change, facts become outdated, past advice may no longer apply.</p>
<p><strong>Context dependence</strong>: Memories from one context may not apply to another. The system must recognize when past experience is relevant vs. when the situation is fundamentally different.</p>
<p><strong>Forgetting mechanisms:</strong> Production systems need ways to manage memory:</p>
<ul>
<li><strong>Time-based decay</strong>: Old memories fade or are archived</li>
<li><strong>Relevance filtering</strong>: Rarely-accessed memories are demoted</li>
<li><strong>Explicit deletion</strong>: Users can remove specific memories</li>
<li><strong>Summary and compression</strong>: Detailed memories are compressed into general patterns over time</li>
</ul>
<p>The engineering challenge is balancing <strong>continuity</strong> (remember enough to be useful) with <strong>flexibility</strong> (don‚Äôt over-fit to past patterns) and <strong>privacy</strong> (forget what should not be remembered).</p>
<hr>
<h2 id="engineering-takeaway-ch30">Engineering Takeaway</h2>
<p><strong>Memory transforms stateless language models into stateful systems with identity and continuity.</strong> This transformation is fundamental: a model with memory is not just more capable‚Äîit becomes a different kind of system. This shift has several implications for production engineering:</p>
<p><strong>Long-term memory enables personalization and learning.</strong> Systems with memory can adapt to individual users without model retraining. They learn preferences, recognize patterns, and improve through experience. This makes AI systems feel less like tools and more like assistants: they know you, remember your context, and build on past interactions. The value is in accumulated knowledge, not just model capability.</p>
<p><strong>Memory retrieval is the critical challenge.</strong> Having memory is useless if you can‚Äôt find the relevant information when you need it. Production systems require sophisticated retrieval: semantic search for meaning, recency weighting for relevance, structured queries for facts. The quality of retrieval determines whether memory helps or adds noise. Poor retrieval is worse than no memory‚Äîirrelevant context confuses the model and wastes tokens.</p>
<p><strong>Planning with memory enables continuous improvement.</strong> Agents that remember past actions can learn what works and what fails. This creates a feedback loop: try strategy, observe result, remember outcome, improve future attempts. Unlike model training (which happens once on static data), memory-based learning is continuous and context-specific. The system gets better at your specific tasks through experience, not through generic training.</p>
<p><strong>Privacy is paramount and non-negotiable.</strong> Memory systems store sensitive information: user preferences, conversation history, personal facts, business data. This requires serious security: encryption at rest and in transit, access controls, audit logging, user-controlled deletion. GDPR and similar regulations apply: users must be able to see what‚Äôs stored and request deletion. Memory systems are data systems, and data systems have legal obligations.</p>
<p><strong>Memory changes alignment‚Äîmodels adapt to interactions over time.</strong> This is powerful but dangerous. A model that learns from every interaction can absorb biases, harmful patterns, or user-specific quirks that shouldn‚Äôt generalize. Memory-based adaptation happens faster than RLHF-style alignment but with less oversight. Production systems need guardrails: filter what gets stored, review patterns that emerge, prevent accumulation of harmful behaviors.</p>
<p><strong>State management is the core engineering problem.</strong> Production AI systems with memory are fundamentally about managing state: what to keep in context, what to store long-term, when to retrieve, when to forget. This requires database design, caching strategies, consistency guarantees, backup and recovery. Building stateful AI systems has more in common with building distributed databases than with training models. The challenge is state, not statistics.</p>
<p><strong>AI systems with memory are no longer stateless services‚Äîthey‚Äôre stateful applications.</strong> This changes deployment, testing, and maintenance. You can‚Äôt just rollback to a previous model version if the system has accumulated user-specific state. Testing requires seeding memory, not just checking input-output pairs. Debugging requires inspecting what the system remembers, not just what it generates. The system‚Äôs behavior depends on its history, making every deployment unique to its accumulated experience.</p>
<hr>
<h2 id="references-and-further-reading-ch30">References and Further Reading</h2>
<p><strong>MemGPT: Towards LLMs as Operating Systems</strong>
Packer, C., Fang, V., Patil, S. G., Wooders, K., &#x26; Gonzalez, J. E. (2023). <em>arXiv:2310.08560</em></p>
<p><em>Why it matters:</em> This paper introduced the analogy between operating system memory hierarchies (registers, cache, RAM, disk) and LLM memory systems (context window, working memory, long-term storage). MemGPT demonstrated how to manage context overflow by intelligently paging information in and out of the context window, similar to virtual memory in OS design. This architecture enables agents to operate indefinitely by treating the context window as a cache for a much larger memory space, solving one of the fundamental scalability challenges of long-running agents.</p>
<p><strong>Memory Networks</strong>
Weston, J., Chopra, S., &#x26; Bordes, A. (2015). <em>ICLR 2015</em></p>
<p><em>Why it matters:</em> While predating modern LLMs, this foundational work introduced the idea of neural networks with explicit external memory that can be read from and written to. Memory Networks showed that models could learn to store facts in memory and retrieve them when needed, rather than encoding everything in weights. This separation of computation (the model) from storage (the memory) influenced modern RAG systems and demonstrated that retrieval-augmented architectures could outperform purely parametric models on knowledge-intensive tasks.</p>
<p><strong>Generative Agents: Interactive Simulacra of Human Behavior</strong>
Park, J. S., O‚ÄôBrien, J. C., Cai, C. J., Morris, M. R., Liang, P., &#x26; Bernstein, M. S. (2023). <em>UIST 2023</em></p>
<p><em>Why it matters:</em> This work demonstrated agents with rich memory systems operating in a simulated environment over extended periods. Each agent maintained three types of memory: observations (what happened), reflections (higher-level insights), and plans (future intentions). Agents retrieved relevant memories through recency, importance, and relevance scoring. The system showed how memory enables coherent long-term behavior, social interaction, and emergent phenomena like coordinated planning. It revealed both the power of memory (enabling complex behavior) and its challenges (managing growth, ensuring relevance, maintaining consistency).</p>
<hr>
<p>With memory and planning, we complete the picture of modern AI systems: models (Part 5) augmented with prompting (Ch. 26), retrieval (Ch. 27), tools (Ch. 28), agency (Ch. 29), and memory (Ch. 30). These components compose into systems that <strong>perceive, reason, act, remember, and improve over time</strong>‚Äîthe fundamental capabilities required for AI to be useful in the real world.</p>
<p>Part 7 will examine <strong>Engineering Reality</strong>: how these systems fail, how to evaluate them, how to deploy them safely, and what challenges remain unsolved.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part6/29-agents" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Agents - Models That Decide What to Do</span> </a> <a href="/eng-ai/part7" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Part VII: Engineering Reality</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#short-term-vs-long-term-memory-ch30" data-astro-cid-xvrfupwn>Short-Term vs. Long-Term Memory</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#memory-types-episodic-and-semantic-ch30" data-astro-cid-xvrfupwn>Memory Types: Episodic and Semantic</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#memory-retrieval-finding-relevant-context-ch30" data-astro-cid-xvrfupwn>Memory Retrieval: Finding Relevant Context</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#planning-with-memory-learning-from-experience-ch30" data-astro-cid-xvrfupwn>Planning with Memory: Learning from Experience</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#alignment-and-identity-how-memory-changes-behavior-ch30" data-astro-cid-xvrfupwn>Alignment and Identity: How Memory Changes Behavior</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch30" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch30" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>