<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 32: Training vs Inference - Two Different Worlds | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part7/32-training-vs-inference/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part7/32-training-vs-inference/"><meta property="og:title" content="Chapter 32: Training vs Inference - Two Different Worlds | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part7/32-training-vs-inference/"><meta name="twitter:title" content="Chapter 32: Training vs Inference - Two Different Worlds | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part7" data-astro-cid-ilhxcym7>Part VII: Engineering Reality</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Training vs Inference - Two Different Worlds</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-32-training-vs-inference---two-different-worlds">Chapter 32: Training vs Inference - Two Different Worlds</h1>
<p>Training a model and serving it in production are fundamentally different operations. They have different performance requirements, different cost structures, different failure modes, and different engineering challenges. Understanding this divide is understanding why deploying ML is hard.</p>
<p>Training is offline, batch-oriented, and runs on powerful GPU clusters over hours or days. Accuracy is what matters. Inference is online, request-oriented, and must respond in milliseconds on limited hardware. Latency is what matters. A model that achieves 99% accuracy but takes 10 seconds to respond is useless for most applications.</p>
<p>This chapter explains the training-inference divide, why it creates constraints, and how production systems navigate these trade-offs.</p>
<hr>
<h2 id="offline-vs-online-different-performance-worlds-ch32">Offline vs Online: Different Performance Worlds</h2>
<p><strong>Training is offline batch processing.</strong> You have a fixed dataset. You process it multiple times (epochs). You can use large batches (64, 128, 512 examples at once) to maximize GPU utilization. You can take as long as needed‚Äîif training takes a week, you wait a week. The goal is to minimize loss on the training set (and generalize to test set).</p>
<p>Training characteristics:</p>
<ul>
<li><strong>Time scale</strong>: Hours to weeks</li>
<li><strong>Throughput over latency</strong>: Process millions of examples, don‚Äôt care about per-example time</li>
<li><strong>Hardware</strong>: High-end GPUs (A100, H100), often 8-64 GPUs in parallel</li>
<li><strong>Cost structure</strong>: Fixed upfront cost (train once, deploy forever‚Ä¶ until you retrain)</li>
<li><strong>Optimization target</strong>: Minimize loss function</li>
</ul>
<p><strong>Inference is online request processing.</strong> A user sends a query. The model must respond immediately. You process one request at a time (or small batches). Latency matters more than throughput‚Äîusers won‚Äôt wait 10 seconds for a search result. The goal is to minimize response time while maintaining accuracy.</p>
<p>Inference characteristics:</p>
<ul>
<li><strong>Time scale</strong>: Milliseconds to seconds</li>
<li><strong>Latency over throughput</strong>: Each request must complete quickly</li>
<li><strong>Hardware</strong>: CPUs, GPUs, or specialized accelerators (TPUs, edge devices)</li>
<li><strong>Cost structure</strong>: Per-query cost that scales with traffic</li>
<li><strong>Optimization target</strong>: Minimize latency, minimize cost per query</li>
</ul>
<p><strong>Latency requirements vary by application:</strong></p>
<ul>
<li><strong>Search</strong>: 50-100ms total budget (query understanding, retrieval, ranking, rendering)</li>
<li><strong>Recommendations</strong>: 100-200ms (more tolerance than search)</li>
<li><strong>Chatbots</strong>: 1-2 seconds (users tolerate slight delay for conversational UI)</li>
<li><strong>Real-time systems</strong>: &#x3C;10ms (trading algorithms, autonomous vehicles)</li>
</ul>
<p>Every 100ms of latency costs engagement. Google found that 500ms of delay reduces traffic by 20%. Amazon found that 100ms of latency costs 1% of sales. Users are impatient. Latency kills.</p>
<p><strong>Example: Google Search latency budget</strong></p>
<p>A Google Search query has ~50ms latency budget from query submission to results displayed. Within this budget:</p>
<ul>
<li>~10ms: Network latency (user to data center)</li>
<li>~5ms: Query understanding (spelling correction, intent classification)</li>
<li>~20ms: Retrieval and ranking (fetch candidates, score with ML models)</li>
<li>~10ms: Snippet generation and rendering</li>
<li>~5ms: Network return</li>
</ul>
<p>The ranking model‚Äîpotentially the most complex ML component‚Äîgets 20ms. If the model takes 200ms, it is unusable. The model must be fast, even if that means sacrificing accuracy. A 95% accurate model that runs in 15ms beats a 99% accurate model that runs in 100ms.</p>
<hr>
<h2 id="model-size-vs-latency-the-central-trade-off-ch32">Model Size vs Latency: The Central Trade-Off</h2>
<p>Bigger models are generally more accurate. More parameters capture more patterns. But bigger models are slower at inference. Every parameter must be loaded from memory, every layer must be computed. The trade-off is fundamental.</p>
<p><strong>GPT-3</strong> has 175 billion parameters, occupying ~350GB of memory in FP16. Running inference requires loading these parameters and computing matrix multiplications. On a single A100 GPU, GPT-3 inference takes ~1-2 seconds per token. For a 100-token response, that is 100-200 seconds‚Äîunusable for chat applications.</p>
<p>OpenAI solves this with <strong>model sharding</strong> (splitting the model across multiple GPUs) and <strong>batching</strong> (processing multiple requests simultaneously). But even optimized, GPT-3 inference costs ~$0.002 per 1K tokens. At billions of queries, this adds up.</p>
<p><strong>Model compression techniques</strong> trade accuracy for speed:</p>
<p><strong>Quantization</strong>: Reduce numerical precision. Standard training uses FP32 (32-bit floating point). Inference can use FP16 (16-bit), INT8 (8-bit integers), or even INT4. Lower precision means:</p>
<ul>
<li><strong>2-4x smaller models</strong>: 110M parameter BERT in FP32 is 440MB; in INT8 it is 110MB</li>
<li><strong>2-4x faster inference</strong>: Fewer bits to move, faster arithmetic</li>
<li><strong>Small accuracy loss</strong>: Typically &#x3C;1% accuracy drop with INT8</li>
</ul>
<p>Quantization works because models are over-parameterized. Many weights contribute little to predictions. Rounding them to lower precision has minimal impact.</p>
<p><strong>Distillation</strong>: Train a small model (student) to mimic a large model (teacher). The teacher model produces soft predictions (probabilities over all classes), which contain more information than hard labels. The student learns to match these predictions.</p>
<p>DistilBERT achieves 97% of BERT‚Äôs performance with 40% fewer parameters and 60% faster inference. TinyBERT goes further: 96% performance, 7.5x smaller, 9.4x faster. Distilled models are not just smaller‚Äîthey are often more efficient because they are trained to be small from the start.</p>
<p><strong>Pruning</strong>: Remove unimportant weights. After training, identify weights close to zero and set them to zero. Sparse models (many zero weights) can be stored and computed more efficiently.</p>
<p>Pruning can remove 50-90% of weights with &#x3C;1% accuracy loss. But exploiting sparsity requires specialized hardware or libraries (NVIDIA‚Äôs sparsity support, Intel‚Äôs Deep Learning Boost). On standard hardware, pruning saves memory but not necessarily compute.</p>
<p><strong>Early exit</strong>: Add intermediate classifiers at early layers. For easy examples, the model can exit early without computing all layers. For hard examples, compute the full model. This makes average latency faster while maintaining accuracy on hard cases.</p>
<p>Mobile models like MobileNet and EfficientNet are designed for inference. They use depthwise separable convolutions (cheaper than standard convolutions) and carefully balance width, depth, and resolution to maximize accuracy per FLOP.</p>
<hr>
<h2 id="caching-and-batching-how-systems-survive-at-scale-ch32">Caching and Batching: How Systems Survive at Scale</h2>
<p>Even with fast models, inference at billions of queries per day requires system-level optimizations: caching and batching.</p>
<p><strong>Caching</strong> stores results of previous queries. If a user searches ‚Äúweather New York,‚Äù the results are cached. If another user searches the same query within minutes, serve the cached result‚Äîno inference needed. Latency drops from 50ms to 5ms. Cost drops to near zero.</p>
<p>Caching works well for:</p>
<ul>
<li><strong>Duplicate queries</strong>: Many users search the same popular terms</li>
<li><strong>Near-duplicate queries</strong>: Minor variations (case, whitespace) map to same cache key</li>
<li><strong>Temporal locality</strong>: Same user repeats queries</li>
</ul>
<p>Caching does not work for:</p>
<ul>
<li><strong>Personalized queries</strong>: Results depend on user context</li>
<li><strong>Long-tail queries</strong>: Unique queries never hit cache</li>
<li><strong>Time-sensitive queries</strong>: Results go stale quickly (news, stock prices)</li>
</ul>
<p><strong>Cache hit rates</strong> vary by application. Google Search might have 30-40% cache hit rate (many unique queries). Netflix recommendations have lower hit rates (personalized). ChatGPT cannot cache much (every conversation is unique).</p>
<p><strong>Batching</strong> processes multiple requests together. GPUs are parallel processors‚Äîthey are most efficient when computing on large batches. Processing 1 request on a GPU is wasteful. Processing 64 requests simultaneously is efficient.</p>
<p>Batching trades latency for throughput. If you batch 64 requests:</p>
<ul>
<li><strong>Throughput</strong>: 64 requests processed in one forward pass ‚Üí 64x throughput increase</li>
<li><strong>Latency</strong>: Each request waits for batch to fill ‚Üí slight latency increase</li>
</ul>
<p><strong>Dynamic batching</strong> waits a short time (e.g., 10ms) to accumulate requests, then processes the batch. If traffic is high, batches fill quickly. If traffic is low, you wait the full 10ms. This is a latency-throughput trade-off: you accept 10ms extra latency to gain 10-50x throughput.</p>
<p>Production serving systems (TensorFlow Serving, TorchServe, Triton Inference Server) implement dynamic batching automatically. They monitor request arrivals, form batches, and adjust batch size based on latency constraints.</p>
<p><strong>Example: GPT inference with batching</strong></p>
<p>GPT-3 inference on a single request: 1 second per token, 100 tokens = 100 seconds.</p>
<p>GPT-3 inference with batch size 32: 1.5 seconds per token (slightly slower due to larger batch), 100 tokens = 150 seconds total, but 32 requests complete. Per-request time: 150/32 = 4.7 seconds.</p>
<p>Batching made individual requests slower (4.7s vs 1s), but throughput increased 20x. For services with high traffic, this trade-off is essential. The challenge is keeping batches full without making users wait when traffic is low.</p>
<hr>
<h2 id="deployment-why-shipping-ml-is-hard-ch32">Deployment: Why Shipping ML Is Hard</h2>
<p>Training produces a model. Deployment makes it serve traffic. Deployment is where most ML projects fail. Models that work perfectly in notebooks break in production.</p>
<p><strong>Deployment challenges:</strong></p>
<p><strong>Model serving infrastructure</strong>: You need a service that loads the model, accepts requests, runs inference, and returns results. This sounds simple but involves:</p>
<ul>
<li><strong>Model loading</strong>: Load GB-sized models into memory at startup (slow)</li>
<li><strong>Request handling</strong>: Parse requests, validate inputs, handle malformed data</li>
<li><strong>Inference execution</strong>: Run the model (GPU vs CPU, batching, caching)</li>
<li><strong>Response formatting</strong>: Convert model outputs to API responses</li>
<li><strong>Error handling</strong>: Timeouts, OOM errors, model crashes</li>
</ul>
<p>TensorFlow Serving, TorchServe, and Triton abstract some of this, but you still need to configure batching, resource limits, and monitoring.</p>
<p><strong>A/B testing</strong>: Before fully replacing an old model, you want to compare it to the new model on real traffic. A/B testing routes a fraction of traffic (e.g., 5%) to the new model, the rest to the old model. Measure metrics: accuracy, latency, user engagement. If the new model is better, gradually increase its traffic. If it is worse, roll back.</p>
<p>A/B testing requires:</p>
<ul>
<li><strong>Traffic splitting</strong>: Route users deterministically to models (sticky assignment based on user ID)</li>
<li><strong>Metrics tracking</strong>: Log predictions and outcomes for both models</li>
<li><strong>Statistical testing</strong>: Determine if differences are significant</li>
</ul>
<p><strong>Shadow deployment</strong>: Run the new model alongside the old model, but only log its predictions‚Äîdo not serve them to users. This lets you measure the new model‚Äôs performance without risking user experience. If the new model makes wildly wrong predictions, you catch it before users see it.</p>
<p>Shadow mode is safer than A/B testing but does not measure user impact (engagement, satisfaction). It measures model metrics (accuracy, precision, recall), not business metrics.</p>
<p><strong>Gradual rollout</strong>: Start with 1% of traffic, monitor for issues, increase to 5%, 10%, 50%, 100%. If problems arise (latency spikes, errors, bad predictions), roll back immediately. Gradual rollout limits blast radius‚Äîif the model fails, only 1% of users are affected.</p>
<p><strong>Canary deployment</strong>: Deploy the new model to a single region or data center first. Monitor closely. If it works, deploy globally. If it fails, only one region is affected.</p>
<p><strong>Rollback mechanisms</strong>: Models fail in production. Rollback must be fast. Keep the old model loaded in memory so you can switch with a config change. Do not require redeploying code or restarting services.</p>
<p><strong>Example: Netflix recommendation rollout</strong></p>
<p>Netflix tests new recommendation algorithms carefully:</p>
<ol>
<li><strong>Offline evaluation</strong>: Test on historical data (watch patterns, ratings)</li>
<li><strong>Online A/B test</strong>: Route 1% of users to new algorithm</li>
<li><strong>Measure engagement</strong>: Watch time, retention, satisfaction surveys</li>
<li><strong>Gradual rollout</strong>: If metrics improve, increase to 10%, 50%, 100%</li>
<li><strong>Rollback plan</strong>: Keep old algorithm running, switch back if needed</li>
</ol>
<p>Even a 1% improvement in engagement is worth millions. But deploying a worse algorithm costs millions. Rigorous testing is mandatory.</p>
<hr>
<h2 id="cost-at-scale-when-inference-dominates-ch32">Cost at Scale: When Inference Dominates</h2>
<p>Training costs are one-time. Inference costs are ongoing, per query. At billions of queries, inference dominates total cost.</p>
<p><strong>Example: GPT-4 deployment costs</strong></p>
<p>Assume:</p>
<ul>
<li>Training cost: $100 million (one-time, estimate)</li>
<li>Inference cost: $0.03 per 1K tokens</li>
<li>Average query: 200 tokens input + 200 tokens output = 400 tokens</li>
<li>Cost per query: $0.012</li>
</ul>
<p>At 1 billion queries/day:</p>
<ul>
<li>Daily inference cost: $12 million</li>
<li>Annual inference cost: $4.4 billion</li>
</ul>
<p>Inference costs dwarf training costs at scale. After 100 billion queries, inference costs 1000x more than training.</p>
<p>This is why companies optimize inference aggressively: quantization, distillation, caching, batching. A 2x speedup halves inference costs. At billions of queries, that is millions of dollars saved.</p>
<p><strong>Inference hardware matters.</strong> GPUs are powerful but expensive ($10K-30K per device, high power consumption). TPUs (Google‚Äôs Tensor Processing Units) are specialized for inference‚Äîfaster and cheaper per query. Edge devices (phones, cameras, cars) use even cheaper hardware (ARM CPUs, mobile GPUs). Model size and latency must fit the hardware constraints.</p>
<p><strong>Example: Tesla Autopilot inference</strong></p>
<p>Tesla‚Äôs Full Self-Driving (FSD) computer has two custom AI chips, each capable of 36 TOPS (trillion operations per second). The model must run on this hardware:</p>
<ul>
<li>Process 8 camera feeds at 36 FPS</li>
<li>Run perception (object detection), prediction (trajectory forecasting), and planning</li>
<li>Total latency budget: &#x3C;100ms (real-time control requirement)</li>
</ul>
<p>The model cannot be too big (must fit in on-device memory) or too slow (must meet latency). Tesla uses custom-designed networks optimized for their hardware. Training uses large GPUs in data centers, but inference runs on $1K custom chips in cars.</p>
<p>The constraints are hardware, not model capability. A better model that does not fit the hardware is useless.</p>
<p><img  src="/eng-ai/_astro/32-diagram.DoQ_4_zd_ZiRgD4.svg" alt="Cost at Scale: When Inference Dominates diagram" width="800" height="450" loading="lazy" decoding="async"></p>
<p><strong>Figure 32.1</strong>: Training vs inference comparison. Training is offline batch processing optimized for accuracy on powerful hardware. Inference is online request processing optimized for latency on constrained hardware. Deployment bridges these two worlds, requiring trade-offs between model size, accuracy, and speed.</p>
<hr>
<h2 id="engineering-takeaway-ch32">Engineering Takeaway</h2>
<p><strong>Training happens once (or periodically), inference happens billions of times‚Äîoptimize for the common case.</strong> Every millisecond of inference latency affects millions of users. Every megabyte of model size increases serving costs. Training budgets can be large (spend <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mi>M</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi>e</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>b</mi><mi>u</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>f</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>b</mi><mi>u</mi><mi>d</mi><mi>g</mi><mi>e</mi><mi>t</mi><mi>s</mi><mi>m</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>w</mi><mo stretchy="false">(</mo></mrow><annotation encoding="application/x-tex">100M on compute), but inference budgets must be low (</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">100</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal">co</span><span class="mord mathnormal">m</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">ere</span><span class="mord mathnormal">n</span><span class="mord mathnormal">ce</span><span class="mord mathnormal">b</span><span class="mord mathnormal">u</span><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">s</span><span class="mord mathnormal">m</span><span class="mord mathnormal">u</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">b</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mopen">(</span></span></span></span>0.001 per query). The model that wins is not the most accurate‚Äîit is the model that achieves acceptable accuracy at acceptable latency and cost.</p>
<p><strong>Latency kills user experience‚Äîevery 100ms matters for engagement.</strong> Users are impatient. Google, Amazon, and Facebook have measured this precisely: latency costs traffic, sales, and engagement. A slightly more accurate model that is 2x slower loses to a slightly less accurate model that is 2x faster. Latency is a hard constraint, not a nice-to-have. If your model does not meet latency requirements, it does not deploy.</p>
<p><strong>Model size is your enemy at inference time‚Äîcompression techniques are mandatory.</strong> Big models are accurate but slow and expensive. Quantization, distillation, pruning, and efficient architectures reduce model size without destroying accuracy. INT8 quantization gives 4x speedup with &#x3C;1% accuracy loss. Distillation gives 2-3x speedup with 3-5% accuracy loss. These techniques are not optional‚Äîthey are required to serve at scale.</p>
<p><strong>Caching and batching are essential for surviving at scale.</strong> Caching eliminates inference for duplicate queries‚Äîfree speedup if your workload has repetition. Batching increases GPU utilization by 10-50x, trading slight latency for massive throughput. Without these optimizations, serving billions of queries is economically infeasible. Every major ML serving system implements caching and dynamic batching.</p>
<p><strong>Deployment is risky‚Äîgradual rollouts, A/B testing, and rollback are mandatory.</strong> Models that pass offline evaluation fail in production. Users behave differently than test sets predict. Edge cases appear. Latency degrades under load. Gradual rollout (1% ‚Üí 5% ‚Üí 50% ‚Üí 100%) limits blast radius. A/B testing measures real impact on users, not just model metrics. Rollback lets you revert instantly when things break. Deploy without these safeguards, and you will have outages.</p>
<p><strong>Monitoring inference is harder than monitoring training‚Äîtrack latency, throughput, error rates, and data drift.</strong> Training monitoring is straightforward: loss goes down, accuracy goes up. Inference monitoring is multi-dimensional: p50/p95/p99 latency, queries per second, error rates, cache hit rates, model prediction distributions (drift detection). Inference failures are subtle‚Äîlatency spikes at 3am, prediction quality degrades slowly, edge cases increase. Real-time monitoring catches problems before users complain.</p>
<p><strong>Why MLOps exists‚Äîproduction ML is reliability engineering, not model training.</strong> MLOps is the discipline of operating ML systems in production: model serving, deployment pipelines, monitoring, retraining, versioning, rollback. It is DevOps for ML. The hard problems are not ‚ÄúHow do I train a model?‚Äù‚Äîthey are ‚ÄúHow do I serve a model to a billion users with 99.9% uptime, 50ms latency, and without breaking the bank?‚Äù MLOps is the engineering that makes ML production-ready.</p>
<hr>
<h2 id="references-and-further-reading-ch32">References and Further Reading</h2>
<p><strong>TFX: A TensorFlow-Based Production-Scale Machine Learning Platform</strong>
Baylor, D., Breck, E., Cheng, H.-T., Fiedel, N., Foo, C. Y., Haque, Z., Haykal, S., Ispir, M., Jain, V., Koc, L., et al. (2017). <em>KDD 2017</em></p>
<p><em>Why it matters:</em> This paper from Google describes TFX (TensorFlow Extended), the production ML infrastructure used internally at Google. It covers data validation, feature engineering, training orchestration, model analysis, and serving at scale. TFX handles billions of queries per day across products like Search, YouTube, and Gmail. The paper emphasizes that the model is a small part of the system‚Äîdata validation, monitoring, and serving infrastructure are the majority of the work. TFX became the blueprint for many companies building ML platforms.</p>
<p><strong>Clipper: A Low-Latency Online Prediction Serving System</strong>
Crankshaw, D., Wang, X., Zhou, G., Franklin, M. J., Gonzalez, J. E., &#x26; Stoica, I. (2017). <em>NSDI 2017</em></p>
<p><em>Why it matters:</em> Clipper is a research system from UC Berkeley that addresses the latency challenge of inference. It introduces adaptive batching (dynamically adjusting batch size to meet latency SLOs) and model caching. Clipper showed that careful system design can reduce latency by 2-4x while increasing throughput by 10x. The paper highlights that inference is a systems problem, not just a model problem‚Äîcaching, batching, and scheduling matter as much as model architecture.</p>
<p><strong>Data Management Challenges in Production Machine Learning</strong>
Polyzotis, N., Roy, S., Whang, S. E., &#x26; Zinkevich, M. (2017). <em>SIGMOD 2017</em></p>
<p><em>Why it matters:</em> This Google paper examines the data management challenges of production ML. It describes how models trained offline must serve online, how data distributions shift, and how monitoring must detect these changes. The paper introduces the concept of ‚Äúdata debugging‚Äù‚Äîtracking data lineage, validating schemas, detecting anomalies. It argues that production ML is fundamentally about data engineering, and that most failures are data failures. The paper influenced the design of tools like TensorFlow Data Validation and Facets.</p>
<hr>
<p>The next chapter examines evaluation: why accuracy is not enough, why benchmarks mislead, and how production metrics differ from research metrics.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part7/31-data-pipelines" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Data Pipelines - Where Models Are Born and Die</span> </a> <a href="/eng-ai/part7/33-evaluation" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Evaluation - Why Accuracy Is Not Enough</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#offline-vs-online-different-performance-worlds-ch32" data-astro-cid-xvrfupwn>Offline vs Online: Different Performance Worlds</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#model-size-vs-latency-the-central-trade-off-ch32" data-astro-cid-xvrfupwn>Model Size vs Latency: The Central Trade-Off</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#caching-and-batching-how-systems-survive-at-scale-ch32" data-astro-cid-xvrfupwn>Caching and Batching: How Systems Survive at Scale</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#deployment-why-shipping-ml-is-hard-ch32" data-astro-cid-xvrfupwn>Deployment: Why Shipping ML Is Hard</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#cost-at-scale-when-inference-dominates-ch32" data-astro-cid-xvrfupwn>Cost at Scale: When Inference Dominates</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch32" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch32" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>