<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Chapter 34: Hallucinations, Bias, and Brittleness | Engineering Intelligence</title><meta name="description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><!-- Canonical URL --><link rel="canonical" href="https://olivernguyen.io/eng-ai/part7/34-hallucinations-bias-brittleness/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://olivernguyen.io/eng-ai/part7/34-hallucinations-bias-brittleness/"><meta property="og:title" content="Chapter 34: Hallucinations, Bias, and Brittleness | Engineering Intelligence"><meta property="og:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta property="og:image" content="https://olivernguyen.io/eng-ai/og-image.png"><meta property="og:site_name" content="Engineering Intelligence"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://olivernguyen.io/eng-ai/part7/34-hallucinations-bias-brittleness/"><meta name="twitter:title" content="Chapter 34: Hallucinations, Bias, and Brittleness | Engineering Intelligence"><meta name="twitter:description" content="A book for engineers who want to understand Machine Learning and AI from first principles"><meta name="twitter:image" content="https://olivernguyen.io/eng-ai/og-image.png"><!-- Additional Meta --><meta name="author" content="Oliver Nguyen"><meta name="theme-color" content="#2563eb"><!-- Favicon (SVG preferred, PNG fallback) --><link rel="icon" type="image/svg+xml" href="/eng-ai/favicon.svg"><link rel="icon" type="image/png" sizes="32x32" href="/eng-ai/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/eng-ai/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/eng-ai/apple-touch-icon.png"><!-- KaTeX CSS for math rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><!-- Site styles --><link rel="stylesheet" href="/eng-ai/styles/global.css"><link rel="stylesheet" href="/eng-ai/styles/typography.css"><link rel="stylesheet" href="/eng-ai/styles/themes.css"><link rel="stylesheet" href="/eng-ai/styles/print.css"><style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:2rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{display:flex;flex-wrap:wrap;gap:.5rem;list-style:none;padding:0;font-size:.9rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"‚Ä∫";color:var(--text-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--accent);text-decoration:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--text-muted)}.toc[data-astro-cid-xvrfupwn]{position:sticky;top:2rem;padding:1rem}.toc[data-astro-cid-xvrfupwn] h4[data-astro-cid-xvrfupwn]{font-size:.9rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);margin-bottom:.75rem}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;padding:0}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn]{margin:.5rem 0}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{font-size:.85rem;color:var(--text-muted);text-decoration:none;transition:color .2s}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--accent)}.toc-level-2[data-astro-cid-xvrfupwn]{padding-left:0}.toc-level-3[data-astro-cid-xvrfupwn]{padding-left:1rem;font-size:.8rem}.chapter-navigation[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;gap:2rem;margin:4rem 0 2rem;padding-top:2rem;border-top:1px solid var(--border)}.nav-prev[data-astro-cid-pux6a34n],.nav-next[data-astro-cid-pux6a34n]{display:flex;flex-direction:column;gap:.5rem;padding:1rem;border:1px solid var(--border);border-radius:8px;text-decoration:none;transition:border-color .2s,background .2s;flex:1;max-width:45%}.nav-prev[data-astro-cid-pux6a34n]:hover,.nav-next[data-astro-cid-pux6a34n]:hover{border-color:var(--accent);background:var(--hover-bg)}.nav-prev[data-astro-cid-pux6a34n].placeholder{visibility:hidden}.nav-next[data-astro-cid-pux6a34n]{align-items:flex-end;text-align:right}.nav-label[data-astro-cid-pux6a34n]{font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;color:var(--accent)}.nav-title[data-astro-cid-pux6a34n]{font-size:1rem;color:var(--text)}
</style>
<link rel="stylesheet" href="/eng-ai/_astro/_chapter_.BUailOsj.css"><script type="module" src="/eng-ai/_astro/hoisted.0uyw1jl9.js"></script></head> <body>  <div class="book-layout"> <button id="mobile-menu-toggle" class="hamburger-button" aria-label="Toggle navigation menu" aria-expanded="false" data-astro-cid-odmlyywb> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> <span class="hamburger-line" data-astro-cid-odmlyywb></span> </button>  <div class="sidebar-overlay" id="sidebar-overlay" data-astro-cid-ssfzsv2f></div> <aside class="sidebar" id="sidebar" data-astro-cid-ssfzsv2f> <!-- Sidebar Controls --> <div class="sidebar-controls" data-astro-cid-ssfzsv2f> <button id="sidebar-close" class="sidebar-close-button" aria-label="Close menu" data-astro-cid-ssfzsv2f> <svg width="20" height="20" viewBox="0 0 20 20" fill="none" data-astro-cid-ssfzsv2f> <line x1="4" y1="4" x2="16" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16" y1="4" x2="4" y2="16" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> </button> <button id="theme-toggle" class="theme-toggle-button" aria-label="Toggle dark mode" data-astro-cid-ssfzsv2f> <svg class="sun-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <circle cx="10" cy="10" r="4" fill="currentColor" data-astro-cid-ssfzsv2f></circle> <line x1="10" y1="1" x2="10" y2="3" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="10" y1="17" x2="10" y2="19" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="1" y1="10" x2="3" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="17" y1="10" x2="19" y2="10" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="3.5" y1="3.5" x2="5" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="15" y1="15" x2="16.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="16.5" y1="3.5" x2="15" y2="5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> <line x1="5" y1="15" x2="3.5" y2="16.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" data-astro-cid-ssfzsv2f></line> </svg> <svg class="moon-icon" width="20" height="20" viewBox="0 0 20 20" data-astro-cid-ssfzsv2f> <path d="M10 2a8 8 0 108 8 6 6 0 01-8-8z" fill="currentColor" data-astro-cid-ssfzsv2f></path> </svg> </button> </div> <div class="sidebar-header" data-astro-cid-ssfzsv2f> <a href="/eng-ai/" data-astro-cid-ssfzsv2f> <h2 data-astro-cid-ssfzsv2f>Engineering Intelligence</h2> </a> </div> <nav class="sidebar-nav" data-astro-cid-ssfzsv2f> <section class="intro-section" data-astro-cid-ssfzsv2f> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/introduction" <span class="intro-icon" data-astro-cid-ssfzsv2f>üìñ
              Introduction
</a> </li> </ul> </section> <section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1" data-astro-cid-ssfzsv2f>
Part I: Foundations </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/01-why-intelligence-is-not-magic" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>1.</span> Why Intelligence Is Not Magic </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/02-data-is-the-new-physics" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>2.</span> Data Is the New Physics </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/03-models-are-compression-machines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>3.</span> Models Are Compression Machines </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/04-bias-variance-tradeoff" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>4.</span> The Bias-Variance Tradeoff </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part1/05-features-how-machines-see-the-world" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>5.</span> Features: How Machines See the World </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2" data-astro-cid-ssfzsv2f>
Part II: Classical ML </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/06-linear-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>6.</span> Linear Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/07-logistic-regression" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>7.</span> Logistic Regression </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/08-decision-trees" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>8.</span> Decision Trees </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/09-ensembles" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>9.</span> Ensembles </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part2/10-loss-functions" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>10.</span> Loss Functions and Optimization </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3" data-astro-cid-ssfzsv2f>
Part III: Neural Networks </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/11-neurons-as-math" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>11.</span> Neurons as Math </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/12-forward-pass" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>12.</span> The Forward Pass </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/13-backpropagation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>13.</span> Backpropagation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/14-optimization-deep-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>14.</span> Optimization in Deep Learning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part3/15-representation-learning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>15.</span> Representation Learning </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4" data-astro-cid-ssfzsv2f>
Part IV: Deep Architectures </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/16-convolutional-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>16.</span> Convolutional Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/17-recurrent-neural-networks" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>17.</span> Recurrent Neural Networks </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/18-embeddings" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>18.</span> Embeddings </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/19-attention" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>19.</span> Attention </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part4/20-transformers" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>20.</span> Transformers </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5" data-astro-cid-ssfzsv2f>
Part V: Language Models </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/21-next-token-prediction" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>21.</span> Next-Token Prediction </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/22-pretraining" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>22.</span> Pretraining </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/23-fine-tuning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>23.</span> Fine-Tuning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/24-rlhf" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>24.</span> RLHF </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part5/25-emergent-abilities-and-scaling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>25.</span> Emergent Abilities and Scaling </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6" data-astro-cid-ssfzsv2f>
Part VI: Modern AI Systems </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/26-prompting-as-programming" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>26.</span> Prompting as Programming </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/27-retrieval-augmented-generation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>27.</span> Retrieval-Augmented Generation </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/28-tools-and-function-calling" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>28.</span> Tools and Function Calling </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/29-agents" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>29.</span> Agents - Models That Decide What to Do </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part6/30-memory-and-planning" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>30.</span> Memory, Planning, and Long-Term Behavior </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7" data-astro-cid-ssfzsv2f>
Part VII: Engineering Reality </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/31-data-pipelines" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>31.</span> Data Pipelines - Where Models Are Born and Die </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/32-training-vs-inference" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>32.</span> Training vs Inference - Two Different Worlds </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/33-evaluation" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>33.</span> Evaluation - Why Accuracy Is Not Enough </a> </li><li class="active" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/34-hallucinations-bias-brittleness" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>34.</span> Hallucinations, Bias, and Brittleness </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part7/35-safety-alignment-control" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>35.</span> Safety, Alignment, and Control </a> </li> </ul> </section><section class="part-section" data-astro-cid-ssfzsv2f> <h3 class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8" data-astro-cid-ssfzsv2f>
Part VIII: The Frontier </a> </h3> <ul data-astro-cid-ssfzsv2f> <li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/36-scaling-laws" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>36.</span> Scaling Laws - Why Bigger Keeps Winning </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/37-multimodal-models" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>37.</span> Multimodal Models </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/38-self-improving-systems" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>38.</span> Self-Improving Systems </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/39-artificial-general-intelligence" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>39.</span> Artificial General Intelligence </a> </li><li class="" data-astro-cid-ssfzsv2f> <a href="/eng-ai/part8/40-the-engineers-role" data-astro-cid-ssfzsv2f> <span class="chapter-num" data-astro-cid-ssfzsv2f>40.</span> The Engineer&#39;s Role </a> </li> </ul> </section> </nav> </aside>   <main class="chapter-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/eng-ai/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7><a href="/eng-ai/part7" data-astro-cid-ilhxcym7>Part VII: Engineering Reality</a></li> <li aria-current="page" data-astro-cid-ilhxcym7>Hallucinations, Bias, and Brittleness</li> </ol> </nav>  <article class="chapter"> <h1 id="chapter-34-hallucinations-bias-and-brittleness">Chapter 34: Hallucinations, Bias, and Brittleness</h1>
<p>Models do not just fail quietly by giving wrong answers. They fail in ways that are insidious, systematic, and often invisible until catastrophic damage occurs. Language models confidently generate facts that are completely false. Image classifiers predict ‚Äúostrich‚Äù when shown a school bus with tiny stickers on it. Hiring models systematically discriminate based on gender and race. Medical models trained on biased data amplify healthcare disparities.</p>
<p>These are not random errors. They are predictable failure modes that emerge from how models are trained, what data they see, and what they optimize. Understanding these failures is understanding why deploying AI requires guardrails, monitoring, and humility.</p>
<p>This chapter explains hallucinations (models generate plausible falsehoods), bias (models learn and amplify discrimination), adversarial brittleness (tiny perturbations break models), and robustness failures (models collapse under pressure). These are the failure modes that make AI dangerous.</p>
<hr>
<h2 id="why-hallucinations-happen-probability-is-not-truth-ch34">Why Hallucinations Happen: Probability Is Not Truth</h2>
<p><strong>Hallucination</strong> is when a model generates outputs that are fluent, coherent, and confident, but factually false. Language models hallucinate because they optimize likelihood, not truth. They predict what text is likely to follow, not what is correct.</p>
<p><strong>Why models hallucinate:</strong></p>
<p><strong>Models optimize next-token likelihood.</strong> Given a prompt, the model outputs the most probable continuation according to its training data. If training data contains misinformation, the model learns to generate misinformation. If training data lacks information on a topic, the model fabricates plausible-sounding text.</p>
<p><strong>Confidence is not correctness.</strong> Models assign high probability to hallucinated content because hallucinations follow linguistic patterns seen in training. A fluent, grammatical sentence is assigned high likelihood even if factually wrong.</p>
<p><strong>Example: GPT hallucinating legal cases</strong></p>
<p>A lawyer used ChatGPT to write a legal brief. ChatGPT cited six case precedents‚Äîcases with official-looking names, docket numbers, and legal reasoning. All six cases were fabricated. They did not exist. ChatGPT generated plausible legal citations because it learned the pattern of how citations look, not because it verified their existence.</p>
<p>The model optimized likelihood: ‚ÄúWhat would a legal citation look like here?‚Äù It did not optimize truth: ‚ÄúDoes this case exist?‚Äù The result was confidently wrong output that passed surface inspection but failed fact-checking.</p>
<p><strong>Example: Medical misinformation</strong></p>
<p>A user asks a medical chatbot: ‚ÄúWhat is the cure for lupus?‚Äù The model responds: ‚ÄúLupus can be cured with a combination of vitamin D, turmeric, and gluten-free diet.‚Äù This is false. Lupus is a chronic autoimmune disease with no cure. Treatment involves immunosuppressants and corticosteroids, not dietary supplements.</p>
<p>The model hallucinated because:</p>
<ol>
<li>Alternative medicine misinformation is common in training data (blogs, forums)</li>
<li>The pattern ‚ÄúX can be cured with‚Ä¶‚Äù appears frequently</li>
<li>The model did not verify against medical consensus</li>
</ol>
<p>The response is fluent and confident. A non-expert might believe it. This is dangerous.</p>
<p><strong>Grounding and citation reduce hallucinations.</strong> Retrieval-Augmented Generation (RAG, Chapter 27) fetches documents and instructs the model to generate responses grounded in those documents. The model is less likely to hallucinate when constrained to paraphrase retrieved text. Citations let users verify claims.</p>
<p>But grounding does not eliminate hallucinations. Models can:</p>
<ul>
<li>Cite retrieved documents but misinterpret them</li>
<li>Cite irrelevant documents to justify hallucinated content</li>
<li>Generate confident claims despite weak evidence</li>
</ul>
<p>Hallucinations are a fundamental property of generative models. They can be reduced but not eliminated. Any deployment of generative models must assume hallucinations will occur and design safeguards accordingly.</p>
<hr>
<h2 id="bias-data-becomes-destiny-ch34">Bias: Data Becomes Destiny</h2>
<p>Machine learning models learn from data. If the data reflects societal biases‚Äîsexism, racism, ableism‚Äîthe model learns those biases. If the data overrepresents some groups and underrepresents others, the model performs better on overrepresented groups. Bias in data becomes bias in models.</p>
<p><strong>Types of bias:</strong></p>
<p><strong>Representation bias</strong>: Some groups are underrepresented in training data. Models trained on biased data perform worse on underrepresented groups.</p>
<p><strong>Example: Face recognition bias</strong></p>
<p>Early face recognition datasets (e.g., Labeled Faces in the Wild) were 77% male and 83% light-skinned. Models trained on this data achieved 99% accuracy on light-skinned males but 65% accuracy on dark-skinned females. The model learned features that discriminate between light-skinned males (abundant examples) but struggled with dark-skinned females (rare examples).</p>
<p>This is not a model architecture problem. The model learned what the data taught. The data was biased, so the model became biased.</p>
<p><strong>Measurement bias</strong>: The data uses proxies that do not capture what matters, leading to biased predictions.</p>
<p><strong>Example: Recidivism prediction (COMPAS)</strong></p>
<p>COMPAS is a tool used in US courts to predict recidivism (likelihood of reoffending). It uses features like prior arrests, age, and neighborhood. Studies found it falsely flagged Black defendants as high-risk twice as often as white defendants, while falsely flagging white defendants as low-risk twice as often as Black defendants.</p>
<p>Why? The data reflects biased policing. Black individuals are arrested more often for the same behavior due to over-policing in Black neighborhoods. The model learns that arrest history predicts recidivism, but arrest history is a biased proxy. The model amplifies existing discrimination.</p>
<p><strong>Historical bias</strong>: Data reflects past discrimination. Models trained on historical data perpetuate that discrimination.</p>
<p><strong>Example: Amazon hiring tool</strong></p>
<p>Amazon built a resume screening tool trained on 10 years of hiring data. The model learned that male candidates were hired more often (tech industry is male-dominated). It penalized resumes containing ‚Äúwomen‚Äôs‚Äù (e.g., ‚Äúwomen‚Äôs chess club‚Äù) and preferred resumes with male-associated language.</p>
<p>The model did not learn ‚Äúgood candidates.‚Äù It learned ‚Äúwhat past hires looked like.‚Äù Past hires were biased, so the model became biased. Amazon scrapped the tool.</p>
<p><strong>Amplification bias</strong>: Models can amplify biases beyond what exists in training data. Small correlations in data become strong signals in models.</p>
<p><strong>Example: Word embeddings and gender stereotypes</strong></p>
<p>Word embeddings (Chapter 18) trained on text corpus learn associations:</p>
<ul>
<li>‚Äúdoctor‚Äù is closer to ‚Äúman‚Äù than ‚Äúwoman‚Äù</li>
<li>‚Äúnurse‚Äù is closer to ‚Äúwoman‚Äù than ‚Äúman‚Äù</li>
<li>‚Äúprogrammer‚Äù is closer to ‚Äúhe‚Äù than ‚Äúshe‚Äù</li>
</ul>
<p>These embeddings reflect gendered language in text (doctors are more often referred to as ‚Äúhe‚Äù). But when used in downstream tasks (search, recommendation, hiring), they amplify stereotypes. A search for ‚Äúdoctor‚Äù shows male doctors preferentially. A resume ranker penalizes women in technical roles.</p>
<p><strong>Debiasing is hard.</strong> You can:</p>
<ul>
<li><strong>Rebalance training data</strong>: Oversample underrepresented groups, undersample overrepresented groups</li>
<li><strong>Debias representations</strong>: Remove gender/race signals from embeddings</li>
<li><strong>Add fairness constraints</strong>: Penalize disparate impact during training</li>
<li><strong>Post-process outputs</strong>: Adjust predictions to equalize false positive rates across groups</li>
</ul>
<p>None of these fully solve bias. Rebalancing changes the data distribution (test accuracy may drop). Debiasing removes explicit signals but implicit correlations remain (occupation ‚Üí gender is still learned through proxies). Fairness constraints may improve one metric but worsen another (equal false positive rates may worsen overall accuracy).</p>
<p><strong>Bias is not a technical problem alone‚Äîit is a sociotechnical problem.</strong> Technical fixes do not address root causes: discriminatory data collection, biased labels, unjust ground truth. You cannot debias a hiring model if historical hiring was discriminatory. You cannot debias a recidivism model if policing is discriminatory. The model learns what the data teaches. Fix the data, or accept that the model will be biased.</p>
<hr>
<h2 id="adversarial-inputs-how-models-are-tricked-ch34">Adversarial Inputs: How Models Are Tricked</h2>
<p><strong>Adversarial examples</strong> are inputs carefully crafted to fool the model. They are imperceptible to humans but cause the model to make catastrophically wrong predictions.</p>
<p><strong>Example: Adversarial stickers on stop signs</strong></p>
<p>Researchers placed small, carefully designed stickers on stop signs. To humans, the stop sign looks normal. To a neural network, it is no longer a stop sign‚Äîit is classified as ‚Äúspeed limit 45.‚Äù A self-driving car seeing this sign might not stop, causing a crash.</p>
<p>The stickers are adversarial perturbations. They exploit how the model learned to recognize stop signs. The model relies on brittle features (edges, colors, textures) that can be manipulated without changing the sign‚Äôs appearance to humans.</p>
<p><strong>Why adversarial examples exist:</strong></p>
<p>Models learn decision boundaries in high-dimensional space. In these spaces, small perturbations (invisible to humans) can move an example from one side of the boundary to the other. The model has not learned robust features‚Äîit has learned shortcuts.</p>
<p><strong>Generating adversarial examples:</strong></p>
<ol>
<li>Start with a clean image (e.g., a panda)</li>
<li>Compute the gradient of the loss with respect to the input: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">‚àá</mi><mi>x</mi></msub><mi>L</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\nabla_x L(f(x), y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord">‚àá</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></li>
<li>Modify the input in the direction that increases loss: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>a</mi><mi>d</mi><mi>v</mi></mrow></msub><mo>=</mo><mi>x</mi><mo>+</mo><mi>œµ</mi><mo>‚ãÖ</mo><mtext>sign</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="normal">‚àá</mi><mi>x</mi></msub><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{adv} = x + \epsilon \cdot \text{sign}(\nabla_x L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord mathnormal">œµ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚ãÖ</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">sign</span></span><span class="mopen">(</span><span class="mord"><span class="mord">‚àá</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">L</span><span class="mclose">)</span></span></span></span></li>
<li>The modified image looks nearly identical but the model misclassifies it</li>
</ol>
<p>With <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi><mo>=</mo><mn>0.007</mn></mrow><annotation encoding="application/x-tex">\epsilon = 0.007</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">œµ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.007</span></span></span></span> (imperceptible change), an image classifier predicts ‚Äúpanda‚Äù ‚Üí ‚Äúgibbon‚Äù with 99% confidence.</p>
<p><strong>Physical adversarial examples:</strong></p>
<p>Digital adversarial examples are interesting but not directly threatening (you cannot inject noise into a real-world scene). Physical adversarial examples work in the real world:</p>
<ul>
<li><strong>Adversarial glasses</strong>: 3D-printed glasses that fool face recognition</li>
<li><strong>Adversarial patches</strong>: Printed stickers that cause misclassification</li>
<li><strong>Adversarial clothing</strong>: T-shirts with patterns that make pedestrian detectors miss people</li>
</ul>
<p>These examples exploit models deployed in physical environments: surveillance cameras, self-driving cars, security systems.</p>
<p><strong>Robustness to adversarial examples is hard.</strong> Adversarial training (training on adversarial examples) improves robustness but does not eliminate vulnerability. Models can be robust to known attacks but vulnerable to new attacks. The cat-and-mouse game continues.</p>
<p><strong>Why adversarial robustness matters:</strong></p>
<ul>
<li><strong>Security systems</strong>: Attackers can craft adversarial inputs to evade detection (malware classifiers, spam filters, fraud detection)</li>
<li><strong>Safety-critical systems</strong>: Self-driving cars must not misclassify stop signs</li>
<li><strong>Trustworthiness</strong>: If tiny perturbations break models, can we trust them?</li>
</ul>
<p>Adversarial examples reveal that models learn brittle, superficial features, not robust concepts. A model that sees a stop sign with stickers and predicts ‚Äúspeed limit‚Äù has not learned what a stop sign is‚Äîit has learned a fragile pattern.</p>
<hr>
<h2 id="brittleness-and-shortcut-learning-ch34">Brittleness and Shortcut Learning</h2>
<p>Models are brittle: they fail on inputs slightly different from training data. Small changes in distribution, format, or context break predictions. This brittleness arises from <strong>shortcut learning</strong>: models exploit spurious correlations rather than learning robust features.</p>
<p><strong>Shortcut learning examples:</strong></p>
<p><strong>Cows in fields:</strong> Image classifiers trained on photos of cows (mostly in grassy fields) learn ‚Äúgrass texture predicts cow.‚Äù Shown a cow on a beach, the model fails. It learned the background, not the object.</p>
<p><strong>Sentiment analysis and negation:</strong> Sentiment classifiers trained on ‚ÄúThis movie is good‚Äù (positive) and ‚ÄúThis movie is bad‚Äù (negative) struggle with negation: ‚ÄúThis movie is not bad‚Äù is classified as negative because ‚Äúbad‚Äù is a strong negative signal. The model learned word-level shortcuts, not compositional semantics.</p>
<p><strong>BERT and word overlap:</strong> BERT-based question answering models trained on SQuAD (Stanford Question Answering Dataset) learned to exploit word overlap between question and passage. Questions like ‚ÄúWho scored the goal?‚Äù are answered by finding the sentence with ‚Äúscored‚Äù and ‚Äúgoal.‚Äù Adversarial datasets (SQuAD 2.0, adversarial SQuAD) break this shortcut by adding distractor sentences with high word overlap. Performance drops 40%.</p>
<p><strong>NLI and overlap heuristics:</strong> Natural Language Inference models trained on SNLI/MNLI learn shortcuts: if the hypothesis contains negation words (‚Äúnot,‚Äù ‚Äúnever‚Äù), predict ‚Äúcontradiction.‚Äù If the hypothesis is shorter than the premise, predict ‚Äúentailment.‚Äù These shortcuts work on the training set but fail on out-of-distribution examples.</p>
<p><strong>Why shortcuts are learned:</strong></p>
<p>Models optimize for training accuracy using the easiest features. If a spurious correlation (grass ‚Üí cow) achieves 95% accuracy, the model uses it. Learning robust features (actual cow shape) requires more data, more capacity, or better inductive biases.</p>
<p>Shortcuts are not bugs‚Äîthey are optimal solutions to the training objective. The training set does not punish shortcuts, so models exploit them. Only out-of-distribution evaluation reveals that shortcuts fail.</p>
<p><strong>Robustness failures under distribution shift:</strong></p>
<p>Models trained on one distribution fail when deployed on another. Even small shifts break performance.</p>
<p><strong>Example: COVID-19 and chest X-ray models</strong></p>
<p>Researchers trained models to detect pneumonia from chest X-rays. Deployed during COVID-19, the models failed. Why? Training data came from specific hospitals with specific imaging protocols. COVID patients had different characteristics (disease presentation, demographics, comorbidities). The models learned hospital-specific artifacts (scanner type, positioning) as signals, not actual pathology.</p>
<p>The model memorized features of the training hospital, not generalizable medical features. Out-of-distribution deployment revealed this brittleness.</p>
<p><strong>Lack of common sense:</strong></p>
<p>Models lack world knowledge and fail on cases obvious to humans.</p>
<p><strong>Example: ‚ÄúHow many eyes does a horse have?‚Äù</strong></p>
<p>Model: ‚ÄúFour.‚Äù</p>
<p>The model did not learn that horses are animals and animals have two eyes (except insects, spiders). It pattern-matched ‚Äúhow many‚Äù questions and generated plausible-sounding wrong answers.</p>
<p><img  src="/eng-ai/_astro/34-diagram.DkVw39Fn_25rx8V.svg" alt="Brittleness and Shortcut Learning diagram" width="800" height="450" loading="lazy" decoding="async"></p>
<p><strong>Figure 34.1</strong>: Adversarial example showing how imperceptible noise (Œµ = 0.007) causes a confident misclassification. The model predicts ‚Äúpanda‚Äù with 99.8% confidence on the original image, but ‚Äúgibbon‚Äù with 99.3% confidence on the perturbed image. To humans, the images are indistinguishable. To the model, they are completely different. This reveals the brittleness of learned features.</p>
<hr>
<h2 id="engineering-takeaway-ch34">Engineering Takeaway</h2>
<p><strong>Hallucinations are fundamental to generative models‚Äîcannot be eliminated, only reduced through grounding, citations, and guardrails.</strong> Generative models optimize likelihood, not truth. They produce fluent, plausible output regardless of factual correctness. Retrieval-augmented generation and citation reduce hallucinations by grounding outputs in verified sources, but models can still misinterpret, cherry-pick, or fabricate despite constraints. Every deployment of generative models must assume hallucinations occur and implement verification mechanisms‚Äîhuman review for high-stakes domains, user-facing citations for fact-checking, confidence calibration to flag uncertain outputs.</p>
<p><strong>Bias is in the data, not just the model‚Äîfixing bias requires fixing data sources, labels, and ground truth definitions.</strong> Debiasing techniques (rebalancing, fairness constraints, representation editing) address symptoms, not causes. If historical hiring data is sexist, a hiring model will be sexist. If recidivism labels reflect biased policing, a recidivism model will be biased. Technical fixes cannot eliminate bias when the data itself encodes discrimination. Addressing bias requires auditing data sources, questioning whether labels are just, and often deciding that some prediction tasks should not be automated at all.</p>
<p><strong>Adversarial robustness is hard‚Äîmodels learn surface patterns, not deep understanding, making them vulnerable to attacks.</strong> Small perturbations invisible to humans fool models with high confidence. Adversarial training improves robustness to known attacks but does not generalize to new attacks. Physical adversarial examples (stickers, glasses, patches) threaten real-world systems. For security-critical applications (authentication, malware detection) and safety-critical applications (autonomous vehicles, medical diagnosis), adversarial vulnerabilities are unacceptable. Defense requires multiple layers: robust models, anomaly detection, redundancy, human oversight.</p>
<p><strong>Distribution shift breaks models‚Äîtest on realistic deployment scenarios, including edge cases and out-of-distribution inputs.</strong> Models trained on curated benchmarks fail on real-world messiness: noisy inputs, missing data, unusual formats, domain shifts, temporal changes. Test sets must include out-of-distribution examples that resemble deployment challenges. Robustness evaluation (ImageNet-C for corruption, ImageNet-A for adversarial natural examples) reveals brittleness hidden by standard benchmarks. Stress testing‚Äîdeliberate probing for failures‚Äîis essential before deployment.</p>
<p><strong>Guardrails are mandatory for high-stakes applications‚Äîno single model is reliable enough for unsupervised deployment.</strong> Hallucinations, bias, adversarial vulnerability, and brittleness mean models will fail. For low-stakes applications (entertainment, recommendations), failures are tolerable. For high-stakes applications (medical diagnosis, hiring, criminal justice, autonomous vehicles), failures cause harm. Guardrails mitigate risk: human-in-the-loop approval for critical decisions, confidence thresholds for flagging uncertain predictions, ensemble models for redundancy, rule-based checks for constraint violations, appeals processes for affected individuals.</p>
<p><strong>Explainability helps debug but does not solve brittleness‚Äîunderstanding why a model failed does not make it robust.</strong> Explainability techniques (saliency maps, LIME, SHAP) show what features the model used. This is useful for debugging shortcut learning and bias. But knowing the model relies on grass texture to predict cows does not make the model robust to cows on beaches. Explainability is a diagnostic tool, not a fix. Robustness requires better training data, better inductive biases, better architectures, or constraining deployment to domains where the model is known to work.</p>
<p><strong>Safety-critical systems cannot rely on ML alone‚Äîneed redundancy, verification, and fallback mechanisms.</strong> A 99.9% accurate model still fails 0.1% of the time. In safety-critical domains, that is unacceptable. Self-driving cars need redundant sensors and perception systems. Medical diagnosis needs human review. Financial systems need rule-based sanity checks. ML models provide capability, but system design provides safety. Redundancy (multiple models, diverse approaches), verification (rule-based checks on outputs), fallback mechanisms (human override, safe degraded mode) ensure that single-point ML failures do not cause catastrophic outcomes.</p>
<hr>
<h2 id="references-and-further-reading-ch34">References and Further Reading</h2>
<p><strong>Survey of Hallucination in Natural Language Generation</strong>
Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., &#x26; Fung, P. (2023). <em>ACM Computing Surveys</em></p>
<p><em>Why it matters:</em> This comprehensive survey categorizes hallucination types in NLG (factual, faithfulness, instruction-following), explains causes (data quality, training objectives, decoding strategies), and reviews mitigation techniques (retrieval augmentation, fact verification, calibration). It shows that hallucinations are not rare bugs but systematic failures inherent to generation models. The survey is essential for understanding the scope of the hallucination problem and why it cannot be eliminated, only managed.</p>
<p><strong>Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</strong>
Buolamwini, J., &#x26; Gebru, T. (2018). <em>FAT</em> 2018*</p>
<p><em>Why it matters:</em> This paper audited three commercial face recognition systems (Microsoft, IBM, Face++) and found significant accuracy disparities: 99% accuracy on light-skinned males, 65% accuracy on dark-skinned females. The cause: biased training datasets that underrepresent darker-skinned individuals and women. The paper demonstrated that bias is measurable, significant, and harms marginalized groups. It catalyzed discussions about algorithmic fairness and led companies to audit and improve their systems. Gender Shades is a landmark in AI ethics.</p>
<p><strong>Explaining and Harnessing Adversarial Examples</strong>
Goodfellow, I. J., Shlens, J., &#x26; Szegedy, C. (2015). <em>ICLR 2015</em></p>
<p><em>Why it matters:</em> This paper introduced the Fast Gradient Sign Method (FGSM) for generating adversarial examples and explained why they exist: neural networks learn linear decision boundaries in high-dimensional space, where small perturbations can cross boundaries. The paper showed that adversarial examples transfer across models (black-box attacks) and proposed adversarial training as a defense. It established adversarial robustness as a fundamental ML challenge and spawned a research field on attacks and defenses.</p>
<hr>
<p>The final chapter addresses safety, alignment, and control: why models optimize the wrong objectives, how to align them with human intent, what guardrails are necessary, and why safety is system design, not just better models.</p> </article> <nav class="chapter-navigation" data-astro-cid-pux6a34n> <a href="/eng-ai/part7/33-evaluation" class="nav-prev " data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>‚Üê Previous</span> <span class="nav-title" data-astro-cid-pux6a34n>Evaluation - Why Accuracy Is Not Enough</span> </a> <a href="/eng-ai/part7/35-safety-alignment-control" class="nav-next" data-astro-cid-pux6a34n> <span class="nav-label" data-astro-cid-pux6a34n>Next ‚Üí</span> <span class="nav-title" data-astro-cid-pux6a34n>Safety, Alignment, and Control</span> </a> </nav>  </main> <aside class="chapter-toc"> <nav class="toc" data-astro-cid-xvrfupwn><h4 data-astro-cid-xvrfupwn>On This Page</h4><ul data-astro-cid-xvrfupwn><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#why-hallucinations-happen-probability-is-not-truth-ch34" data-astro-cid-xvrfupwn>Why Hallucinations Happen: Probability Is Not Truth</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#bias-data-becomes-destiny-ch34" data-astro-cid-xvrfupwn>Bias: Data Becomes Destiny</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#adversarial-inputs-how-models-are-tricked-ch34" data-astro-cid-xvrfupwn>Adversarial Inputs: How Models Are Tricked</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#brittleness-and-shortcut-learning-ch34" data-astro-cid-xvrfupwn>Brittleness and Shortcut Learning</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#engineering-takeaway-ch34" data-astro-cid-xvrfupwn>Engineering Takeaway</a></li><li class="toc-level-2" data-astro-cid-xvrfupwn><a href="#references-and-further-reading-ch34" data-astro-cid-xvrfupwn>References and Further Reading</a></li></ul></nav> </aside> </div>   </body> </html>